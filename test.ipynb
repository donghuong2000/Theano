{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    },
    "colab": {
      "name": "test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOhK5X6DnjWX"
      },
      "source": [
        "# network3.ipynb based on network3.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "vjJyNzFTnjWa"
      },
      "source": [
        "#### from http://neuralnetworksanddeeplearning.com/chap6.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4OHjsJsnjWc"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9TEi8u5njWc"
      },
      "source": [
        "### Standard Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKCaJ7-PnjWc"
      },
      "source": [
        "import cPickle \n",
        "import gzip"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZv1cBnsnjWd"
      },
      "source": [
        "### Third-party libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAqsv4oFnjWd"
      },
      "source": [
        "import numpy as np\n",
        "import theano\n",
        "import theano.tensor as T\n",
        "from theano.tensor.nnet import conv\n",
        "from theano.tensor.nnet import softmax\n",
        "from theano.tensor import shared_randomstreams\n",
        "from theano.tensor.signal import pool\n"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLSGOWcBnjWe"
      },
      "source": [
        "### Activation functions for neurons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "NacnDPVZnjWe"
      },
      "source": [
        "def linear(z): return z\n",
        "def ReLU(z): return T.maximum(0.0, z)\n",
        "from theano.tensor.nnet import sigmoid\n",
        "from theano.tensor import tanh"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2HxkILKnjWf"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDVikj8tnjWf",
        "outputId": "f67921d5-9b57-4035-97e1-ba946e2fe2aa"
      },
      "source": [
        "GPU = False\n",
        "if GPU:\n",
        "    print \"Trying to run under a GPU.  If this is not desired, then modify \"+\\\n",
        "        \"network3.py\\nto set the GPU flag to False.\"\n",
        "    try: theano.config.device = 'gpu'\n",
        "    except: pass # it's already set\n",
        "    theano.config.floatX = 'float32'\n",
        "else:\n",
        "    print \"Running with a CPU.  If this is not desired, then the modify \"+\\\n",
        "        \"network3.py to set\\nthe GPU flag to True.\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running with a CPU.  If this is not desired, then the modify network3.py to set\n",
            "the GPU flag to True.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXaJjy73njWf"
      },
      "source": [
        "## Load the MNIST data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "LruL0hcvnjWg"
      },
      "source": [
        "#download data\n",
        "#!wget http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n",
        "import matplotlib\n",
        "import numpy \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def load_data_shared(filename=\"/content/mnist.pkl.gz\"):\n",
        "    f = gzip.open(filename, 'rb')\n",
        "    training_data, validation_data, test_data = cPickle.load(f)\n",
        "    f.close()\n",
        "    plt.rcParams['figure.figsize'] = (10, 10)\n",
        "    plt.rcParams['image.cmap'] = 'gray'\n",
        "    for i in range(9):\n",
        "      plt.subplot(1,10,i+1)\n",
        "      plt.imshow(test_data[0][i].reshape(28,28))\n",
        "      plt.axis('off')\n",
        "      plt.title(str(test_data[1][i]))\n",
        "\n",
        "    plt.show()\n",
        "    def shared(data):\n",
        "        \"\"\"Place the data into shared variables.  This allows Theano to copy\n",
        "        the data to the GPU, if one is available.\n",
        "\n",
        "        \"\"\"\n",
        "        shared_x = theano.shared(\n",
        "            np.asarray(data[0], dtype=theano.config.floatX), borrow=True)\n",
        "        shared_y = theano.shared(\n",
        "            np.asarray(data[1], dtype=theano.config.floatX), borrow=True)\n",
        "        return shared_x, T.cast(shared_y, \"int32\")\n",
        "    return [shared(training_data), shared(validation_data), shared(test_data)]"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A7fJU3LCmXK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-H2PrgpnjWg"
      },
      "source": [
        "## Main class used to construct and train networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "vBnaHllUnjWg"
      },
      "source": [
        "class Network(object):\n",
        "\n",
        "    def __init__(self, layers, mini_batch_size):\n",
        "        \"\"\"Takes a list of `layers`, describing the network architecture, and\n",
        "        a value for the `mini_batch_size` to be used during training\n",
        "        by stochastic gradient descent.\n",
        "\n",
        "        \"\"\"\n",
        "        self.layers = layers\n",
        "        self.mini_batch_size = mini_batch_size\n",
        "        self.params = [param for layer in self.layers for param in layer.params]\n",
        "        self.x = T.matrix(\"x\")\n",
        "        self.y = T.ivector(\"y\")\n",
        "        init_layer = self.layers[0]\n",
        "        init_layer.set_inpt(self.x, self.x, self.mini_batch_size)\n",
        "        for j in xrange(1, len(self.layers)):\n",
        "            prev_layer, layer  = self.layers[j-1], self.layers[j]\n",
        "            layer.set_inpt(\n",
        "                prev_layer.output, prev_layer.output_dropout, self.mini_batch_size)\n",
        "        self.output = self.layers[-1].output\n",
        "        self.output_dropout = self.layers[-1].output_dropout\n",
        "        \n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
        "            validation_data, test_data, lmbda=0.0):\n",
        "        \"\"\"Train the network using mini-batch stochastic gradient descent.\"\"\"\n",
        "        training_x, training_y = training_data\n",
        "        validation_x, validation_y = validation_data\n",
        "        test_x, test_y = test_data\n",
        "\n",
        "        # compute number of minibatches for training, validation and testing\n",
        "        num_training_batches = size(training_data)/mini_batch_size\n",
        "        num_validation_batches = size(validation_data)/mini_batch_size\n",
        "        num_test_batches = size(test_data)/mini_batch_size\n",
        "\n",
        "        # define the (regularized) cost function, symbolic gradients, and updates\n",
        "        l2_norm_squared = sum([(layer.w**2).sum() for layer in self.layers])\n",
        "        cost = self.layers[-1].cost(self)+\\\n",
        "               0.5*lmbda*l2_norm_squared/num_training_batches\n",
        "        grads = T.grad(cost, self.params)\n",
        "        updates = [(param, param-eta*grad)\n",
        "                   for param, grad in zip(self.params, grads)]\n",
        "\n",
        "        # define functions to train a mini-batch, and to compute the\n",
        "        # accuracy in validation and test mini-batches.\n",
        "        i = T.lscalar() # mini-batch index\n",
        "        train_mb = theano.function(\n",
        "            [i], cost, updates=updates,\n",
        "            givens={\n",
        "                self.x:\n",
        "                training_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
        "                self.y:\n",
        "                training_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
        "            })\n",
        "        validate_mb_accuracy = theano.function(\n",
        "            [i], self.layers[-1].accuracy(self.y),\n",
        "            givens={\n",
        "                self.x:\n",
        "                validation_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
        "                self.y:\n",
        "                validation_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
        "            })\n",
        "        test_mb_accuracy = theano.function(\n",
        "            [i], self.layers[-1].accuracy(self.y),\n",
        "            givens={\n",
        "                self.x:\n",
        "                test_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
        "                self.y:\n",
        "                test_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
        "            })\n",
        "        self.test_mb_predictions = theano.function(\n",
        "            [i], self.layers[-1].y_out,\n",
        "            givens={\n",
        "                self.x:\n",
        "                test_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
        "            })\n",
        "        # Do the actual training\n",
        "        best_validation_accuracy = 0.0\n",
        "        for epoch in xrange(epochs):\n",
        "            for minibatch_index in xrange(num_training_batches):\n",
        "                iteration = num_training_batches*epoch+minibatch_index\n",
        "                if iteration % 1000 == 0:\n",
        "                    print(\"Training mini-batch number {0}\".format(iteration))\n",
        "                cost_ij = train_mb(minibatch_index)\n",
        "                if (iteration+1) % num_training_batches == 0:\n",
        "                    validation_accuracy = np.mean(\n",
        "                        [validate_mb_accuracy(j) for j in xrange(num_validation_batches)])\n",
        "                    print(\"Epoch {0}: validation accuracy {1:.2%}\".format(\n",
        "                        epoch, validation_accuracy))\n",
        "                    if validation_accuracy >= best_validation_accuracy:\n",
        "                        print(\"This is the best validation accuracy to date.\")\n",
        "                        best_validation_accuracy = validation_accuracy\n",
        "                        best_iteration = iteration\n",
        "                        if test_data:\n",
        "                            test_accuracy = np.mean([test_mb_accuracy(j) for j in xrange(num_test_batches)])\n",
        "                            print('The corresponding test accuracy is {0:.2%}'.format(test_accuracy))\n",
        "        print(\"Finished training network.\")\n",
        "        print(\"Best validation accuracy of {0:.2%} obtained at iteration {1}\".format(\n",
        "            best_validation_accuracy, best_iteration))\n",
        "        print(\"Corresponding test accuracy of {0:.2%}\".format(test_accuracy))"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f25-e6y0njWl"
      },
      "source": [
        "## Define layer types"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "q8n4ncOJnjWn"
      },
      "source": [
        "class ConvPoolLayer(object):\n",
        "    \"\"\"Used to create a combination of a convolutional and a max-pooling\n",
        "    layer.  A more sophisticated implementation would separate the\n",
        "    two, but for our purposes we'll always use them together, and it\n",
        "    simplifies the code, so it makes sense to combine them.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, filter_shape, image_shape, poolsize=(2, 2),\n",
        "                 activation_fn=sigmoid):\n",
        "        \"\"\"`filter_shape` is a tuple of length 4, whose entries are the number\n",
        "        of filters, the number of input feature maps, the filter height, and the\n",
        "        filter width.\n",
        "\n",
        "        `image_shape` is a tuple of length 4, whose entries are the\n",
        "        mini-batch size, the number of input feature maps, the image\n",
        "        height, and the image width.\n",
        "\n",
        "        `poolsize` is a tuple of length 2, whose entries are the y and\n",
        "        x pooling sizes.\n",
        "\n",
        "        \"\"\"\n",
        "        self.filter_shape = filter_shape\n",
        "        self.image_shape = image_shape\n",
        "        self.poolsize = poolsize\n",
        "        self.activation_fn=activation_fn\n",
        "        # initialize weights and biases\n",
        "        n_out = (filter_shape[0]*np.prod(filter_shape[2:])/np.prod(poolsize))\n",
        "        self.w = theano.shared(\n",
        "            np.asarray(\n",
        "                np.random.normal(loc=0, scale=np.sqrt(1.0/n_out), size=filter_shape),\n",
        "                dtype=theano.config.floatX),\n",
        "            borrow=True)\n",
        "        self.b = theano.shared(\n",
        "            np.asarray(\n",
        "                np.random.normal(loc=0, scale=1.0, size=(filter_shape[0],)),\n",
        "                dtype=theano.config.floatX),\n",
        "            borrow=True)\n",
        "        self.params = [self.w, self.b]\n",
        "\n",
        "    def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
        "        self.inpt = inpt.reshape(self.image_shape)\n",
        "        conv_out = conv.conv2d(\n",
        "            input=self.inpt, filters=self.w, filter_shape=self.filter_shape,\n",
        "            image_shape=self.image_shape)\n",
        "        pooled_out = pool.pool_2d(\n",
        "            input=conv_out, ws=self.poolsize, ignore_border=True)\n",
        "        self.output = self.activation_fn(\n",
        "            pooled_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n",
        "        self.output_dropout = self.output # no dropout in the convolutional layers"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "qmv7qKtxnjWn"
      },
      "source": [
        "class FullyConnectedLayer(object):\n",
        "\n",
        "    def __init__(self, n_in, n_out, activation_fn=sigmoid, p_dropout=0.0):\n",
        "        self.n_in = n_in\n",
        "        self.n_out = n_out\n",
        "        self.activation_fn = activation_fn\n",
        "        self.p_dropout = p_dropout\n",
        "        # Initialize weights and biases\n",
        "        self.w = theano.shared(\n",
        "            np.asarray(\n",
        "                np.random.normal(\n",
        "                    loc=0.0, scale=np.sqrt(1.0/n_out), size=(n_in, n_out)),\n",
        "                dtype=theano.config.floatX),\n",
        "            name='w', borrow=True)\n",
        "        self.b = theano.shared(\n",
        "            np.asarray(np.random.normal(loc=0.0, scale=1.0, size=(n_out,)),\n",
        "                       dtype=theano.config.floatX),\n",
        "            name='b', borrow=True)\n",
        "        self.params = [self.w, self.b]\n",
        "\n",
        "    def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
        "        self.inpt = inpt.reshape((mini_batch_size, self.n_in))\n",
        "        self.output = self.activation_fn(\n",
        "            (1-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)\n",
        "        self.y_out = T.argmax(self.output, axis=1)\n",
        "        self.inpt_dropout = dropout_layer(\n",
        "            inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)\n",
        "        self.output_dropout = self.activation_fn(\n",
        "            T.dot(self.inpt_dropout, self.w) + self.b)\n",
        "\n",
        "    def accuracy(self, y):\n",
        "        \"Return the accuracy for the mini-batch.\"\n",
        "        return T.mean(T.eq(y, self.y_out))\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "X1BWBW6TnjWo"
      },
      "source": [
        "class SoftmaxLayer(object):\n",
        "\n",
        "    def __init__(self, n_in, n_out, p_dropout=0.0):\n",
        "        self.n_in = n_in\n",
        "        self.n_out = n_out\n",
        "        self.p_dropout = p_dropout\n",
        "        # Initialize weights and biases\n",
        "        self.w = theano.shared(\n",
        "            np.zeros((n_in, n_out), dtype=theano.config.floatX),\n",
        "            name='w', borrow=True)\n",
        "        self.b = theano.shared(\n",
        "            np.zeros((n_out,), dtype=theano.config.floatX),\n",
        "            name='b', borrow=True)\n",
        "        self.params = [self.w, self.b]\n",
        "\n",
        "    def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
        "        self.inpt = inpt.reshape((mini_batch_size, self.n_in))\n",
        "        self.output = softmax((1-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)\n",
        "        self.y_out = T.argmax(self.output, axis=1)\n",
        "        self.inpt_dropout = dropout_layer(\n",
        "            inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)\n",
        "        self.output_dropout = softmax(T.dot(self.inpt_dropout, self.w) + self.b)\n",
        "\n",
        "    def cost(self, net):\n",
        "        \"Return the log-likelihood cost.\"\n",
        "        return -T.mean(T.log(self.output_dropout)[T.arange(net.y.shape[0]), net.y])\n",
        "\n",
        "    def accuracy(self, y):\n",
        "        \"Return the accuracy for the mini-batch.\"\n",
        "        return T.mean(T.eq(y, self.y_out))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrNdGC1onjWp"
      },
      "source": [
        "## Miscellanea"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "0QTYsSAhnjWp"
      },
      "source": [
        "def size(data):\n",
        "    \"Return the size of the dataset `data`.\"\n",
        "    return data[0].get_value(borrow=True).shape[0]\n",
        "\n",
        "def dropout_layer(layer, p_dropout):\n",
        "    srng = shared_randomstreams.RandomStreams(\n",
        "        np.random.RandomState(0).randint(999999))\n",
        "    mask = srng.binomial(n=1, p=1-p_dropout, size=layer.shape)\n",
        "    return layer*T.cast(mask, theano.config.floatX)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrXLQRBlnjWp"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "## Load Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "9bOjlxCSnjWp",
        "outputId": "f5702fdc-3d15-4080-9ca0-d1d7beb563e0"
      },
      "source": [
        "training_data, validation_data, test_data = load_data_shared()\n"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsdwHGrZnjWq"
      },
      "source": [
        "# Run shallow architecture (single hidden layer) as baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "nBoJjArOnjWq"
      },
      "source": [
        "mini_batch_size = 10"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "qFAL6b6FnjWq"
      },
      "source": [
        "net = Network([\n",
        "        FullyConnectedLayer(n_in=784, n_out=100),\n",
        "        SoftmaxLayer(n_in=100, n_out=10)\n",
        "    ], mini_batch_size)"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_q2QqSSnjWq",
        "outputId": "7b413fa4-3031-43db-b48f-2e3e7b608c6e"
      },
      "source": [
        "net.SGD(training_data, 1, mini_batch_size, 0.1, validation_data, test_data)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training mini-batch number 0\n",
            "Training mini-batch number 1000\n",
            "Training mini-batch number 2000\n",
            "Training mini-batch number 3000\n",
            "Training mini-batch number 4000\n",
            "Epoch 0: validation accuracy 92.52%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 91.96%\n",
            "Finished training network.\n",
            "Best validation accuracy of 92.52% obtained at iteration 4999\n",
            "Corresponding test accuracy of 91.96%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKQxbMECnjWr"
      },
      "source": [
        "# add a convolutional layer: \n",
        "net = Network([\n",
        "        ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28),\n",
        "                     filter_shape=(20, 1, 5, 5),\n",
        "                     poolsize=(2, 2)),\n",
        "        FullyConnectedLayer(n_in=20*12*12, n_out=100),\n",
        "        SoftmaxLayer(n_in=100, n_out=10)], \n",
        "              mini_batch_size)"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S5u3k6jnjWr",
        "outputId": "3e1e8cc6-4d19-4b12-a324-3ab858f299a6"
      },
      "source": [
        "net.SGD(training_data, 60, mini_batch_size, 0.1, validation_data, test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training mini-batch number 0\n",
            "Training mini-batch number 1000\n",
            "Training mini-batch number 2000\n",
            "Training mini-batch number 3000\n",
            "Training mini-batch number 4000\n",
            "Epoch 0: validation accuracy 93.56%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 92.74%\n",
            "Training mini-batch number 5000\n",
            "Training mini-batch number 6000\n",
            "Training mini-batch number 7000\n",
            "Training mini-batch number 8000\n",
            "Training mini-batch number 9000\n",
            "Epoch 1: validation accuracy 96.03%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 95.51%\n",
            "Training mini-batch number 10000\n",
            "Training mini-batch number 11000\n",
            "Training mini-batch number 12000\n",
            "Training mini-batch number 13000\n",
            "Training mini-batch number 14000\n",
            "Epoch 2: validation accuracy 96.96%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 96.57%\n",
            "Training mini-batch number 15000\n",
            "Training mini-batch number 16000\n",
            "Training mini-batch number 17000\n",
            "Training mini-batch number 18000\n",
            "Training mini-batch number 19000\n",
            "Epoch 3: validation accuracy 97.41%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 97.10%\n",
            "Training mini-batch number 20000\n",
            "Training mini-batch number 21000\n",
            "Training mini-batch number 22000\n",
            "Training mini-batch number 23000\n",
            "Training mini-batch number 24000\n",
            "Epoch 4: validation accuracy 97.80%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 97.47%\n",
            "Training mini-batch number 25000\n",
            "Training mini-batch number 26000\n",
            "Training mini-batch number 27000\n",
            "Training mini-batch number 28000\n",
            "Training mini-batch number 29000\n",
            "Epoch 5: validation accuracy 97.99%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 97.73%\n",
            "Training mini-batch number 30000\n",
            "Training mini-batch number 31000\n",
            "Training mini-batch number 32000\n",
            "Training mini-batch number 33000\n",
            "Training mini-batch number 34000\n",
            "Epoch 6: validation accuracy 98.00%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 97.83%\n",
            "Training mini-batch number 35000\n",
            "Training mini-batch number 36000\n",
            "Training mini-batch number 37000\n",
            "Training mini-batch number 38000\n",
            "Training mini-batch number 39000\n",
            "Epoch 7: validation accuracy 98.14%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 97.88%\n",
            "Training mini-batch number 40000\n",
            "Training mini-batch number 41000\n",
            "Training mini-batch number 42000\n",
            "Training mini-batch number 43000\n",
            "Training mini-batch number 44000\n",
            "Epoch 8: validation accuracy 98.25%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.07%\n",
            "Training mini-batch number 45000\n",
            "Training mini-batch number 46000\n",
            "Training mini-batch number 47000\n",
            "Training mini-batch number 48000\n",
            "Training mini-batch number 49000\n",
            "Epoch 9: validation accuracy 98.40%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.24%\n",
            "Training mini-batch number 50000\n",
            "Training mini-batch number 51000\n",
            "Training mini-batch number 52000\n",
            "Training mini-batch number 53000\n",
            "Training mini-batch number 54000\n",
            "Epoch 10: validation accuracy 98.52%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.31%\n",
            "Training mini-batch number 55000\n",
            "Training mini-batch number 56000\n",
            "Training mini-batch number 57000\n",
            "Training mini-batch number 58000\n",
            "Training mini-batch number 59000\n",
            "Epoch 11: validation accuracy 98.58%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.43%\n",
            "Training mini-batch number 60000\n",
            "Training mini-batch number 61000\n",
            "Training mini-batch number 62000\n",
            "Training mini-batch number 63000\n",
            "Training mini-batch number 64000\n",
            "Epoch 12: validation accuracy 98.61%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.51%\n",
            "Training mini-batch number 65000\n",
            "Training mini-batch number 66000\n",
            "Training mini-batch number 67000\n",
            "Training mini-batch number 68000\n",
            "Training mini-batch number 69000\n",
            "Epoch 13: validation accuracy 98.66%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.58%\n",
            "Training mini-batch number 70000\n",
            "Training mini-batch number 71000\n",
            "Training mini-batch number 72000\n",
            "Training mini-batch number 73000\n",
            "Training mini-batch number 74000\n",
            "Epoch 14: validation accuracy 98.66%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.67%\n",
            "Training mini-batch number 75000\n",
            "Training mini-batch number 76000\n",
            "Training mini-batch number 77000\n",
            "Training mini-batch number 78000\n",
            "Training mini-batch number 79000\n",
            "Epoch 15: validation accuracy 98.68%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.69%\n",
            "Training mini-batch number 80000\n",
            "Training mini-batch number 81000\n",
            "Training mini-batch number 82000\n",
            "Training mini-batch number 83000\n",
            "Training mini-batch number 84000\n",
            "Epoch 16: validation accuracy 98.69%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.76%\n",
            "Training mini-batch number 85000\n",
            "Training mini-batch number 86000\n",
            "Training mini-batch number 87000\n",
            "Training mini-batch number 88000\n",
            "Training mini-batch number 89000\n",
            "Epoch 17: validation accuracy 98.68%\n",
            "Training mini-batch number 90000\n",
            "Training mini-batch number 91000\n",
            "Training mini-batch number 92000\n",
            "Training mini-batch number 93000\n",
            "Training mini-batch number 94000\n",
            "Epoch 18: validation accuracy 98.71%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.74%\n",
            "Training mini-batch number 95000\n",
            "Training mini-batch number 96000\n",
            "Training mini-batch number 97000\n",
            "Training mini-batch number 98000\n",
            "Training mini-batch number 99000\n",
            "Epoch 19: validation accuracy 98.71%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.74%\n",
            "Training mini-batch number 100000\n",
            "Training mini-batch number 101000\n",
            "Training mini-batch number 102000\n",
            "Training mini-batch number 103000\n",
            "Training mini-batch number 104000\n",
            "Epoch 20: validation accuracy 98.73%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.74%\n",
            "Training mini-batch number 105000\n",
            "Training mini-batch number 106000\n",
            "Training mini-batch number 107000\n",
            "Training mini-batch number 108000\n",
            "Training mini-batch number 109000\n",
            "Epoch 21: validation accuracy 98.74%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.78%\n",
            "Training mini-batch number 110000\n",
            "Training mini-batch number 111000\n",
            "Training mini-batch number 112000\n",
            "Training mini-batch number 113000\n",
            "Training mini-batch number 114000\n",
            "Epoch 22: validation accuracy 98.75%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.78%\n",
            "Training mini-batch number 115000\n",
            "Training mini-batch number 116000\n",
            "Training mini-batch number 117000\n",
            "Training mini-batch number 118000\n",
            "Training mini-batch number 119000\n",
            "Epoch 23: validation accuracy 98.73%\n",
            "Training mini-batch number 120000\n",
            "Training mini-batch number 121000\n",
            "Training mini-batch number 122000\n",
            "Training mini-batch number 123000\n",
            "Training mini-batch number 124000\n",
            "Epoch 24: validation accuracy 98.75%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.81%\n",
            "Training mini-batch number 125000\n",
            "Training mini-batch number 126000\n",
            "Training mini-batch number 127000\n",
            "Training mini-batch number 128000\n",
            "Training mini-batch number 129000\n",
            "Epoch 25: validation accuracy 98.78%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.80%\n",
            "Training mini-batch number 130000\n",
            "Training mini-batch number 131000\n",
            "Training mini-batch number 132000\n",
            "Training mini-batch number 133000\n",
            "Training mini-batch number 134000\n",
            "Epoch 26: validation accuracy 98.76%\n",
            "Training mini-batch number 135000\n",
            "Training mini-batch number 136000\n",
            "Training mini-batch number 137000\n",
            "Training mini-batch number 138000\n",
            "Training mini-batch number 139000\n",
            "Epoch 27: validation accuracy 98.75%\n",
            "Training mini-batch number 140000\n",
            "Training mini-batch number 141000\n",
            "Training mini-batch number 142000\n",
            "Training mini-batch number 143000\n",
            "Training mini-batch number 144000\n",
            "Epoch 28: validation accuracy 98.74%\n",
            "Training mini-batch number 145000\n",
            "Training mini-batch number 146000\n",
            "Training mini-batch number 147000\n",
            "Training mini-batch number 148000\n",
            "Training mini-batch number 149000\n",
            "Epoch 29: validation accuracy 98.75%\n",
            "Training mini-batch number 150000\n",
            "Training mini-batch number 151000\n",
            "Training mini-batch number 152000\n",
            "Training mini-batch number 153000\n",
            "Training mini-batch number 154000\n",
            "Epoch 30: validation accuracy 98.75%\n",
            "Training mini-batch number 155000\n",
            "Training mini-batch number 156000\n",
            "Training mini-batch number 157000\n",
            "Training mini-batch number 158000\n",
            "Training mini-batch number 159000\n",
            "Epoch 31: validation accuracy 98.75%\n",
            "Training mini-batch number 160000\n",
            "Training mini-batch number 161000\n",
            "Training mini-batch number 162000\n",
            "Training mini-batch number 163000\n",
            "Training mini-batch number 164000\n",
            "Epoch 32: validation accuracy 98.76%\n",
            "Training mini-batch number 165000\n",
            "Training mini-batch number 166000\n",
            "Training mini-batch number 167000\n",
            "Training mini-batch number 168000\n",
            "Training mini-batch number 169000\n",
            "Epoch 33: validation accuracy 98.76%\n",
            "Training mini-batch number 170000\n",
            "Training mini-batch number 171000\n",
            "Training mini-batch number 172000\n",
            "Training mini-batch number 173000\n",
            "Training mini-batch number 174000\n",
            "Epoch 34: validation accuracy 98.76%\n",
            "Training mini-batch number 175000\n",
            "Training mini-batch number 176000\n",
            "Training mini-batch number 177000\n",
            "Training mini-batch number 178000\n",
            "Training mini-batch number 179000\n",
            "Epoch 35: validation accuracy 98.75%\n",
            "Training mini-batch number 180000\n",
            "Training mini-batch number 181000\n",
            "Training mini-batch number 182000\n",
            "Training mini-batch number 183000\n",
            "Training mini-batch number 184000\n",
            "Epoch 36: validation accuracy 98.75%\n",
            "Training mini-batch number 185000\n",
            "Training mini-batch number 186000\n",
            "Training mini-batch number 187000\n",
            "Training mini-batch number 188000\n",
            "Training mini-batch number 189000\n",
            "Epoch 37: validation accuracy 98.75%\n",
            "Training mini-batch number 190000\n",
            "Training mini-batch number 191000\n",
            "Training mini-batch number 192000\n",
            "Training mini-batch number 193000\n",
            "Training mini-batch number 194000\n",
            "Epoch 38: validation accuracy 98.76%\n",
            "Training mini-batch number 195000\n",
            "Training mini-batch number 196000\n",
            "Training mini-batch number 197000\n",
            "Training mini-batch number 198000\n",
            "Training mini-batch number 199000\n",
            "Epoch 39: validation accuracy 98.75%\n",
            "Training mini-batch number 200000\n",
            "Training mini-batch number 201000\n",
            "Training mini-batch number 202000\n",
            "Training mini-batch number 203000\n",
            "Training mini-batch number 204000\n",
            "Epoch 40: validation accuracy 98.75%\n",
            "Training mini-batch number 205000\n",
            "Training mini-batch number 206000\n",
            "Training mini-batch number 207000\n",
            "Training mini-batch number 208000\n",
            "Training mini-batch number 209000\n",
            "Epoch 41: validation accuracy 98.76%\n",
            "Training mini-batch number 210000\n",
            "Training mini-batch number 211000\n",
            "Training mini-batch number 212000\n",
            "Training mini-batch number 213000\n",
            "Training mini-batch number 214000\n",
            "Epoch 42: validation accuracy 98.76%\n",
            "Training mini-batch number 215000\n",
            "Training mini-batch number 216000\n",
            "Training mini-batch number 217000\n",
            "Training mini-batch number 218000\n",
            "Training mini-batch number 219000\n",
            "Epoch 43: validation accuracy 98.77%\n",
            "Training mini-batch number 220000\n",
            "Training mini-batch number 221000\n",
            "Training mini-batch number 222000\n",
            "Training mini-batch number 223000\n",
            "Training mini-batch number 224000\n",
            "Epoch 44: validation accuracy 98.77%\n",
            "Training mini-batch number 225000\n",
            "Training mini-batch number 226000\n",
            "Training mini-batch number 227000\n",
            "Training mini-batch number 228000\n",
            "Training mini-batch number 229000\n",
            "Epoch 45: validation accuracy 98.77%\n",
            "Training mini-batch number 230000\n",
            "Training mini-batch number 231000\n",
            "Training mini-batch number 232000\n",
            "Training mini-batch number 233000\n",
            "Training mini-batch number 234000\n",
            "Epoch 46: validation accuracy 98.78%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.90%\n",
            "Training mini-batch number 235000\n",
            "Training mini-batch number 236000\n",
            "Training mini-batch number 237000\n",
            "Training mini-batch number 238000\n",
            "Training mini-batch number 239000\n",
            "Epoch 47: validation accuracy 98.79%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.90%\n",
            "Training mini-batch number 240000\n",
            "Training mini-batch number 241000\n",
            "Training mini-batch number 242000\n",
            "Training mini-batch number 243000\n",
            "Training mini-batch number 244000\n",
            "Epoch 48: validation accuracy 98.79%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.90%\n",
            "Training mini-batch number 245000\n",
            "Training mini-batch number 246000\n",
            "Training mini-batch number 247000\n",
            "Training mini-batch number 248000\n",
            "Training mini-batch number 249000\n",
            "Epoch 49: validation accuracy 98.78%\n",
            "Training mini-batch number 250000\n",
            "Training mini-batch number 251000\n",
            "Training mini-batch number 252000\n",
            "Training mini-batch number 253000\n",
            "Training mini-batch number 254000\n",
            "Epoch 50: validation accuracy 98.80%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.92%\n",
            "Training mini-batch number 255000\n",
            "Training mini-batch number 256000\n",
            "Training mini-batch number 257000\n",
            "Training mini-batch number 258000\n",
            "Training mini-batch number 259000\n",
            "Epoch 51: validation accuracy 98.80%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.93%\n",
            "Training mini-batch number 260000\n",
            "Training mini-batch number 261000\n",
            "Training mini-batch number 262000\n",
            "Training mini-batch number 263000\n",
            "Training mini-batch number 264000\n",
            "Epoch 52: validation accuracy 98.80%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.95%\n",
            "Training mini-batch number 265000\n",
            "Training mini-batch number 266000\n",
            "Training mini-batch number 267000\n",
            "Training mini-batch number 268000\n",
            "Training mini-batch number 269000\n",
            "Epoch 53: validation accuracy 98.81%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.95%\n",
            "Training mini-batch number 270000\n",
            "Training mini-batch number 271000\n",
            "Training mini-batch number 272000\n",
            "Training mini-batch number 273000\n",
            "Training mini-batch number 274000\n",
            "Epoch 54: validation accuracy 98.81%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.95%\n",
            "Training mini-batch number 275000\n",
            "Training mini-batch number 276000\n",
            "Training mini-batch number 277000\n",
            "Training mini-batch number 278000\n",
            "Training mini-batch number 279000\n",
            "Epoch 55: validation accuracy 98.80%\n",
            "Training mini-batch number 280000\n",
            "Training mini-batch number 281000\n",
            "Training mini-batch number 282000\n",
            "Training mini-batch number 283000\n",
            "Training mini-batch number 284000\n",
            "Epoch 56: validation accuracy 98.80%\n",
            "Training mini-batch number 285000\n",
            "Training mini-batch number 286000\n",
            "Training mini-batch number 287000\n",
            "Training mini-batch number 288000\n",
            "Training mini-batch number 289000\n",
            "Epoch 57: validation accuracy 98.82%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.95%\n",
            "Training mini-batch number 290000\n",
            "Training mini-batch number 291000\n",
            "Training mini-batch number 292000\n",
            "Training mini-batch number 293000\n",
            "Training mini-batch number 294000\n",
            "Epoch 58: validation accuracy 98.82%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.93%\n",
            "Training mini-batch number 295000\n",
            "Training mini-batch number 296000\n",
            "Training mini-batch number 297000\n",
            "Training mini-batch number 298000\n",
            "Training mini-batch number 299000\n",
            "Epoch 59: validation accuracy 98.82%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.92%\n",
            "Finished training network.\n",
            "Best validation accuracy of 98.82% obtained at iteration 299999\n",
            "Corresponding test accuracy of 98.92%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "J0iv4kjOnjWr"
      },
      "source": [
        "# remove fully connected layer: \n",
        "net = Network([\n",
        "        ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28),\n",
        "                     filter_shape=(20, 1, 5, 5), \n",
        "                     poolsize=(2, 2)),\n",
        "        # FullyConnectedLayer(n_in=20*12*12, n_out=100),\n",
        "        SoftmaxLayer(n_in=20*12*12, n_out=10)], \n",
        "              mini_batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kqPy0FHnjWr",
        "outputId": "3eb3327a-d594-4d36-961a-fd3096df53fd"
      },
      "source": [
        "net.SGD(training_data, 60, mini_batch_size, 0.1, validation_data, test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training mini-batch number 0\n",
            "Training mini-batch number 1000\n",
            "Training mini-batch number 2000\n",
            "Training mini-batch number 3000\n",
            "Training mini-batch number 4000\n",
            "Epoch 0: validation accuracy 94.05%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 93.53%\n",
            "Training mini-batch number 5000\n",
            "Training mini-batch number 6000\n",
            "Training mini-batch number 7000\n",
            "Training mini-batch number 8000\n",
            "Training mini-batch number 9000\n",
            "Epoch 1: validation accuracy 96.88%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 96.58%\n",
            "Training mini-batch number 10000\n",
            "Training mini-batch number 11000\n",
            "Training mini-batch number 12000\n",
            "Training mini-batch number 13000\n",
            "Training mini-batch number 14000\n",
            "Epoch 2: validation accuracy 97.62%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 97.52%\n",
            "Training mini-batch number 15000\n",
            "Training mini-batch number 16000\n",
            "Training mini-batch number 17000\n",
            "Training mini-batch number 18000\n",
            "Training mini-batch number 19000\n",
            "Epoch 3: validation accuracy 98.00%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 97.80%\n",
            "Training mini-batch number 20000\n",
            "Training mini-batch number 21000\n",
            "Training mini-batch number 22000\n",
            "Training mini-batch number 23000\n",
            "Training mini-batch number 24000\n",
            "Epoch 4: validation accuracy 98.16%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 97.99%\n",
            "Training mini-batch number 25000\n",
            "Training mini-batch number 26000\n",
            "Training mini-batch number 27000\n",
            "Training mini-batch number 28000\n",
            "Training mini-batch number 29000\n",
            "Epoch 5: validation accuracy 98.22%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.10%\n",
            "Training mini-batch number 30000\n",
            "Training mini-batch number 31000\n",
            "Training mini-batch number 32000\n",
            "Training mini-batch number 33000\n",
            "Training mini-batch number 34000\n",
            "Epoch 6: validation accuracy 98.30%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.19%\n",
            "Training mini-batch number 35000\n",
            "Training mini-batch number 36000\n",
            "Training mini-batch number 37000\n",
            "Training mini-batch number 38000\n",
            "Training mini-batch number 39000\n",
            "Epoch 7: validation accuracy 98.37%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.20%\n",
            "Training mini-batch number 40000\n",
            "Training mini-batch number 41000\n",
            "Training mini-batch number 42000\n",
            "Training mini-batch number 43000\n",
            "Training mini-batch number 44000\n",
            "Epoch 8: validation accuracy 98.43%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.26%\n",
            "Training mini-batch number 45000\n",
            "Training mini-batch number 46000\n",
            "Training mini-batch number 47000\n",
            "Training mini-batch number 48000\n",
            "Training mini-batch number 49000\n",
            "Epoch 9: validation accuracy 98.46%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.33%\n",
            "Training mini-batch number 50000\n",
            "Training mini-batch number 51000\n",
            "Training mini-batch number 52000\n",
            "Training mini-batch number 53000\n",
            "Training mini-batch number 54000\n",
            "Epoch 10: validation accuracy 98.44%\n",
            "Training mini-batch number 55000\n",
            "Training mini-batch number 56000\n",
            "Training mini-batch number 57000\n",
            "Training mini-batch number 58000\n",
            "Training mini-batch number 59000\n",
            "Epoch 11: validation accuracy 98.50%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.34%\n",
            "Training mini-batch number 60000\n",
            "Training mini-batch number 61000\n",
            "Training mini-batch number 62000\n",
            "Training mini-batch number 63000\n",
            "Training mini-batch number 64000\n",
            "Epoch 12: validation accuracy 98.54%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.34%\n",
            "Training mini-batch number 65000\n",
            "Training mini-batch number 66000\n",
            "Training mini-batch number 67000\n",
            "Training mini-batch number 68000\n",
            "Training mini-batch number 69000\n",
            "Epoch 13: validation accuracy 98.55%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.36%\n",
            "Training mini-batch number 70000\n",
            "Training mini-batch number 71000\n",
            "Training mini-batch number 72000\n",
            "Training mini-batch number 73000\n",
            "Training mini-batch number 74000\n",
            "Epoch 14: validation accuracy 98.54%\n",
            "Training mini-batch number 75000\n",
            "Training mini-batch number 76000\n",
            "Training mini-batch number 77000\n",
            "Training mini-batch number 78000\n",
            "Training mini-batch number 79000\n",
            "Epoch 15: validation accuracy 98.57%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.39%\n",
            "Training mini-batch number 80000\n",
            "Training mini-batch number 81000\n",
            "Training mini-batch number 82000\n",
            "Training mini-batch number 83000\n",
            "Training mini-batch number 84000\n",
            "Epoch 16: validation accuracy 98.55%\n",
            "Training mini-batch number 85000\n",
            "Training mini-batch number 86000\n",
            "Training mini-batch number 87000\n",
            "Training mini-batch number 88000\n",
            "Training mini-batch number 89000\n",
            "Epoch 17: validation accuracy 98.57%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.43%\n",
            "Training mini-batch number 90000\n",
            "Training mini-batch number 91000\n",
            "Training mini-batch number 92000\n",
            "Training mini-batch number 93000\n",
            "Training mini-batch number 94000\n",
            "Epoch 18: validation accuracy 98.60%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.42%\n",
            "Training mini-batch number 95000\n",
            "Training mini-batch number 96000\n",
            "Training mini-batch number 97000\n",
            "Training mini-batch number 98000\n",
            "Training mini-batch number 99000\n",
            "Epoch 19: validation accuracy 98.61%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.43%\n",
            "Training mini-batch number 100000\n",
            "Training mini-batch number 101000\n",
            "Training mini-batch number 102000\n",
            "Training mini-batch number 103000\n",
            "Training mini-batch number 104000\n",
            "Epoch 20: validation accuracy 98.61%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.43%\n",
            "Training mini-batch number 105000\n",
            "Training mini-batch number 106000\n",
            "Training mini-batch number 107000\n",
            "Training mini-batch number 108000\n",
            "Training mini-batch number 109000\n",
            "Epoch 21: validation accuracy 98.61%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.47%\n",
            "Training mini-batch number 110000\n",
            "Training mini-batch number 111000\n",
            "Training mini-batch number 112000\n",
            "Training mini-batch number 113000\n",
            "Training mini-batch number 114000\n",
            "Epoch 22: validation accuracy 98.60%\n",
            "Training mini-batch number 115000\n",
            "Training mini-batch number 116000\n",
            "Training mini-batch number 117000\n",
            "Training mini-batch number 118000\n",
            "Training mini-batch number 119000\n",
            "Epoch 23: validation accuracy 98.60%\n",
            "Training mini-batch number 120000\n",
            "Training mini-batch number 121000\n",
            "Training mini-batch number 122000\n",
            "Training mini-batch number 123000\n",
            "Training mini-batch number 124000\n",
            "Epoch 24: validation accuracy 98.59%\n",
            "Training mini-batch number 125000\n",
            "Training mini-batch number 126000\n",
            "Training mini-batch number 127000\n",
            "Training mini-batch number 128000\n",
            "Training mini-batch number 129000\n",
            "Epoch 25: validation accuracy 98.58%\n",
            "Training mini-batch number 130000\n",
            "Training mini-batch number 131000\n",
            "Training mini-batch number 132000\n",
            "Training mini-batch number 133000\n",
            "Training mini-batch number 134000\n",
            "Epoch 26: validation accuracy 98.59%\n",
            "Training mini-batch number 135000\n",
            "Training mini-batch number 136000\n",
            "Training mini-batch number 137000\n",
            "Training mini-batch number 138000\n",
            "Training mini-batch number 139000\n",
            "Epoch 27: validation accuracy 98.58%\n",
            "Training mini-batch number 140000\n",
            "Training mini-batch number 141000\n",
            "Training mini-batch number 142000\n",
            "Training mini-batch number 143000\n",
            "Training mini-batch number 144000\n",
            "Epoch 28: validation accuracy 98.59%\n",
            "Training mini-batch number 145000\n",
            "Training mini-batch number 146000\n",
            "Training mini-batch number 147000\n",
            "Training mini-batch number 148000\n",
            "Training mini-batch number 149000\n",
            "Epoch 29: validation accuracy 98.58%\n",
            "Training mini-batch number 150000\n",
            "Training mini-batch number 151000\n",
            "Training mini-batch number 152000\n",
            "Training mini-batch number 153000\n",
            "Training mini-batch number 154000\n",
            "Epoch 30: validation accuracy 98.58%\n",
            "Training mini-batch number 155000\n",
            "Training mini-batch number 156000\n",
            "Training mini-batch number 157000\n",
            "Training mini-batch number 158000\n",
            "Training mini-batch number 159000\n",
            "Epoch 31: validation accuracy 98.57%\n",
            "Training mini-batch number 160000\n",
            "Training mini-batch number 161000\n",
            "Training mini-batch number 162000\n",
            "Training mini-batch number 163000\n",
            "Training mini-batch number 164000\n",
            "Epoch 32: validation accuracy 98.57%\n",
            "Training mini-batch number 165000\n",
            "Training mini-batch number 166000\n",
            "Training mini-batch number 167000\n",
            "Training mini-batch number 168000\n",
            "Training mini-batch number 169000\n",
            "Epoch 33: validation accuracy 98.57%\n",
            "Training mini-batch number 170000\n",
            "Training mini-batch number 171000\n",
            "Training mini-batch number 172000\n",
            "Training mini-batch number 173000\n",
            "Training mini-batch number 174000\n",
            "Epoch 34: validation accuracy 98.55%\n",
            "Training mini-batch number 175000\n",
            "Training mini-batch number 176000\n",
            "Training mini-batch number 177000\n",
            "Training mini-batch number 178000\n",
            "Training mini-batch number 179000\n",
            "Epoch 35: validation accuracy 98.53%\n",
            "Training mini-batch number 180000\n",
            "Training mini-batch number 181000\n",
            "Training mini-batch number 182000\n",
            "Training mini-batch number 183000\n",
            "Training mini-batch number 184000\n",
            "Epoch 36: validation accuracy 98.50%\n",
            "Training mini-batch number 185000\n",
            "Training mini-batch number 186000\n",
            "Training mini-batch number 187000\n",
            "Training mini-batch number 188000\n",
            "Training mini-batch number 189000\n",
            "Epoch 37: validation accuracy 98.50%\n",
            "Training mini-batch number 190000\n",
            "Training mini-batch number 191000\n",
            "Training mini-batch number 192000\n",
            "Training mini-batch number 193000\n",
            "Training mini-batch number 194000\n",
            "Epoch 38: validation accuracy 98.53%\n",
            "Training mini-batch number 195000\n",
            "Training mini-batch number 196000\n",
            "Training mini-batch number 197000\n",
            "Training mini-batch number 198000\n",
            "Training mini-batch number 199000\n",
            "Epoch 39: validation accuracy 98.54%\n",
            "Training mini-batch number 200000\n",
            "Training mini-batch number 201000\n",
            "Training mini-batch number 202000\n",
            "Training mini-batch number 203000\n",
            "Training mini-batch number 204000\n",
            "Epoch 40: validation accuracy 98.53%\n",
            "Training mini-batch number 205000\n",
            "Training mini-batch number 206000\n",
            "Training mini-batch number 207000\n",
            "Training mini-batch number 208000\n",
            "Training mini-batch number 209000\n",
            "Epoch 41: validation accuracy 98.53%\n",
            "Training mini-batch number 210000\n",
            "Training mini-batch number 211000\n",
            "Training mini-batch number 212000\n",
            "Training mini-batch number 213000\n",
            "Training mini-batch number 214000\n",
            "Epoch 42: validation accuracy 98.54%\n",
            "Training mini-batch number 215000\n",
            "Training mini-batch number 216000\n",
            "Training mini-batch number 217000\n",
            "Training mini-batch number 218000\n",
            "Training mini-batch number 219000\n",
            "Epoch 43: validation accuracy 98.54%\n",
            "Training mini-batch number 220000\n",
            "Training mini-batch number 221000\n",
            "Training mini-batch number 222000\n",
            "Training mini-batch number 223000\n",
            "Training mini-batch number 224000\n",
            "Epoch 44: validation accuracy 98.55%\n",
            "Training mini-batch number 225000\n",
            "Training mini-batch number 226000\n",
            "Training mini-batch number 227000\n",
            "Training mini-batch number 228000\n",
            "Training mini-batch number 229000\n",
            "Epoch 45: validation accuracy 98.52%\n",
            "Training mini-batch number 230000\n",
            "Training mini-batch number 231000\n",
            "Training mini-batch number 232000\n",
            "Training mini-batch number 233000\n",
            "Training mini-batch number 234000\n",
            "Epoch 46: validation accuracy 98.53%\n",
            "Training mini-batch number 235000\n",
            "Training mini-batch number 236000\n",
            "Training mini-batch number 237000\n",
            "Training mini-batch number 238000\n",
            "Training mini-batch number 239000\n",
            "Epoch 47: validation accuracy 98.51%\n",
            "Training mini-batch number 240000\n",
            "Training mini-batch number 241000\n",
            "Training mini-batch number 242000\n",
            "Training mini-batch number 243000\n",
            "Training mini-batch number 244000\n",
            "Epoch 48: validation accuracy 98.51%\n",
            "Training mini-batch number 245000\n",
            "Training mini-batch number 246000\n",
            "Training mini-batch number 247000\n",
            "Training mini-batch number 248000\n",
            "Training mini-batch number 249000\n",
            "Epoch 49: validation accuracy 98.51%\n",
            "Training mini-batch number 250000\n",
            "Training mini-batch number 251000\n",
            "Training mini-batch number 252000\n",
            "Training mini-batch number 253000\n",
            "Training mini-batch number 254000\n",
            "Epoch 50: validation accuracy 98.52%\n",
            "Training mini-batch number 255000\n",
            "Training mini-batch number 256000\n",
            "Training mini-batch number 257000\n",
            "Training mini-batch number 258000\n",
            "Training mini-batch number 259000\n",
            "Epoch 51: validation accuracy 98.51%\n",
            "Training mini-batch number 260000\n",
            "Training mini-batch number 261000\n",
            "Training mini-batch number 262000\n",
            "Training mini-batch number 263000\n",
            "Training mini-batch number 264000\n",
            "Epoch 52: validation accuracy 98.52%\n",
            "Training mini-batch number 265000\n",
            "Training mini-batch number 266000\n",
            "Training mini-batch number 267000\n",
            "Training mini-batch number 268000\n",
            "Training mini-batch number 269000\n",
            "Epoch 53: validation accuracy 98.52%\n",
            "Training mini-batch number 270000\n",
            "Training mini-batch number 271000\n",
            "Training mini-batch number 272000\n",
            "Training mini-batch number 273000\n",
            "Training mini-batch number 274000\n",
            "Epoch 54: validation accuracy 98.53%\n",
            "Training mini-batch number 275000\n",
            "Training mini-batch number 276000\n",
            "Training mini-batch number 277000\n",
            "Training mini-batch number 278000\n",
            "Training mini-batch number 279000\n",
            "Epoch 55: validation accuracy 98.52%\n",
            "Training mini-batch number 280000\n",
            "Training mini-batch number 281000\n",
            "Training mini-batch number 282000\n",
            "Training mini-batch number 283000\n",
            "Training mini-batch number 284000\n",
            "Epoch 56: validation accuracy 98.53%\n",
            "Training mini-batch number 285000\n",
            "Training mini-batch number 286000\n",
            "Training mini-batch number 287000\n",
            "Training mini-batch number 288000\n",
            "Training mini-batch number 289000\n",
            "Epoch 57: validation accuracy 98.53%\n",
            "Training mini-batch number 290000\n",
            "Training mini-batch number 291000\n",
            "Training mini-batch number 292000\n",
            "Training mini-batch number 293000\n",
            "Training mini-batch number 294000\n",
            "Epoch 58: validation accuracy 98.51%\n",
            "Training mini-batch number 295000\n",
            "Training mini-batch number 296000\n",
            "Training mini-batch number 297000\n",
            "Training mini-batch number 298000\n",
            "Training mini-batch number 299000\n",
            "Epoch 59: validation accuracy 98.51%\n",
            "Finished training network.\n",
            "Best validation accuracy of 98.61% obtained at iteration 109999\n",
            "Corresponding test accuracy of 98.47%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C321uzQXnjWs"
      },
      "source": [
        "# add a second convolutional layer: \n",
        "net = Network([\n",
        "        ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28),\n",
        "                     filter_shape=(20, 1, 5, 5),\n",
        "                     poolsize=(2, 2)),\n",
        "        ConvPoolLayer(image_shape=(mini_batch_size, 20, 12, 12),\n",
        "                     filter_shape=(40, 20, 5, 5),\n",
        "                     poolsize=(2, 2)),\n",
        "        FullyConnectedLayer(n_in=40*4*4, n_out=100),\n",
        "        SoftmaxLayer(n_in=100, n_out=10)\n",
        "    ], mini_batch_size)"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "GY1QKSzCnjWs",
        "outputId": "8e6bfa0e-3106-4ba6-9eb4-bd34c6d4e960"
      },
      "source": [
        "net.SGD(training_data, 60, mini_batch_size, 0.1, validation_data, test_data)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training mini-batch number 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-144-a1596d9ec213>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-136-1703581f7a36>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(self, training_data, epochs, mini_batch_size, eta, validation_data, test_data, lmbda)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training mini-batch number {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mcost_ij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnum_training_batches\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                     validation_accuracy = np.mean(\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "bQawS6fZnjWs"
      },
      "source": [
        "# switch from sigmoid to tanh neurons: \n",
        "net = Network([\n",
        "        ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28),\n",
        "                     filter_shape=(20, 1, 5, 5),\n",
        "                     poolsize=(2, 2),\n",
        "                     activation_fn=tanh),\n",
        "        ConvPoolLayer(image_shape=(mini_batch_size, 20, 12, 12),\n",
        "                     filter_shape=(40, 20, 5, 5),\n",
        "                     poolsize=(2, 2),\n",
        "                     activation_fn=tanh),\n",
        "        FullyConnectedLayer(n_in=40*4*4, n_out=100, activation_fn=tanh),\n",
        "        SoftmaxLayer(n_in=100, n_out=10)\n",
        "    ], mini_batch_size)"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "pnLBbSAYnjWs",
        "outputId": "2abe176d-aba1-47e2-9714-0e4e5ed7af7f"
      },
      "source": [
        "net.SGD(training_data, 60, mini_batch_size, 0.1, validation_data, test_data)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-149-a1596d9ec213>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-136-1703581f7a36>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(self, training_data, epochs, mini_batch_size, eta, validation_data, test_data, lmbda)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mtraining_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mtraining_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             })\n\u001b[1;32m     53\u001b[0m         validate_mb_accuracy = theano.function(\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function.pyc\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[1;32m    315\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                    output_keys=output_keys)\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/pfunc.pyc\u001b[0m in \u001b[0;36mpfunc\u001b[0;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m    484\u001b[0m                          \u001b[0maccept_inplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                          \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m                          output_keys=output_keys)\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36morig_function\u001b[0;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m   1839\u001b[0m                   name=name)\n\u001b[1;32m   1840\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchange_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_test_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1841\u001b[0;31m             \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1842\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, input_storage, trustme, storage_map)\u001b[0m\n\u001b[1;32m   1713\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m             _fn, _i, _o = self.linker.make_thunk(\n\u001b[0;32m-> 1715\u001b[0;31m                 input_storage=input_storage_lists, storage_map=storage_map)\n\u001b[0m\u001b[1;32m   1716\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1717\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlimit_orig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, input_storage, output_storage, storage_map)\u001b[0m\n\u001b[1;32m    697\u001b[0m         return self.make_all(input_storage=input_storage,\n\u001b[1;32m    698\u001b[0m                              \u001b[0moutput_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_storage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                              storage_map=storage_map)[:3]\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/gof/vm.pyc\u001b[0m in \u001b[0;36mmake_all\u001b[0;34m(self, profiler, input_storage, output_storage, storage_map)\u001b[0m\n\u001b[1;32m   1089\u001b[0m                                                  \u001b[0mcompute_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m                                                  \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m                                                  impl=impl))\n\u001b[0m\u001b[1;32m   1092\u001b[0m                 \u001b[0mlinker_make_thunk_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mthunk_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lazy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, node, storage_map, compute_map, no_recycling, impl)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m                 return self.make_c_thunk(node, storage_map, compute_map,\n\u001b[0;32m--> 955\u001b[0;31m                                          no_recycling)\n\u001b[0m\u001b[1;32m    956\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNotImplementedError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethodNotDefined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m                 \u001b[0;31m# We requested the c code, so don't catch the error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mmake_c_thunk\u001b[0;34m(self, node, storage_map, compute_map, no_recycling)\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Trying CLinker.make_thunk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m         outputs = cl.make_thunk(input_storage=node_input_storage,\n\u001b[0;32m--> 858\u001b[0;31m                                 output_storage=node_output_storage)\n\u001b[0m\u001b[1;32m    859\u001b[0m         \u001b[0mthunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_input_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_output_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/gof/cc.pyc\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, input_storage, output_storage, storage_map, keep_lock)\u001b[0m\n\u001b[1;32m   1215\u001b[0m         cthunk, module, in_storage, out_storage, error_storage = self.__compile__(\n\u001b[1;32m   1216\u001b[0m             \u001b[0minput_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m             keep_lock=keep_lock)\n\u001b[0m\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CThunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcthunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_tasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/gof/cc.pyc\u001b[0m in \u001b[0;36m__compile__\u001b[0;34m(self, input_storage, output_storage, storage_map, keep_lock)\u001b[0m\n\u001b[1;32m   1155\u001b[0m                                             \u001b[0moutput_storage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m                                             \u001b[0mstorage_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1157\u001b[0;31m                                             keep_lock=keep_lock)\n\u001b[0m\u001b[1;32m   1158\u001b[0m         return (thunk,\n\u001b[1;32m   1159\u001b[0m                 \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/gof/cc.pyc\u001b[0m in \u001b[0;36mcthunk_factory\u001b[0;34m(self, error_storage, in_storage, out_storage, storage_map, keep_lock)\u001b[0m\n\u001b[1;32m   1622\u001b[0m                 \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m             module = get_module_cache().module_from_key(\n\u001b[0;32m-> 1624\u001b[0;31m                 key=key, lnk=self, keep_lock=keep_lock)\n\u001b[0m\u001b[1;32m   1625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m         \u001b[0mvars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morphans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/gof/cmodule.pyc\u001b[0m in \u001b[0;36mmodule_from_key\u001b[0;34m(self, key, lnk, keep_lock)\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m                 \u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlimport_workdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlnk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_cmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m                 \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/gof/cc.pyc\u001b[0m in \u001b[0;36mcompile_cmodule\u001b[0;34m(self, location)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0mlib_dirs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib_dirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 \u001b[0mlibs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlibs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1527\u001b[0;31m                 preargs=preargs)\n\u001b[0m\u001b[1;32m   1528\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/gof/cmodule.pyc\u001b[0m in \u001b[0;36mcompile_str\u001b[0;34m(module_name, src_code, location, include_dirs, lib_dirs, libs, preargs, py_module, hide_symbols)\u001b[0m\n\u001b[1;32m   2349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2351\u001b[0;31m             \u001b[0mp_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_subprocess_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2352\u001b[0m             \u001b[0mcompile_stderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2353\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/misc/windows.pyc\u001b[0m in \u001b[0;36moutput_subprocess_Popen\u001b[0;34m(command, **params)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# we need to use communicate to make sure we don't deadlock around\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# the stdout/stderr pipe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_has_poll\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate_with_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate_with_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36m_communicate_with_poll\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1176\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mfd2file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "4sHS6w8SnjWt"
      },
      "source": [
        "# switch to ReLU neurons: \n",
        "net = Network([\n",
        "        ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28), \n",
        "                      filter_shape=(20, 1, 5, 5), \n",
        "                      poolsize=(2, 2), \n",
        "                      activation_fn=ReLU),\n",
        "        ConvPoolLayer(image_shape=(mini_batch_size, 20, 12, 12), \n",
        "                      filter_shape=(40, 20, 5, 5), \n",
        "                      poolsize=(2, 2), \n",
        "                      activation_fn=ReLU),\n",
        "        FullyConnectedLayer(n_in=40*4*4, n_out=100, activation_fn=ReLU),\n",
        "        SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZkzK7_wnjWt",
        "outputId": "9c8706f4-694d-4248-dd31-b5bea1f13509"
      },
      "source": [
        "# note the eta is an order of magnitude smaller, and lambda has been added as a parameter: \n",
        "net.SGD(training_data, 60, mini_batch_size, 0.03, \n",
        "            validation_data, test_data, lmbda=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training mini-batch number 0\n",
            "Training mini-batch number 1000\n",
            "Training mini-batch number 2000\n",
            "Training mini-batch number 3000\n",
            "Training mini-batch number 4000\n",
            "Epoch 0: validation accuracy 97.54%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 97.26%\n",
            "Training mini-batch number 5000\n",
            "Training mini-batch number 6000\n",
            "Training mini-batch number 7000\n",
            "Training mini-batch number 8000\n",
            "Training mini-batch number 9000\n",
            "Epoch 1: validation accuracy 98.05%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.00%\n",
            "Training mini-batch number 10000\n",
            "Training mini-batch number 11000\n",
            "Training mini-batch number 12000\n",
            "Training mini-batch number 13000\n",
            "Training mini-batch number 14000\n",
            "Epoch 2: validation accuracy 98.29%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.27%\n",
            "Training mini-batch number 15000\n",
            "Training mini-batch number 16000\n",
            "Training mini-batch number 17000\n",
            "Training mini-batch number 18000\n",
            "Training mini-batch number 19000\n",
            "Epoch 3: validation accuracy 98.57%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.57%\n",
            "Training mini-batch number 20000\n",
            "Training mini-batch number 21000\n",
            "Training mini-batch number 22000\n",
            "Training mini-batch number 23000\n",
            "Training mini-batch number 24000\n",
            "Epoch 4: validation accuracy 98.68%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.61%\n",
            "Training mini-batch number 25000\n",
            "Training mini-batch number 26000\n",
            "Training mini-batch number 27000\n",
            "Training mini-batch number 28000\n",
            "Training mini-batch number 29000\n",
            "Epoch 5: validation accuracy 98.82%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.77%\n",
            "Training mini-batch number 30000\n",
            "Training mini-batch number 31000\n",
            "Training mini-batch number 32000\n",
            "Training mini-batch number 33000\n",
            "Training mini-batch number 34000\n",
            "Epoch 6: validation accuracy 98.75%\n",
            "Training mini-batch number 35000\n",
            "Training mini-batch number 36000\n",
            "Training mini-batch number 37000\n",
            "Training mini-batch number 38000\n",
            "Training mini-batch number 39000\n",
            "Epoch 7: validation accuracy 98.71%\n",
            "Training mini-batch number 40000\n",
            "Training mini-batch number 41000\n",
            "Training mini-batch number 42000\n",
            "Training mini-batch number 43000\n",
            "Training mini-batch number 44000\n",
            "Epoch 8: validation accuracy 98.77%\n",
            "Training mini-batch number 45000\n",
            "Training mini-batch number 46000\n",
            "Training mini-batch number 47000\n",
            "Training mini-batch number 48000\n",
            "Training mini-batch number 49000\n",
            "Epoch 9: validation accuracy 98.90%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.76%\n",
            "Training mini-batch number 50000\n",
            "Training mini-batch number 51000\n",
            "Training mini-batch number 52000\n",
            "Training mini-batch number 53000\n",
            "Training mini-batch number 54000\n",
            "Epoch 10: validation accuracy 98.74%\n",
            "Training mini-batch number 55000\n",
            "Training mini-batch number 56000\n",
            "Training mini-batch number 57000\n",
            "Training mini-batch number 58000\n",
            "Training mini-batch number 59000\n",
            "Epoch 11: validation accuracy 98.74%\n",
            "Training mini-batch number 60000\n",
            "Training mini-batch number 61000\n",
            "Training mini-batch number 62000\n",
            "Training mini-batch number 63000\n",
            "Training mini-batch number 64000\n",
            "Epoch 12: validation accuracy 98.94%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.93%\n",
            "Training mini-batch number 65000\n",
            "Training mini-batch number 66000\n",
            "Training mini-batch number 67000\n",
            "Training mini-batch number 68000\n",
            "Training mini-batch number 69000\n",
            "Epoch 13: validation accuracy 98.93%\n",
            "Training mini-batch number 70000\n",
            "Training mini-batch number 71000\n",
            "Training mini-batch number 72000\n",
            "Training mini-batch number 73000\n",
            "Training mini-batch number 74000\n",
            "Epoch 14: validation accuracy 98.75%\n",
            "Training mini-batch number 75000\n",
            "Training mini-batch number 76000\n",
            "Training mini-batch number 77000\n",
            "Training mini-batch number 78000\n",
            "Training mini-batch number 79000\n",
            "Epoch 15: validation accuracy 98.77%\n",
            "Training mini-batch number 80000\n",
            "Training mini-batch number 81000\n",
            "Training mini-batch number 82000\n",
            "Training mini-batch number 83000\n",
            "Training mini-batch number 84000\n",
            "Epoch 16: validation accuracy 98.95%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.98%\n",
            "Training mini-batch number 85000\n",
            "Training mini-batch number 86000\n",
            "Training mini-batch number 87000\n",
            "Training mini-batch number 88000\n",
            "Training mini-batch number 89000\n",
            "Epoch 17: validation accuracy 98.91%\n",
            "Training mini-batch number 90000\n",
            "Training mini-batch number 91000\n",
            "Training mini-batch number 92000\n",
            "Training mini-batch number 93000\n",
            "Training mini-batch number 94000\n",
            "Epoch 18: validation accuracy 98.66%\n",
            "Training mini-batch number 95000\n",
            "Training mini-batch number 96000\n",
            "Training mini-batch number 97000\n",
            "Training mini-batch number 98000\n",
            "Training mini-batch number 99000\n",
            "Epoch 19: validation accuracy 98.80%\n",
            "Training mini-batch number 100000\n",
            "Training mini-batch number 101000\n",
            "Training mini-batch number 102000\n",
            "Training mini-batch number 103000\n",
            "Training mini-batch number 104000\n",
            "Epoch 20: validation accuracy 98.82%\n",
            "Training mini-batch number 105000\n",
            "Training mini-batch number 106000\n",
            "Training mini-batch number 107000\n",
            "Training mini-batch number 108000\n",
            "Training mini-batch number 109000\n",
            "Epoch 21: validation accuracy 98.84%\n",
            "Training mini-batch number 110000\n",
            "Training mini-batch number 111000\n",
            "Training mini-batch number 112000\n",
            "Training mini-batch number 113000\n",
            "Training mini-batch number 114000\n",
            "Epoch 22: validation accuracy 98.81%\n",
            "Training mini-batch number 115000\n",
            "Training mini-batch number 116000\n",
            "Training mini-batch number 117000\n",
            "Training mini-batch number 118000\n",
            "Training mini-batch number 119000\n",
            "Epoch 23: validation accuracy 99.01%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 99.00%\n",
            "Training mini-batch number 120000\n",
            "Training mini-batch number 121000\n",
            "Training mini-batch number 122000\n",
            "Training mini-batch number 123000\n",
            "Training mini-batch number 124000\n",
            "Epoch 24: validation accuracy 98.87%\n",
            "Training mini-batch number 125000\n",
            "Training mini-batch number 126000\n",
            "Training mini-batch number 127000\n",
            "Training mini-batch number 128000\n",
            "Training mini-batch number 129000\n",
            "Epoch 25: validation accuracy 99.04%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.99%\n",
            "Training mini-batch number 130000\n",
            "Training mini-batch number 131000\n",
            "Training mini-batch number 132000\n",
            "Training mini-batch number 133000\n",
            "Training mini-batch number 134000\n",
            "Epoch 26: validation accuracy 98.79%\n",
            "Training mini-batch number 135000\n",
            "Training mini-batch number 136000\n",
            "Training mini-batch number 137000\n",
            "Training mini-batch number 138000\n",
            "Training mini-batch number 139000\n",
            "Epoch 27: validation accuracy 99.02%\n",
            "Training mini-batch number 140000\n",
            "Training mini-batch number 141000\n",
            "Training mini-batch number 142000\n",
            "Training mini-batch number 143000\n",
            "Training mini-batch number 144000\n",
            "Epoch 28: validation accuracy 99.11%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 99.09%\n",
            "Training mini-batch number 145000\n",
            "Training mini-batch number 146000\n",
            "Training mini-batch number 147000\n",
            "Training mini-batch number 148000\n",
            "Training mini-batch number 149000\n",
            "Epoch 29: validation accuracy 99.20%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 99.09%\n",
            "Training mini-batch number 150000\n",
            "Training mini-batch number 151000\n",
            "Training mini-batch number 152000\n",
            "Training mini-batch number 153000\n",
            "Training mini-batch number 154000\n",
            "Epoch 30: validation accuracy 99.20%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 99.11%\n",
            "Training mini-batch number 155000\n",
            "Training mini-batch number 156000\n",
            "Training mini-batch number 157000\n",
            "Training mini-batch number 158000\n",
            "Training mini-batch number 159000\n",
            "Epoch 31: validation accuracy 99.19%\n",
            "Training mini-batch number 160000\n",
            "Training mini-batch number 161000\n",
            "Training mini-batch number 162000\n",
            "Training mini-batch number 163000\n",
            "Training mini-batch number 164000\n",
            "Epoch 32: validation accuracy 99.19%\n",
            "Training mini-batch number 165000\n",
            "Training mini-batch number 166000\n",
            "Training mini-batch number 167000\n",
            "Training mini-batch number 168000\n",
            "Training mini-batch number 169000\n",
            "Epoch 33: validation accuracy 99.19%\n",
            "Training mini-batch number 170000\n",
            "Training mini-batch number 171000\n",
            "Training mini-batch number 172000\n",
            "Training mini-batch number 173000\n",
            "Training mini-batch number 174000\n",
            "Epoch 34: validation accuracy 99.17%\n",
            "Training mini-batch number 175000\n",
            "Training mini-batch number 176000\n",
            "Training mini-batch number 177000\n",
            "Training mini-batch number 178000\n",
            "Training mini-batch number 179000\n",
            "Epoch 35: validation accuracy 99.18%\n",
            "Training mini-batch number 180000\n",
            "Training mini-batch number 181000\n",
            "Training mini-batch number 182000\n",
            "Training mini-batch number 183000\n",
            "Training mini-batch number 184000\n",
            "Epoch 36: validation accuracy 99.18%\n",
            "Training mini-batch number 185000\n",
            "Training mini-batch number 186000\n",
            "Training mini-batch number 187000\n",
            "Training mini-batch number 188000\n",
            "Training mini-batch number 189000\n",
            "Epoch 37: validation accuracy 99.18%\n",
            "Training mini-batch number 190000\n",
            "Training mini-batch number 191000\n",
            "Training mini-batch number 192000\n",
            "Training mini-batch number 193000\n",
            "Training mini-batch number 194000\n",
            "Epoch 38: validation accuracy 99.17%\n",
            "Training mini-batch number 195000\n",
            "Training mini-batch number 196000\n",
            "Training mini-batch number 197000\n",
            "Training mini-batch number 198000\n",
            "Training mini-batch number 199000\n",
            "Epoch 39: validation accuracy 99.17%\n",
            "Training mini-batch number 200000\n",
            "Training mini-batch number 201000\n",
            "Training mini-batch number 202000\n",
            "Training mini-batch number 203000\n",
            "Training mini-batch number 204000\n",
            "Epoch 40: validation accuracy 99.18%\n",
            "Training mini-batch number 205000\n",
            "Training mini-batch number 206000\n",
            "Training mini-batch number 207000\n",
            "Training mini-batch number 208000\n",
            "Training mini-batch number 209000\n",
            "Epoch 41: validation accuracy 99.17%\n",
            "Training mini-batch number 210000\n",
            "Training mini-batch number 211000\n",
            "Training mini-batch number 212000\n",
            "Training mini-batch number 213000\n",
            "Training mini-batch number 214000\n",
            "Epoch 42: validation accuracy 99.18%\n",
            "Training mini-batch number 215000\n",
            "Training mini-batch number 216000\n",
            "Training mini-batch number 217000\n",
            "Training mini-batch number 218000\n",
            "Training mini-batch number 219000\n",
            "Epoch 43: validation accuracy 99.16%\n",
            "Training mini-batch number 220000\n",
            "Training mini-batch number 221000\n",
            "Training mini-batch number 222000\n",
            "Training mini-batch number 223000\n",
            "Training mini-batch number 224000\n",
            "Epoch 44: validation accuracy 99.17%\n",
            "Training mini-batch number 225000\n",
            "Training mini-batch number 226000\n",
            "Training mini-batch number 227000\n",
            "Training mini-batch number 228000\n",
            "Training mini-batch number 229000\n",
            "Epoch 45: validation accuracy 99.15%\n",
            "Training mini-batch number 230000\n",
            "Training mini-batch number 231000\n",
            "Training mini-batch number 232000\n",
            "Training mini-batch number 233000\n",
            "Training mini-batch number 234000\n",
            "Epoch 46: validation accuracy 99.15%\n",
            "Training mini-batch number 235000\n",
            "Training mini-batch number 236000\n",
            "Training mini-batch number 237000\n",
            "Training mini-batch number 238000\n",
            "Training mini-batch number 239000\n",
            "Epoch 47: validation accuracy 99.15%\n",
            "Training mini-batch number 240000\n",
            "Training mini-batch number 241000\n",
            "Training mini-batch number 242000\n",
            "Training mini-batch number 243000\n",
            "Training mini-batch number 244000\n",
            "Epoch 48: validation accuracy 99.16%\n",
            "Training mini-batch number 245000\n",
            "Training mini-batch number 246000\n",
            "Training mini-batch number 247000\n",
            "Training mini-batch number 248000\n",
            "Training mini-batch number 249000\n",
            "Epoch 49: validation accuracy 99.17%\n",
            "Training mini-batch number 250000\n",
            "Training mini-batch number 251000\n",
            "Training mini-batch number 252000\n",
            "Training mini-batch number 253000\n",
            "Training mini-batch number 254000\n",
            "Epoch 50: validation accuracy 99.17%\n",
            "Training mini-batch number 255000\n",
            "Training mini-batch number 256000\n",
            "Training mini-batch number 257000\n",
            "Training mini-batch number 258000\n",
            "Training mini-batch number 259000\n",
            "Epoch 51: validation accuracy 99.17%\n",
            "Training mini-batch number 260000\n",
            "Training mini-batch number 261000\n",
            "Training mini-batch number 262000\n",
            "Training mini-batch number 263000\n",
            "Training mini-batch number 264000\n",
            "Epoch 52: validation accuracy 99.17%\n",
            "Training mini-batch number 265000\n",
            "Training mini-batch number 266000\n",
            "Training mini-batch number 267000\n",
            "Training mini-batch number 268000\n",
            "Training mini-batch number 269000\n",
            "Epoch 53: validation accuracy 99.17%\n",
            "Training mini-batch number 270000\n",
            "Training mini-batch number 271000\n",
            "Training mini-batch number 272000\n",
            "Training mini-batch number 273000\n",
            "Training mini-batch number 274000\n",
            "Epoch 54: validation accuracy 99.18%\n",
            "Training mini-batch number 275000\n",
            "Training mini-batch number 276000\n",
            "Training mini-batch number 277000\n",
            "Training mini-batch number 278000\n",
            "Training mini-batch number 279000\n",
            "Epoch 55: validation accuracy 99.18%\n",
            "Training mini-batch number 280000\n",
            "Training mini-batch number 281000\n",
            "Training mini-batch number 282000\n",
            "Training mini-batch number 283000\n",
            "Training mini-batch number 284000\n",
            "Epoch 56: validation accuracy 99.18%\n",
            "Training mini-batch number 285000\n",
            "Training mini-batch number 286000\n",
            "Training mini-batch number 287000\n",
            "Training mini-batch number 288000\n",
            "Training mini-batch number 289000\n",
            "Epoch 57: validation accuracy 99.18%\n",
            "Training mini-batch number 290000\n",
            "Training mini-batch number 291000\n",
            "Training mini-batch number 292000\n",
            "Training mini-batch number 293000\n",
            "Training mini-batch number 294000\n",
            "Epoch 58: validation accuracy 99.18%\n",
            "Training mini-batch number 295000\n",
            "Training mini-batch number 296000\n",
            "Training mini-batch number 297000\n",
            "Training mini-batch number 298000\n",
            "Training mini-batch number 299000\n",
            "Epoch 59: validation accuracy 99.18%\n",
            "Finished training network.\n",
            "Best validation accuracy of 99.20% obtained at iteration 154999\n",
            "Corresponding test accuracy of 99.11%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "kQPGnqQAnjWt"
      },
      "source": [
        "expanded_training_data, _, _ = load_data_shared(\"../../neural-networks-and-deep-learning/data/mnist_expanded.pkl.gz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "IBk3Q-konjWu"
      },
      "source": [
        "net = Network([\n",
        "        ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28), \n",
        "                      filter_shape=(20, 1, 5, 5), \n",
        "                      poolsize=(2, 2), \n",
        "                      activation_fn=ReLU),\n",
        "        ConvPoolLayer(image_shape=(mini_batch_size, 20, 12, 12), \n",
        "                      filter_shape=(40, 20, 5, 5), \n",
        "                      poolsize=(2, 2), \n",
        "                      activation_fn=ReLU),\n",
        "        FullyConnectedLayer(n_in=40*4*4, n_out=100, activation_fn=ReLU),\n",
        "        SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wT8Vy8XjnjWu",
        "outputId": "e9774c3d-ddb3-41d0-ab12-957c5a362b9b"
      },
      "source": [
        "net.SGD(expanded_training_data, 60, mini_batch_size, 0.03, \n",
        "            validation_data, test_data, lmbda=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training mini-batch number 0\n",
            "Training mini-batch number 1000\n",
            "Training mini-batch number 2000\n",
            "Training mini-batch number 3000\n",
            "Training mini-batch number 4000\n",
            "Training mini-batch number 5000\n",
            "Training mini-batch number 6000\n",
            "Training mini-batch number 7000\n",
            "Training mini-batch number 8000\n",
            "Training mini-batch number 9000\n",
            "Training mini-batch number 10000\n",
            "Training mini-batch number 11000\n",
            "Training mini-batch number 12000\n",
            "Training mini-batch number 13000\n",
            "Training mini-batch number 14000\n",
            "Training mini-batch number 15000\n",
            "Training mini-batch number 16000\n",
            "Training mini-batch number 17000\n",
            "Training mini-batch number 18000\n",
            "Training mini-batch number 19000\n",
            "Training mini-batch number 20000\n",
            "Training mini-batch number 21000\n",
            "Training mini-batch number 22000\n",
            "Training mini-batch number 23000\n",
            "Training mini-batch number 24000\n",
            "Epoch 0: validation accuracy 96.92%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 97.05%\n",
            "Training mini-batch number 25000\n",
            "Training mini-batch number 26000\n",
            "Training mini-batch number 27000\n",
            "Training mini-batch number 28000\n",
            "Training mini-batch number 29000\n",
            "Training mini-batch number 30000\n",
            "Training mini-batch number 31000\n",
            "Training mini-batch number 32000\n",
            "Training mini-batch number 33000\n",
            "Training mini-batch number 34000\n",
            "Training mini-batch number 35000\n",
            "Training mini-batch number 36000\n",
            "Training mini-batch number 37000\n",
            "Training mini-batch number 38000\n",
            "Training mini-batch number 39000\n",
            "Training mini-batch number 40000\n",
            "Training mini-batch number 41000\n",
            "Training mini-batch number 42000\n",
            "Training mini-batch number 43000\n",
            "Training mini-batch number 44000\n",
            "Training mini-batch number 45000\n",
            "Training mini-batch number 46000\n",
            "Training mini-batch number 47000\n",
            "Training mini-batch number 48000\n",
            "Training mini-batch number 49000\n",
            "Epoch 1: validation accuracy 98.79%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.87%\n",
            "Training mini-batch number 50000\n",
            "Training mini-batch number 51000\n",
            "Training mini-batch number 52000\n",
            "Training mini-batch number 53000\n",
            "Training mini-batch number 54000\n",
            "Training mini-batch number 55000\n",
            "Training mini-batch number 56000\n",
            "Training mini-batch number 57000\n",
            "Training mini-batch number 58000\n",
            "Training mini-batch number 59000\n",
            "Training mini-batch number 60000\n",
            "Training mini-batch number 61000\n",
            "Training mini-batch number 62000\n",
            "Training mini-batch number 63000\n",
            "Training mini-batch number 64000\n",
            "Training mini-batch number 65000\n",
            "Training mini-batch number 66000\n",
            "Training mini-batch number 67000\n",
            "Training mini-batch number 68000\n",
            "Training mini-batch number 69000\n",
            "Training mini-batch number 70000\n",
            "Training mini-batch number 71000\n",
            "Training mini-batch number 72000\n",
            "Training mini-batch number 73000\n",
            "Training mini-batch number 74000\n",
            "Epoch 2: validation accuracy 98.99%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.88%\n",
            "Training mini-batch number 75000\n",
            "Training mini-batch number 76000\n",
            "Training mini-batch number 77000\n",
            "Training mini-batch number 78000\n",
            "Training mini-batch number 79000\n",
            "Training mini-batch number 80000\n",
            "Training mini-batch number 81000\n",
            "Training mini-batch number 82000\n",
            "Training mini-batch number 83000\n",
            "Training mini-batch number 84000\n",
            "Training mini-batch number 85000\n",
            "Training mini-batch number 86000\n",
            "Training mini-batch number 87000\n",
            "Training mini-batch number 88000\n",
            "Training mini-batch number 89000\n",
            "Training mini-batch number 90000\n",
            "Training mini-batch number 91000\n",
            "Training mini-batch number 92000\n",
            "Training mini-batch number 93000\n",
            "Training mini-batch number 94000\n",
            "Training mini-batch number 95000\n",
            "Training mini-batch number 96000\n",
            "Training mini-batch number 97000\n",
            "Training mini-batch number 98000\n",
            "Training mini-batch number 99000\n",
            "Epoch 3: validation accuracy 99.11%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.93%\n",
            "Training mini-batch number 100000\n",
            "Training mini-batch number 101000\n",
            "Training mini-batch number 102000\n",
            "Training mini-batch number 103000\n",
            "Training mini-batch number 104000\n",
            "Training mini-batch number 105000\n",
            "Training mini-batch number 106000\n",
            "Training mini-batch number 107000\n",
            "Training mini-batch number 108000\n",
            "Training mini-batch number 109000\n",
            "Training mini-batch number 110000\n",
            "Training mini-batch number 111000\n",
            "Training mini-batch number 112000\n",
            "Training mini-batch number 113000\n",
            "Training mini-batch number 114000\n",
            "Training mini-batch number 115000\n",
            "Training mini-batch number 116000\n",
            "Training mini-batch number 117000\n",
            "Training mini-batch number 118000\n",
            "Training mini-batch number 119000\n",
            "Training mini-batch number 120000\n",
            "Training mini-batch number 121000\n",
            "Training mini-batch number 122000\n",
            "Training mini-batch number 123000\n",
            "Training mini-batch number 124000\n",
            "Epoch 4: validation accuracy 99.07%\n",
            "Training mini-batch number 125000\n",
            "Training mini-batch number 126000\n",
            "Training mini-batch number 127000\n",
            "Training mini-batch number 128000\n",
            "Training mini-batch number 129000\n",
            "Training mini-batch number 130000\n",
            "Training mini-batch number 131000\n",
            "Training mini-batch number 132000\n",
            "Training mini-batch number 133000\n",
            "Training mini-batch number 134000\n",
            "Training mini-batch number 135000\n",
            "Training mini-batch number 136000\n",
            "Training mini-batch number 137000\n",
            "Training mini-batch number 138000\n",
            "Training mini-batch number 139000\n",
            "Training mini-batch number 140000\n",
            "Training mini-batch number 141000\n",
            "Training mini-batch number 142000\n",
            "Training mini-batch number 143000\n",
            "Training mini-batch number 144000\n",
            "Training mini-batch number 145000\n",
            "Training mini-batch number 146000\n",
            "Training mini-batch number 147000\n",
            "Training mini-batch number 148000\n",
            "Training mini-batch number 149000\n",
            "Epoch 5: validation accuracy 99.12%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.98%\n",
            "Training mini-batch number 150000\n",
            "Training mini-batch number 151000\n",
            "Training mini-batch number 152000\n",
            "Training mini-batch number 153000\n",
            "Training mini-batch number 154000\n",
            "Training mini-batch number 155000\n",
            "Training mini-batch number 156000\n",
            "Training mini-batch number 157000\n",
            "Training mini-batch number 158000\n",
            "Training mini-batch number 159000\n",
            "Training mini-batch number 160000\n",
            "Training mini-batch number 161000\n",
            "Training mini-batch number 162000\n",
            "Training mini-batch number 163000\n",
            "Training mini-batch number 164000\n",
            "Training mini-batch number 165000\n",
            "Training mini-batch number 166000\n",
            "Training mini-batch number 167000\n",
            "Training mini-batch number 168000\n",
            "Training mini-batch number 169000\n",
            "Training mini-batch number 170000\n",
            "Training mini-batch number 171000\n",
            "Training mini-batch number 172000\n",
            "Training mini-batch number 173000\n",
            "Training mini-batch number 174000\n",
            "Epoch 6: validation accuracy 99.16%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.94%\n",
            "Training mini-batch number 175000\n",
            "Training mini-batch number 176000\n",
            "Training mini-batch number 177000\n",
            "Training mini-batch number 178000\n",
            "Training mini-batch number 179000\n",
            "Training mini-batch number 180000\n",
            "Training mini-batch number 181000\n",
            "Training mini-batch number 182000\n",
            "Training mini-batch number 183000\n",
            "Training mini-batch number 184000\n",
            "Training mini-batch number 185000\n",
            "Training mini-batch number 186000\n",
            "Training mini-batch number 187000\n",
            "Training mini-batch number 188000\n",
            "Training mini-batch number 189000\n",
            "Training mini-batch number 190000\n",
            "Training mini-batch number 191000\n",
            "Training mini-batch number 192000\n",
            "Training mini-batch number 193000\n",
            "Training mini-batch number 194000\n",
            "Training mini-batch number 195000\n",
            "Training mini-batch number 196000\n",
            "Training mini-batch number 197000\n",
            "Training mini-batch number 198000\n",
            "Training mini-batch number 199000\n",
            "Epoch 7: validation accuracy 99.16%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 99.06%\n",
            "Training mini-batch number 200000\n",
            "Training mini-batch number 201000\n",
            "Training mini-batch number 202000\n",
            "Training mini-batch number 203000\n",
            "Training mini-batch number 204000\n",
            "Training mini-batch number 205000\n",
            "Training mini-batch number 206000\n",
            "Training mini-batch number 207000\n",
            "Training mini-batch number 208000\n",
            "Training mini-batch number 209000\n",
            "Training mini-batch number 210000\n",
            "Training mini-batch number 211000\n",
            "Training mini-batch number 212000\n",
            "Training mini-batch number 213000\n",
            "Training mini-batch number 214000\n",
            "Training mini-batch number 215000\n",
            "Training mini-batch number 216000\n",
            "Training mini-batch number 217000\n",
            "Training mini-batch number 218000\n",
            "Training mini-batch number 219000\n",
            "Training mini-batch number 220000\n",
            "Training mini-batch number 221000\n",
            "Training mini-batch number 222000\n",
            "Training mini-batch number 223000\n",
            "Training mini-batch number 224000\n",
            "Epoch 8: validation accuracy 99.27%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 99.19%\n",
            "Training mini-batch number 225000\n",
            "Training mini-batch number 226000\n",
            "Training mini-batch number 227000\n",
            "Training mini-batch number 228000\n",
            "Training mini-batch number 229000\n",
            "Training mini-batch number 230000\n",
            "Training mini-batch number 231000\n",
            "Training mini-batch number 232000\n",
            "Training mini-batch number 233000\n",
            "Training mini-batch number 234000\n",
            "Training mini-batch number 235000\n",
            "Training mini-batch number 236000\n",
            "Training mini-batch number 237000\n",
            "Training mini-batch number 238000\n",
            "Training mini-batch number 239000\n",
            "Training mini-batch number 240000\n",
            "Training mini-batch number 241000\n",
            "Training mini-batch number 242000\n",
            "Training mini-batch number 243000\n",
            "Training mini-batch number 244000\n",
            "Training mini-batch number 245000\n",
            "Training mini-batch number 246000\n",
            "Training mini-batch number 247000\n",
            "Training mini-batch number 248000\n",
            "Training mini-batch number 249000\n",
            "Epoch 9: validation accuracy 99.31%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 99.17%\n",
            "Training mini-batch number 250000\n",
            "Training mini-batch number 251000\n",
            "Training mini-batch number 252000\n",
            "Training mini-batch number 253000\n",
            "Training mini-batch number 254000\n",
            "Training mini-batch number 255000\n",
            "Training mini-batch number 256000\n",
            "Training mini-batch number 257000\n",
            "Training mini-batch number 258000\n",
            "Training mini-batch number 259000\n",
            "Training mini-batch number 260000\n",
            "Training mini-batch number 261000\n",
            "Training mini-batch number 262000\n",
            "Training mini-batch number 263000\n",
            "Training mini-batch number 264000\n",
            "Training mini-batch number 265000\n",
            "Training mini-batch number 266000\n",
            "Training mini-batch number 267000\n",
            "Training mini-batch number 268000\n",
            "Training mini-batch number 269000\n",
            "Training mini-batch number 270000\n",
            "Training mini-batch number 271000\n",
            "Training mini-batch number 272000\n",
            "Training mini-batch number 273000\n",
            "Training mini-batch number 274000\n",
            "Epoch 10: validation accuracy 99.26%\n",
            "Training mini-batch number 275000\n",
            "Training mini-batch number 276000\n",
            "Training mini-batch number 277000\n",
            "Training mini-batch number 278000\n",
            "Training mini-batch number 279000\n",
            "Training mini-batch number 280000\n",
            "Training mini-batch number 281000\n",
            "Training mini-batch number 282000\n",
            "Training mini-batch number 283000\n",
            "Training mini-batch number 284000\n",
            "Training mini-batch number 285000\n",
            "Training mini-batch number 286000\n",
            "Training mini-batch number 287000\n",
            "Training mini-batch number 288000\n",
            "Training mini-batch number 289000\n",
            "Training mini-batch number 290000\n",
            "Training mini-batch number 291000\n",
            "Training mini-batch number 292000\n",
            "Training mini-batch number 293000\n",
            "Training mini-batch number 294000\n",
            "Training mini-batch number 295000\n",
            "Training mini-batch number 296000\n",
            "Training mini-batch number 297000\n",
            "Training mini-batch number 298000\n",
            "Training mini-batch number 299000\n",
            "Epoch 11: validation accuracy 99.34%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 99.24%\n",
            "Training mini-batch number 300000\n",
            "Training mini-batch number 301000\n",
            "Training mini-batch number 302000\n",
            "Training mini-batch number 303000\n",
            "Training mini-batch number 304000\n",
            "Training mini-batch number 305000\n",
            "Training mini-batch number 306000\n",
            "Training mini-batch number 307000\n",
            "Training mini-batch number 308000\n",
            "Training mini-batch number 309000\n",
            "Training mini-batch number 310000\n",
            "Training mini-batch number 311000\n",
            "Training mini-batch number 312000\n",
            "Training mini-batch number 313000\n",
            "Training mini-batch number 314000\n",
            "Training mini-batch number 315000\n",
            "Training mini-batch number 316000\n",
            "Training mini-batch number 317000\n",
            "Training mini-batch number 318000\n",
            "Training mini-batch number 319000\n",
            "Training mini-batch number 320000\n",
            "Training mini-batch number 321000\n",
            "Training mini-batch number 322000\n",
            "Training mini-batch number 323000\n",
            "Training mini-batch number 324000\n",
            "Epoch 12: validation accuracy 99.17%\n",
            "Training mini-batch number 325000\n",
            "Training mini-batch number 326000\n",
            "Training mini-batch number 327000\n",
            "Training mini-batch number 328000\n",
            "Training mini-batch number 329000\n",
            "Training mini-batch number 330000\n",
            "Training mini-batch number 331000\n",
            "Training mini-batch number 332000\n",
            "Training mini-batch number 333000\n",
            "Training mini-batch number 334000\n",
            "Training mini-batch number 335000\n",
            "Training mini-batch number 336000\n",
            "Training mini-batch number 337000\n",
            "Training mini-batch number 338000\n",
            "Training mini-batch number 339000\n",
            "Training mini-batch number 340000\n",
            "Training mini-batch number 341000\n",
            "Training mini-batch number 342000\n",
            "Training mini-batch number 343000\n",
            "Training mini-batch number 344000\n",
            "Training mini-batch number 345000\n",
            "Training mini-batch number 346000\n",
            "Training mini-batch number 347000\n",
            "Training mini-batch number 348000\n",
            "Training mini-batch number 349000\n",
            "Epoch 13: validation accuracy 99.23%\n",
            "Training mini-batch number 350000\n",
            "Training mini-batch number 351000\n",
            "Training mini-batch number 352000\n",
            "Training mini-batch number 353000\n",
            "Training mini-batch number 354000\n",
            "Training mini-batch number 355000\n",
            "Training mini-batch number 356000\n",
            "Training mini-batch number 357000\n",
            "Training mini-batch number 358000\n",
            "Training mini-batch number 359000\n",
            "Training mini-batch number 360000\n",
            "Training mini-batch number 361000\n",
            "Training mini-batch number 362000\n",
            "Training mini-batch number 363000\n",
            "Training mini-batch number 364000\n",
            "Training mini-batch number 365000\n",
            "Training mini-batch number 366000\n",
            "Training mini-batch number 367000\n",
            "Training mini-batch number 368000\n",
            "Training mini-batch number 369000\n",
            "Training mini-batch number 370000\n",
            "Training mini-batch number 371000\n",
            "Training mini-batch number 372000\n",
            "Training mini-batch number 373000\n",
            "Training mini-batch number 374000\n",
            "Epoch 14: validation accuracy 99.15%\n",
            "Training mini-batch number 375000\n",
            "Training mini-batch number 376000\n",
            "Training mini-batch number 377000\n",
            "Training mini-batch number 378000\n",
            "Training mini-batch number 379000\n",
            "Training mini-batch number 380000\n",
            "Training mini-batch number 381000\n",
            "Training mini-batch number 382000\n",
            "Training mini-batch number 383000\n",
            "Training mini-batch number 384000\n",
            "Training mini-batch number 385000\n",
            "Training mini-batch number 386000\n",
            "Training mini-batch number 387000\n",
            "Training mini-batch number 388000\n",
            "Training mini-batch number 389000\n",
            "Training mini-batch number 390000\n",
            "Training mini-batch number 391000\n",
            "Training mini-batch number 392000\n",
            "Training mini-batch number 393000\n",
            "Training mini-batch number 394000\n",
            "Training mini-batch number 395000\n",
            "Training mini-batch number 396000\n",
            "Training mini-batch number 397000\n",
            "Training mini-batch number 398000\n",
            "Training mini-batch number 399000\n",
            "Epoch 15: validation accuracy 99.26%\n",
            "Training mini-batch number 400000\n",
            "Training mini-batch number 401000\n",
            "Training mini-batch number 402000\n",
            "Training mini-batch number 403000\n",
            "Training mini-batch number 404000\n",
            "Training mini-batch number 405000\n",
            "Training mini-batch number 406000\n",
            "Training mini-batch number 407000\n",
            "Training mini-batch number 408000\n",
            "Training mini-batch number 409000\n",
            "Training mini-batch number 410000\n",
            "Training mini-batch number 411000\n",
            "Training mini-batch number 412000\n",
            "Training mini-batch number 413000\n",
            "Training mini-batch number 414000\n",
            "Training mini-batch number 415000\n",
            "Training mini-batch number 416000\n",
            "Training mini-batch number 417000\n",
            "Training mini-batch number 418000\n",
            "Training mini-batch number 419000\n",
            "Training mini-batch number 420000\n",
            "Training mini-batch number 421000\n",
            "Training mini-batch number 422000\n",
            "Training mini-batch number 423000\n",
            "Training mini-batch number 424000\n",
            "Epoch 16: validation accuracy 99.35%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 99.28%\n",
            "Training mini-batch number 425000\n",
            "Training mini-batch number 426000\n",
            "Training mini-batch number 427000\n",
            "Training mini-batch number 428000\n",
            "Training mini-batch number 429000\n",
            "Training mini-batch number 430000\n",
            "Training mini-batch number 431000\n",
            "Training mini-batch number 432000\n",
            "Training mini-batch number 433000\n",
            "Training mini-batch number 434000\n",
            "Training mini-batch number 435000\n",
            "Training mini-batch number 436000\n",
            "Training mini-batch number 437000\n",
            "Training mini-batch number 438000\n",
            "Training mini-batch number 439000\n",
            "Training mini-batch number 440000\n",
            "Training mini-batch number 441000\n",
            "Training mini-batch number 442000\n",
            "Training mini-batch number 443000\n",
            "Training mini-batch number 444000\n",
            "Training mini-batch number 445000\n",
            "Training mini-batch number 446000\n",
            "Training mini-batch number 447000\n",
            "Training mini-batch number 448000\n",
            "Training mini-batch number 449000\n",
            "Epoch 17: validation accuracy 99.30%\n",
            "Training mini-batch number 450000\n",
            "Training mini-batch number 451000\n",
            "Training mini-batch number 452000\n",
            "Training mini-batch number 453000\n",
            "Training mini-batch number 454000\n",
            "Training mini-batch number 455000\n",
            "Training mini-batch number 456000\n",
            "Training mini-batch number 457000\n",
            "Training mini-batch number 458000\n",
            "Training mini-batch number 459000\n",
            "Training mini-batch number 460000\n",
            "Training mini-batch number 461000\n",
            "Training mini-batch number 462000\n",
            "Training mini-batch number 463000\n",
            "Training mini-batch number 464000\n",
            "Training mini-batch number 465000\n",
            "Training mini-batch number 466000\n",
            "Training mini-batch number 467000\n",
            "Training mini-batch number 468000\n",
            "Training mini-batch number 469000\n",
            "Training mini-batch number 470000\n",
            "Training mini-batch number 471000\n",
            "Training mini-batch number 472000\n",
            "Training mini-batch number 473000\n",
            "Training mini-batch number 474000\n",
            "Epoch 18: validation accuracy 99.31%\n",
            "Training mini-batch number 475000\n",
            "Training mini-batch number 476000\n",
            "Training mini-batch number 477000\n",
            "Training mini-batch number 478000\n",
            "Training mini-batch number 479000\n",
            "Training mini-batch number 480000\n",
            "Training mini-batch number 481000\n",
            "Training mini-batch number 482000\n",
            "Training mini-batch number 483000\n",
            "Training mini-batch number 484000\n",
            "Training mini-batch number 485000\n",
            "Training mini-batch number 486000\n",
            "Training mini-batch number 487000\n",
            "Training mini-batch number 488000\n",
            "Training mini-batch number 489000\n",
            "Training mini-batch number 490000\n",
            "Training mini-batch number 491000\n",
            "Training mini-batch number 492000\n",
            "Training mini-batch number 493000\n",
            "Training mini-batch number 494000\n",
            "Training mini-batch number 495000\n",
            "Training mini-batch number 496000\n",
            "Training mini-batch number 497000\n",
            "Training mini-batch number 498000\n",
            "Training mini-batch number 499000\n",
            "Epoch 19: validation accuracy 99.40%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 99.33%\n",
            "Training mini-batch number 500000\n",
            "Training mini-batch number 501000\n",
            "Training mini-batch number 502000\n",
            "Training mini-batch number 503000\n",
            "Training mini-batch number 504000\n",
            "Training mini-batch number 505000\n",
            "Training mini-batch number 506000\n",
            "Training mini-batch number 507000\n",
            "Training mini-batch number 508000\n",
            "Training mini-batch number 509000\n",
            "Training mini-batch number 510000\n",
            "Training mini-batch number 511000\n",
            "Training mini-batch number 512000\n",
            "Training mini-batch number 513000\n",
            "Training mini-batch number 514000\n",
            "Training mini-batch number 515000\n",
            "Training mini-batch number 516000\n",
            "Training mini-batch number 517000\n",
            "Training mini-batch number 518000\n",
            "Training mini-batch number 519000\n",
            "Training mini-batch number 520000\n",
            "Training mini-batch number 521000\n",
            "Training mini-batch number 522000\n",
            "Training mini-batch number 523000\n",
            "Training mini-batch number 524000\n",
            "Epoch 20: validation accuracy 99.33%\n",
            "Training mini-batch number 525000\n",
            "Training mini-batch number 526000\n",
            "Training mini-batch number 527000\n",
            "Training mini-batch number 528000\n",
            "Training mini-batch number 529000\n",
            "Training mini-batch number 530000\n",
            "Training mini-batch number 531000\n",
            "Training mini-batch number 532000\n",
            "Training mini-batch number 533000\n",
            "Training mini-batch number 534000\n",
            "Training mini-batch number 535000\n",
            "Training mini-batch number 536000\n",
            "Training mini-batch number 537000\n",
            "Training mini-batch number 538000\n",
            "Training mini-batch number 539000\n",
            "Training mini-batch number 540000\n",
            "Training mini-batch number 541000\n",
            "Training mini-batch number 542000\n",
            "Training mini-batch number 543000\n",
            "Training mini-batch number 544000\n",
            "Training mini-batch number 545000\n",
            "Training mini-batch number 546000\n",
            "Training mini-batch number 547000\n",
            "Training mini-batch number 548000\n",
            "Training mini-batch number 549000\n",
            "Epoch 21: validation accuracy 99.27%\n",
            "Training mini-batch number 550000\n",
            "Training mini-batch number 551000\n",
            "Training mini-batch number 552000\n",
            "Training mini-batch number 553000\n",
            "Training mini-batch number 554000\n",
            "Training mini-batch number 555000\n",
            "Training mini-batch number 556000\n",
            "Training mini-batch number 557000\n",
            "Training mini-batch number 558000\n",
            "Training mini-batch number 559000\n",
            "Training mini-batch number 560000\n",
            "Training mini-batch number 561000\n",
            "Training mini-batch number 562000\n",
            "Training mini-batch number 563000\n",
            "Training mini-batch number 564000\n",
            "Training mini-batch number 565000\n",
            "Training mini-batch number 566000\n",
            "Training mini-batch number 567000\n",
            "Training mini-batch number 568000\n",
            "Training mini-batch number 569000\n",
            "Training mini-batch number 570000\n",
            "Training mini-batch number 571000\n",
            "Training mini-batch number 572000\n",
            "Training mini-batch number 573000\n",
            "Training mini-batch number 574000\n",
            "Epoch 22: validation accuracy 99.40%\n",
            "Training mini-batch number 575000\n",
            "Training mini-batch number 576000\n",
            "Training mini-batch number 577000\n",
            "Training mini-batch number 578000\n",
            "Training mini-batch number 579000\n",
            "Training mini-batch number 580000\n",
            "Training mini-batch number 581000\n",
            "Training mini-batch number 582000\n",
            "Training mini-batch number 583000\n",
            "Training mini-batch number 584000\n",
            "Training mini-batch number 585000\n",
            "Training mini-batch number 586000\n",
            "Training mini-batch number 587000\n",
            "Training mini-batch number 588000\n",
            "Training mini-batch number 589000\n",
            "Training mini-batch number 590000\n",
            "Training mini-batch number 591000\n",
            "Training mini-batch number 592000\n",
            "Training mini-batch number 593000\n",
            "Training mini-batch number 594000\n",
            "Training mini-batch number 595000\n",
            "Training mini-batch number 596000\n",
            "Training mini-batch number 597000\n",
            "Training mini-batch number 598000\n",
            "Training mini-batch number 599000\n",
            "Epoch 23: validation accuracy 99.37%\n",
            "Training mini-batch number 600000\n",
            "Training mini-batch number 601000\n",
            "Training mini-batch number 602000\n",
            "Training mini-batch number 603000\n",
            "Training mini-batch number 604000\n",
            "Training mini-batch number 605000\n",
            "Training mini-batch number 606000\n",
            "Training mini-batch number 607000\n",
            "Training mini-batch number 608000\n",
            "Training mini-batch number 609000\n",
            "Training mini-batch number 610000\n",
            "Training mini-batch number 611000\n",
            "Training mini-batch number 612000\n",
            "Training mini-batch number 613000\n",
            "Training mini-batch number 614000\n",
            "Training mini-batch number 615000\n",
            "Training mini-batch number 616000\n",
            "Training mini-batch number 617000\n",
            "Training mini-batch number 618000\n",
            "Training mini-batch number 619000\n",
            "Training mini-batch number 620000\n",
            "Training mini-batch number 621000\n",
            "Training mini-batch number 622000\n",
            "Training mini-batch number 623000\n",
            "Training mini-batch number 624000\n",
            "Epoch 24: validation accuracy 99.38%\n",
            "Training mini-batch number 625000\n",
            "Training mini-batch number 626000\n",
            "Training mini-batch number 627000\n",
            "Training mini-batch number 628000\n",
            "Training mini-batch number 629000\n",
            "Training mini-batch number 630000\n",
            "Training mini-batch number 631000\n",
            "Training mini-batch number 632000\n",
            "Training mini-batch number 633000\n",
            "Training mini-batch number 634000\n",
            "Training mini-batch number 635000\n",
            "Training mini-batch number 636000\n",
            "Training mini-batch number 637000\n",
            "Training mini-batch number 638000\n",
            "Training mini-batch number 639000\n",
            "Training mini-batch number 640000\n",
            "Training mini-batch number 641000\n",
            "Training mini-batch number 642000\n",
            "Training mini-batch number 643000\n",
            "Training mini-batch number 644000\n",
            "Training mini-batch number 645000\n",
            "Training mini-batch number 646000\n",
            "Training mini-batch number 647000\n",
            "Training mini-batch number 648000\n",
            "Training mini-batch number 649000\n",
            "Epoch 25: validation accuracy 99.27%\n",
            "Training mini-batch number 650000\n",
            "Training mini-batch number 651000\n",
            "Training mini-batch number 652000\n",
            "Training mini-batch number 653000\n",
            "Training mini-batch number 654000\n",
            "Training mini-batch number 655000\n",
            "Training mini-batch number 656000\n",
            "Training mini-batch number 657000\n",
            "Training mini-batch number 658000\n",
            "Training mini-batch number 659000\n",
            "Training mini-batch number 660000\n",
            "Training mini-batch number 661000\n",
            "Training mini-batch number 662000\n",
            "Training mini-batch number 663000\n",
            "Training mini-batch number 664000\n",
            "Training mini-batch number 665000\n",
            "Training mini-batch number 666000\n",
            "Training mini-batch number 667000\n",
            "Training mini-batch number 668000\n",
            "Training mini-batch number 669000\n",
            "Training mini-batch number 670000\n",
            "Training mini-batch number 671000\n",
            "Training mini-batch number 672000\n",
            "Training mini-batch number 673000\n",
            "Training mini-batch number 674000\n",
            "Epoch 26: validation accuracy 99.25%\n",
            "Training mini-batch number 675000\n",
            "Training mini-batch number 676000\n",
            "Training mini-batch number 677000\n",
            "Training mini-batch number 678000\n",
            "Training mini-batch number 679000\n",
            "Training mini-batch number 680000\n",
            "Training mini-batch number 681000\n",
            "Training mini-batch number 682000\n",
            "Training mini-batch number 683000\n",
            "Training mini-batch number 684000\n",
            "Training mini-batch number 685000\n",
            "Training mini-batch number 686000\n",
            "Training mini-batch number 687000\n",
            "Training mini-batch number 688000\n",
            "Training mini-batch number 689000\n",
            "Training mini-batch number 690000\n",
            "Training mini-batch number 691000\n",
            "Training mini-batch number 692000\n",
            "Training mini-batch number 693000\n",
            "Training mini-batch number 694000\n",
            "Training mini-batch number 695000\n",
            "Training mini-batch number 696000\n",
            "Training mini-batch number 697000\n",
            "Training mini-batch number 698000\n",
            "Training mini-batch number 699000\n",
            "Epoch 27: validation accuracy 99.30%\n",
            "Training mini-batch number 700000\n",
            "Training mini-batch number 701000\n",
            "Training mini-batch number 702000\n",
            "Training mini-batch number 703000\n",
            "Training mini-batch number 704000\n",
            "Training mini-batch number 705000\n",
            "Training mini-batch number 706000\n",
            "Training mini-batch number 707000\n",
            "Training mini-batch number 708000\n",
            "Training mini-batch number 709000\n",
            "Training mini-batch number 710000\n",
            "Training mini-batch number 711000\n",
            "Training mini-batch number 712000\n",
            "Training mini-batch number 713000\n",
            "Training mini-batch number 714000\n",
            "Training mini-batch number 715000\n",
            "Training mini-batch number 716000\n",
            "Training mini-batch number 717000\n",
            "Training mini-batch number 718000\n",
            "Training mini-batch number 719000\n",
            "Training mini-batch number 720000\n",
            "Training mini-batch number 721000\n",
            "Training mini-batch number 722000\n",
            "Training mini-batch number 723000\n",
            "Training mini-batch number 724000\n",
            "Epoch 28: validation accuracy 99.30%\n",
            "Training mini-batch number 725000\n",
            "Training mini-batch number 726000\n",
            "Training mini-batch number 727000\n",
            "Training mini-batch number 728000\n",
            "Training mini-batch number 729000\n",
            "Training mini-batch number 730000\n",
            "Training mini-batch number 731000\n",
            "Training mini-batch number 732000\n",
            "Training mini-batch number 733000\n",
            "Training mini-batch number 734000\n",
            "Training mini-batch number 735000\n",
            "Training mini-batch number 736000\n",
            "Training mini-batch number 737000\n",
            "Training mini-batch number 738000\n",
            "Training mini-batch number 739000\n",
            "Training mini-batch number 740000\n",
            "Training mini-batch number 741000\n",
            "Training mini-batch number 742000\n",
            "Training mini-batch number 743000\n",
            "Training mini-batch number 744000\n",
            "Training mini-batch number 745000\n",
            "Training mini-batch number 746000\n",
            "Training mini-batch number 747000\n",
            "Training mini-batch number 748000\n",
            "Training mini-batch number 749000\n",
            "Epoch 29: validation accuracy 99.33%\n",
            "Training mini-batch number 750000\n",
            "Training mini-batch number 751000\n",
            "Training mini-batch number 752000\n",
            "Training mini-batch number 753000\n",
            "Training mini-batch number 754000\n",
            "Training mini-batch number 755000\n",
            "Training mini-batch number 756000\n",
            "Training mini-batch number 757000\n",
            "Training mini-batch number 758000\n",
            "Training mini-batch number 759000\n",
            "Training mini-batch number 760000\n",
            "Training mini-batch number 761000\n",
            "Training mini-batch number 762000\n",
            "Training mini-batch number 763000\n",
            "Training mini-batch number 764000\n",
            "Training mini-batch number 765000\n",
            "Training mini-batch number 766000\n",
            "Training mini-batch number 767000\n",
            "Training mini-batch number 768000\n",
            "Training mini-batch number 769000\n",
            "Training mini-batch number 770000\n",
            "Training mini-batch number 771000\n",
            "Training mini-batch number 772000\n",
            "Training mini-batch number 773000\n",
            "Training mini-batch number 774000\n",
            "Epoch 30: validation accuracy 99.36%\n",
            "Training mini-batch number 775000\n",
            "Training mini-batch number 776000\n",
            "Training mini-batch number 777000\n",
            "Training mini-batch number 778000\n",
            "Training mini-batch number 779000\n",
            "Training mini-batch number 780000\n",
            "Training mini-batch number 781000\n",
            "Training mini-batch number 782000\n",
            "Training mini-batch number 783000\n",
            "Training mini-batch number 784000\n",
            "Training mini-batch number 785000\n",
            "Training mini-batch number 786000\n",
            "Training mini-batch number 787000\n",
            "Training mini-batch number 788000\n",
            "Training mini-batch number 789000\n",
            "Training mini-batch number 790000\n",
            "Training mini-batch number 791000\n",
            "Training mini-batch number 792000\n",
            "Training mini-batch number 793000\n",
            "Training mini-batch number 794000\n",
            "Training mini-batch number 795000\n",
            "Training mini-batch number 796000\n",
            "Training mini-batch number 797000\n",
            "Training mini-batch number 798000\n",
            "Training mini-batch number 799000\n",
            "Epoch 31: validation accuracy 99.38%\n",
            "Training mini-batch number 800000\n",
            "Training mini-batch number 801000\n",
            "Training mini-batch number 802000\n",
            "Training mini-batch number 803000\n",
            "Training mini-batch number 804000\n",
            "Training mini-batch number 805000\n",
            "Training mini-batch number 806000\n",
            "Training mini-batch number 807000\n",
            "Training mini-batch number 808000\n",
            "Training mini-batch number 809000\n",
            "Training mini-batch number 810000\n",
            "Training mini-batch number 811000\n",
            "Training mini-batch number 812000\n",
            "Training mini-batch number 813000\n",
            "Training mini-batch number 814000\n",
            "Training mini-batch number 815000\n",
            "Training mini-batch number 816000\n",
            "Training mini-batch number 817000\n",
            "Training mini-batch number 818000\n",
            "Training mini-batch number 819000\n",
            "Training mini-batch number 820000\n",
            "Training mini-batch number 821000\n",
            "Training mini-batch number 822000\n",
            "Training mini-batch number 823000\n",
            "Training mini-batch number 824000\n",
            "Epoch 32: validation accuracy 99.38%\n",
            "Training mini-batch number 825000\n",
            "Training mini-batch number 826000\n",
            "Training mini-batch number 827000\n",
            "Training mini-batch number 828000\n",
            "Training mini-batch number 829000\n",
            "Training mini-batch number 830000\n",
            "Training mini-batch number 831000\n",
            "Training mini-batch number 832000\n",
            "Training mini-batch number 833000\n",
            "Training mini-batch number 834000\n",
            "Training mini-batch number 835000\n",
            "Training mini-batch number 836000\n",
            "Training mini-batch number 837000\n",
            "Training mini-batch number 838000\n",
            "Training mini-batch number 839000\n",
            "Training mini-batch number 840000\n",
            "Training mini-batch number 841000\n",
            "Training mini-batch number 842000\n",
            "Training mini-batch number 843000\n",
            "Training mini-batch number 844000\n",
            "Training mini-batch number 845000\n",
            "Training mini-batch number 846000\n",
            "Training mini-batch number 847000\n",
            "Training mini-batch number 848000\n",
            "Training mini-batch number 849000\n",
            "Epoch 33: validation accuracy 99.37%\n",
            "Training mini-batch number 850000\n",
            "Training mini-batch number 851000\n",
            "Training mini-batch number 852000\n",
            "Training mini-batch number 853000\n",
            "Training mini-batch number 854000\n",
            "Training mini-batch number 855000\n",
            "Training mini-batch number 856000\n",
            "Training mini-batch number 857000\n",
            "Training mini-batch number 858000\n",
            "Training mini-batch number 859000\n",
            "Training mini-batch number 860000\n",
            "Training mini-batch number 861000\n",
            "Training mini-batch number 862000\n",
            "Training mini-batch number 863000\n",
            "Training mini-batch number 864000\n",
            "Training mini-batch number 865000\n",
            "Training mini-batch number 866000\n",
            "Training mini-batch number 867000\n",
            "Training mini-batch number 868000\n",
            "Training mini-batch number 869000\n",
            "Training mini-batch number 870000\n",
            "Training mini-batch number 871000\n",
            "Training mini-batch number 872000\n",
            "Training mini-batch number 873000\n",
            "Training mini-batch number 874000\n",
            "Epoch 34: validation accuracy 99.38%\n",
            "Training mini-batch number 875000\n",
            "Training mini-batch number 876000\n",
            "Training mini-batch number 877000\n",
            "Training mini-batch number 878000\n",
            "Training mini-batch number 879000\n",
            "Training mini-batch number 880000\n",
            "Training mini-batch number 881000\n",
            "Training mini-batch number 882000\n",
            "Training mini-batch number 883000\n",
            "Training mini-batch number 884000\n",
            "Training mini-batch number 885000\n",
            "Training mini-batch number 886000\n",
            "Training mini-batch number 887000\n",
            "Training mini-batch number 888000\n",
            "Training mini-batch number 889000\n",
            "Training mini-batch number 890000\n",
            "Training mini-batch number 891000\n",
            "Training mini-batch number 892000\n",
            "Training mini-batch number 893000\n",
            "Training mini-batch number 894000\n",
            "Training mini-batch number 895000\n",
            "Training mini-batch number 896000\n",
            "Training mini-batch number 897000\n",
            "Training mini-batch number 898000\n",
            "Training mini-batch number 899000\n",
            "Epoch 35: validation accuracy 99.37%\n",
            "Training mini-batch number 900000\n",
            "Training mini-batch number 901000\n",
            "Training mini-batch number 902000\n",
            "Training mini-batch number 903000\n",
            "Training mini-batch number 904000\n",
            "Training mini-batch number 905000\n",
            "Training mini-batch number 906000\n",
            "Training mini-batch number 907000\n",
            "Training mini-batch number 908000\n",
            "Training mini-batch number 909000\n",
            "Training mini-batch number 910000\n",
            "Training mini-batch number 911000\n",
            "Training mini-batch number 912000\n",
            "Training mini-batch number 913000\n",
            "Training mini-batch number 914000\n",
            "Training mini-batch number 915000\n",
            "Training mini-batch number 916000\n",
            "Training mini-batch number 917000\n",
            "Training mini-batch number 918000\n",
            "Training mini-batch number 919000\n",
            "Training mini-batch number 920000\n",
            "Training mini-batch number 921000\n",
            "Training mini-batch number 922000\n",
            "Training mini-batch number 923000\n",
            "Training mini-batch number 924000\n",
            "Epoch 36: validation accuracy 99.38%\n",
            "Training mini-batch number 925000\n",
            "Training mini-batch number 926000\n",
            "Training mini-batch number 927000\n",
            "Training mini-batch number 928000\n",
            "Training mini-batch number 929000\n",
            "Training mini-batch number 930000\n",
            "Training mini-batch number 931000\n",
            "Training mini-batch number 932000\n",
            "Training mini-batch number 933000\n",
            "Training mini-batch number 934000\n",
            "Training mini-batch number 935000\n",
            "Training mini-batch number 936000\n",
            "Training mini-batch number 937000\n",
            "Training mini-batch number 938000\n",
            "Training mini-batch number 939000\n",
            "Training mini-batch number 940000\n",
            "Training mini-batch number 941000\n",
            "Training mini-batch number 942000\n",
            "Training mini-batch number 943000\n",
            "Training mini-batch number 944000\n",
            "Training mini-batch number 945000\n",
            "Training mini-batch number 946000\n",
            "Training mini-batch number 947000\n",
            "Training mini-batch number 948000\n",
            "Training mini-batch number 949000\n",
            "Epoch 37: validation accuracy 99.39%\n",
            "Training mini-batch number 950000\n",
            "Training mini-batch number 951000\n",
            "Training mini-batch number 952000\n",
            "Training mini-batch number 953000\n",
            "Training mini-batch number 954000\n",
            "Training mini-batch number 955000\n",
            "Training mini-batch number 956000\n",
            "Training mini-batch number 957000\n",
            "Training mini-batch number 958000\n",
            "Training mini-batch number 959000\n",
            "Training mini-batch number 960000\n",
            "Training mini-batch number 961000\n",
            "Training mini-batch number 962000\n",
            "Training mini-batch number 963000\n",
            "Training mini-batch number 964000\n",
            "Training mini-batch number 965000\n",
            "Training mini-batch number 966000\n",
            "Training mini-batch number 967000\n",
            "Training mini-batch number 968000\n",
            "Training mini-batch number 969000\n",
            "Training mini-batch number 970000\n",
            "Training mini-batch number 971000\n",
            "Training mini-batch number 972000\n",
            "Training mini-batch number 973000\n",
            "Training mini-batch number 974000\n",
            "Epoch 38: validation accuracy 99.38%\n",
            "Training mini-batch number 975000\n",
            "Training mini-batch number 976000\n",
            "Training mini-batch number 977000\n",
            "Training mini-batch number 978000\n",
            "Training mini-batch number 979000\n",
            "Training mini-batch number 980000\n",
            "Training mini-batch number 981000\n",
            "Training mini-batch number 982000\n",
            "Training mini-batch number 983000\n",
            "Training mini-batch number 984000\n",
            "Training mini-batch number 985000\n",
            "Training mini-batch number 986000\n",
            "Training mini-batch number 987000\n",
            "Training mini-batch number 988000\n",
            "Training mini-batch number 989000\n",
            "Training mini-batch number 990000\n",
            "Training mini-batch number 991000\n",
            "Training mini-batch number 992000\n",
            "Training mini-batch number 993000\n",
            "Training mini-batch number 994000\n",
            "Training mini-batch number 995000\n",
            "Training mini-batch number 996000\n",
            "Training mini-batch number 997000\n",
            "Training mini-batch number 998000\n",
            "Training mini-batch number 999000\n",
            "Epoch 39: validation accuracy 99.38%\n",
            "Training mini-batch number 1000000\n",
            "Training mini-batch number 1001000\n",
            "Training mini-batch number 1002000\n",
            "Training mini-batch number 1003000\n",
            "Training mini-batch number 1004000\n",
            "Training mini-batch number 1005000\n",
            "Training mini-batch number 1006000\n",
            "Training mini-batch number 1007000\n",
            "Training mini-batch number 1008000\n",
            "Training mini-batch number 1009000\n",
            "Training mini-batch number 1010000\n",
            "Training mini-batch number 1011000\n",
            "Training mini-batch number 1012000\n",
            "Training mini-batch number 1013000\n",
            "Training mini-batch number 1014000\n",
            "Training mini-batch number 1015000\n",
            "Training mini-batch number 1016000\n",
            "Training mini-batch number 1017000\n",
            "Training mini-batch number 1018000\n",
            "Training mini-batch number 1019000\n",
            "Training mini-batch number 1020000\n",
            "Training mini-batch number 1021000\n",
            "Training mini-batch number 1022000\n",
            "Training mini-batch number 1023000\n",
            "Training mini-batch number 1024000\n",
            "Epoch 40: validation accuracy 99.37%\n",
            "Training mini-batch number 1025000\n",
            "Training mini-batch number 1026000\n",
            "Training mini-batch number 1027000\n",
            "Training mini-batch number 1028000\n",
            "Training mini-batch number 1029000\n",
            "Training mini-batch number 1030000\n",
            "Training mini-batch number 1031000\n",
            "Training mini-batch number 1032000\n",
            "Training mini-batch number 1033000\n",
            "Training mini-batch number 1034000\n",
            "Training mini-batch number 1035000\n",
            "Training mini-batch number 1036000\n",
            "Training mini-batch number 1037000\n",
            "Training mini-batch number 1038000\n",
            "Training mini-batch number 1039000\n",
            "Training mini-batch number 1040000\n",
            "Training mini-batch number 1041000\n",
            "Training mini-batch number 1042000\n",
            "Training mini-batch number 1043000\n",
            "Training mini-batch number 1044000\n",
            "Training mini-batch number 1045000\n",
            "Training mini-batch number 1046000\n",
            "Training mini-batch number 1047000\n",
            "Training mini-batch number 1048000\n",
            "Training mini-batch number 1049000\n",
            "Epoch 41: validation accuracy 99.37%\n",
            "Training mini-batch number 1050000\n",
            "Training mini-batch number 1051000\n",
            "Training mini-batch number 1052000\n",
            "Training mini-batch number 1053000\n",
            "Training mini-batch number 1054000\n",
            "Training mini-batch number 1055000\n",
            "Training mini-batch number 1056000\n",
            "Training mini-batch number 1057000\n",
            "Training mini-batch number 1058000\n",
            "Training mini-batch number 1059000\n",
            "Training mini-batch number 1060000\n",
            "Training mini-batch number 1061000\n",
            "Training mini-batch number 1062000\n",
            "Training mini-batch number 1063000\n",
            "Training mini-batch number 1064000\n",
            "Training mini-batch number 1065000\n",
            "Training mini-batch number 1066000\n",
            "Training mini-batch number 1067000\n",
            "Training mini-batch number 1068000\n",
            "Training mini-batch number 1069000\n",
            "Training mini-batch number 1070000\n",
            "Training mini-batch number 1071000\n",
            "Training mini-batch number 1072000\n",
            "Training mini-batch number 1073000\n",
            "Training mini-batch number 1074000\n",
            "Epoch 42: validation accuracy 99.37%\n",
            "Training mini-batch number 1075000\n",
            "Training mini-batch number 1076000\n",
            "Training mini-batch number 1077000\n",
            "Training mini-batch number 1078000\n",
            "Training mini-batch number 1079000\n",
            "Training mini-batch number 1080000\n",
            "Training mini-batch number 1081000\n",
            "Training mini-batch number 1082000\n",
            "Training mini-batch number 1083000\n",
            "Training mini-batch number 1084000\n",
            "Training mini-batch number 1085000\n",
            "Training mini-batch number 1086000\n",
            "Training mini-batch number 1087000\n",
            "Training mini-batch number 1088000\n",
            "Training mini-batch number 1089000\n",
            "Training mini-batch number 1090000\n",
            "Training mini-batch number 1091000\n",
            "Training mini-batch number 1092000\n",
            "Training mini-batch number 1093000\n",
            "Training mini-batch number 1094000\n",
            "Training mini-batch number 1095000\n",
            "Training mini-batch number 1096000\n",
            "Training mini-batch number 1097000\n",
            "Training mini-batch number 1098000\n",
            "Training mini-batch number 1099000\n",
            "Epoch 43: validation accuracy 99.36%\n",
            "Training mini-batch number 1100000\n",
            "Training mini-batch number 1101000\n",
            "Training mini-batch number 1102000\n",
            "Training mini-batch number 1103000\n",
            "Training mini-batch number 1104000\n",
            "Training mini-batch number 1105000\n",
            "Training mini-batch number 1106000\n",
            "Training mini-batch number 1107000\n",
            "Training mini-batch number 1108000\n",
            "Training mini-batch number 1109000\n",
            "Training mini-batch number 1110000\n",
            "Training mini-batch number 1111000\n",
            "Training mini-batch number 1112000\n",
            "Training mini-batch number 1113000\n",
            "Training mini-batch number 1114000\n",
            "Training mini-batch number 1115000\n",
            "Training mini-batch number 1116000\n",
            "Training mini-batch number 1117000\n",
            "Training mini-batch number 1118000\n",
            "Training mini-batch number 1119000\n",
            "Training mini-batch number 1120000\n",
            "Training mini-batch number 1121000\n",
            "Training mini-batch number 1122000\n",
            "Training mini-batch number 1123000\n",
            "Training mini-batch number 1124000\n",
            "Epoch 44: validation accuracy 99.36%\n",
            "Training mini-batch number 1125000\n",
            "Training mini-batch number 1126000\n",
            "Training mini-batch number 1127000\n",
            "Training mini-batch number 1128000\n",
            "Training mini-batch number 1129000\n",
            "Training mini-batch number 1130000\n",
            "Training mini-batch number 1131000\n",
            "Training mini-batch number 1132000\n",
            "Training mini-batch number 1133000\n",
            "Training mini-batch number 1134000\n",
            "Training mini-batch number 1135000\n",
            "Training mini-batch number 1136000\n",
            "Training mini-batch number 1137000\n",
            "Training mini-batch number 1138000\n",
            "Training mini-batch number 1139000\n",
            "Training mini-batch number 1140000\n",
            "Training mini-batch number 1141000\n",
            "Training mini-batch number 1142000\n",
            "Training mini-batch number 1143000\n",
            "Training mini-batch number 1144000\n",
            "Training mini-batch number 1145000\n",
            "Training mini-batch number 1146000\n",
            "Training mini-batch number 1147000\n",
            "Training mini-batch number 1148000\n",
            "Training mini-batch number 1149000\n",
            "Epoch 45: validation accuracy 99.37%\n",
            "Training mini-batch number 1150000\n",
            "Training mini-batch number 1151000\n",
            "Training mini-batch number 1152000\n",
            "Training mini-batch number 1153000\n",
            "Training mini-batch number 1154000\n",
            "Training mini-batch number 1155000\n",
            "Training mini-batch number 1156000\n",
            "Training mini-batch number 1157000\n",
            "Training mini-batch number 1158000\n",
            "Training mini-batch number 1159000\n",
            "Training mini-batch number 1160000\n",
            "Training mini-batch number 1161000\n",
            "Training mini-batch number 1162000\n",
            "Training mini-batch number 1163000\n",
            "Training mini-batch number 1164000\n",
            "Training mini-batch number 1165000\n",
            "Training mini-batch number 1166000\n",
            "Training mini-batch number 1167000\n",
            "Training mini-batch number 1168000\n",
            "Training mini-batch number 1169000\n",
            "Training mini-batch number 1170000\n",
            "Training mini-batch number 1171000\n",
            "Training mini-batch number 1172000\n",
            "Training mini-batch number 1173000\n",
            "Training mini-batch number 1174000\n",
            "Epoch 46: validation accuracy 99.36%\n",
            "Training mini-batch number 1175000\n",
            "Training mini-batch number 1176000\n",
            "Training mini-batch number 1177000\n",
            "Training mini-batch number 1178000\n",
            "Training mini-batch number 1179000\n",
            "Training mini-batch number 1180000\n",
            "Training mini-batch number 1181000\n",
            "Training mini-batch number 1182000\n",
            "Training mini-batch number 1183000\n",
            "Training mini-batch number 1184000\n",
            "Training mini-batch number 1185000\n",
            "Training mini-batch number 1186000\n",
            "Training mini-batch number 1187000\n",
            "Training mini-batch number 1188000\n",
            "Training mini-batch number 1189000\n",
            "Training mini-batch number 1190000\n",
            "Training mini-batch number 1191000\n",
            "Training mini-batch number 1192000\n",
            "Training mini-batch number 1193000\n",
            "Training mini-batch number 1194000\n",
            "Training mini-batch number 1195000\n",
            "Training mini-batch number 1196000\n",
            "Training mini-batch number 1197000\n",
            "Training mini-batch number 1198000\n",
            "Training mini-batch number 1199000\n",
            "Epoch 47: validation accuracy 99.37%\n",
            "Training mini-batch number 1200000\n",
            "Training mini-batch number 1201000\n",
            "Training mini-batch number 1202000\n",
            "Training mini-batch number 1203000\n",
            "Training mini-batch number 1204000\n",
            "Training mini-batch number 1205000\n",
            "Training mini-batch number 1206000\n",
            "Training mini-batch number 1207000\n",
            "Training mini-batch number 1208000\n",
            "Training mini-batch number 1209000\n",
            "Training mini-batch number 1210000\n",
            "Training mini-batch number 1211000\n",
            "Training mini-batch number 1212000\n",
            "Training mini-batch number 1213000\n",
            "Training mini-batch number 1214000\n",
            "Training mini-batch number 1215000\n",
            "Training mini-batch number 1216000\n",
            "Training mini-batch number 1217000\n",
            "Training mini-batch number 1218000\n",
            "Training mini-batch number 1219000\n",
            "Training mini-batch number 1220000\n",
            "Training mini-batch number 1221000\n",
            "Training mini-batch number 1222000\n",
            "Training mini-batch number 1223000\n",
            "Training mini-batch number 1224000\n",
            "Epoch 48: validation accuracy 99.37%\n",
            "Training mini-batch number 1225000\n",
            "Training mini-batch number 1226000\n",
            "Training mini-batch number 1227000\n",
            "Training mini-batch number 1228000\n",
            "Training mini-batch number 1229000\n",
            "Training mini-batch number 1230000\n",
            "Training mini-batch number 1231000\n",
            "Training mini-batch number 1232000\n",
            "Training mini-batch number 1233000\n",
            "Training mini-batch number 1234000\n",
            "Training mini-batch number 1235000\n",
            "Training mini-batch number 1236000\n",
            "Training mini-batch number 1237000\n",
            "Training mini-batch number 1238000\n",
            "Training mini-batch number 1239000\n",
            "Training mini-batch number 1240000\n",
            "Training mini-batch number 1241000\n",
            "Training mini-batch number 1242000\n",
            "Training mini-batch number 1243000\n",
            "Training mini-batch number 1244000\n",
            "Training mini-batch number 1245000\n",
            "Training mini-batch number 1246000\n",
            "Training mini-batch number 1247000\n",
            "Training mini-batch number 1248000\n",
            "Training mini-batch number 1249000\n",
            "Epoch 49: validation accuracy 99.37%\n",
            "Training mini-batch number 1250000\n",
            "Training mini-batch number 1251000\n",
            "Training mini-batch number 1252000\n",
            "Training mini-batch number 1253000\n",
            "Training mini-batch number 1254000\n",
            "Training mini-batch number 1255000\n",
            "Training mini-batch number 1256000\n",
            "Training mini-batch number 1257000\n",
            "Training mini-batch number 1258000\n",
            "Training mini-batch number 1259000\n",
            "Training mini-batch number 1260000\n",
            "Training mini-batch number 1261000\n",
            "Training mini-batch number 1262000\n",
            "Training mini-batch number 1263000\n",
            "Training mini-batch number 1264000\n",
            "Training mini-batch number 1265000\n",
            "Training mini-batch number 1266000\n",
            "Training mini-batch number 1267000\n",
            "Training mini-batch number 1268000\n",
            "Training mini-batch number 1269000\n",
            "Training mini-batch number 1270000\n",
            "Training mini-batch number 1271000\n",
            "Training mini-batch number 1272000\n",
            "Training mini-batch number 1273000\n",
            "Training mini-batch number 1274000\n",
            "Epoch 50: validation accuracy 99.36%\n",
            "Training mini-batch number 1275000\n",
            "Training mini-batch number 1276000\n",
            "Training mini-batch number 1277000\n",
            "Training mini-batch number 1278000\n",
            "Training mini-batch number 1279000\n",
            "Training mini-batch number 1280000\n",
            "Training mini-batch number 1281000\n",
            "Training mini-batch number 1282000\n",
            "Training mini-batch number 1283000\n",
            "Training mini-batch number 1284000\n",
            "Training mini-batch number 1285000\n",
            "Training mini-batch number 1286000\n",
            "Training mini-batch number 1287000\n",
            "Training mini-batch number 1288000\n",
            "Training mini-batch number 1289000\n",
            "Training mini-batch number 1290000\n",
            "Training mini-batch number 1291000\n",
            "Training mini-batch number 1292000\n",
            "Training mini-batch number 1293000\n",
            "Training mini-batch number 1294000\n",
            "Training mini-batch number 1295000\n",
            "Training mini-batch number 1296000\n",
            "Training mini-batch number 1297000\n",
            "Training mini-batch number 1298000\n",
            "Training mini-batch number 1299000\n",
            "Epoch 51: validation accuracy 99.36%\n",
            "Training mini-batch number 1300000\n",
            "Training mini-batch number 1301000\n",
            "Training mini-batch number 1302000\n",
            "Training mini-batch number 1303000\n",
            "Training mini-batch number 1304000\n",
            "Training mini-batch number 1305000\n",
            "Training mini-batch number 1306000\n",
            "Training mini-batch number 1307000\n",
            "Training mini-batch number 1308000\n",
            "Training mini-batch number 1309000\n",
            "Training mini-batch number 1310000\n",
            "Training mini-batch number 1311000\n",
            "Training mini-batch number 1312000\n",
            "Training mini-batch number 1313000\n",
            "Training mini-batch number 1314000\n",
            "Training mini-batch number 1315000\n",
            "Training mini-batch number 1316000\n",
            "Training mini-batch number 1317000\n",
            "Training mini-batch number 1318000\n",
            "Training mini-batch number 1319000\n",
            "Training mini-batch number 1320000\n",
            "Training mini-batch number 1321000\n",
            "Training mini-batch number 1322000\n",
            "Training mini-batch number 1323000\n",
            "Training mini-batch number 1324000\n",
            "Epoch 52: validation accuracy 99.37%\n",
            "Training mini-batch number 1325000\n",
            "Training mini-batch number 1326000\n",
            "Training mini-batch number 1327000\n",
            "Training mini-batch number 1328000\n",
            "Training mini-batch number 1329000\n",
            "Training mini-batch number 1330000\n",
            "Training mini-batch number 1331000\n",
            "Training mini-batch number 1332000\n",
            "Training mini-batch number 1333000\n",
            "Training mini-batch number 1334000\n",
            "Training mini-batch number 1335000\n",
            "Training mini-batch number 1336000\n",
            "Training mini-batch number 1337000\n",
            "Training mini-batch number 1338000\n",
            "Training mini-batch number 1339000\n",
            "Training mini-batch number 1340000\n",
            "Training mini-batch number 1341000\n",
            "Training mini-batch number 1342000\n",
            "Training mini-batch number 1343000\n",
            "Training mini-batch number 1344000\n",
            "Training mini-batch number 1345000\n",
            "Training mini-batch number 1346000\n",
            "Training mini-batch number 1347000\n",
            "Training mini-batch number 1348000\n",
            "Training mini-batch number 1349000\n",
            "Epoch 53: validation accuracy 99.37%\n",
            "Training mini-batch number 1350000\n",
            "Training mini-batch number 1351000\n",
            "Training mini-batch number 1352000\n",
            "Training mini-batch number 1353000\n",
            "Training mini-batch number 1354000\n",
            "Training mini-batch number 1355000\n",
            "Training mini-batch number 1356000\n",
            "Training mini-batch number 1357000\n",
            "Training mini-batch number 1358000\n",
            "Training mini-batch number 1359000\n",
            "Training mini-batch number 1360000\n",
            "Training mini-batch number 1361000\n",
            "Training mini-batch number 1362000\n",
            "Training mini-batch number 1363000\n",
            "Training mini-batch number 1364000\n",
            "Training mini-batch number 1365000\n",
            "Training mini-batch number 1366000\n",
            "Training mini-batch number 1367000\n",
            "Training mini-batch number 1368000\n",
            "Training mini-batch number 1369000\n",
            "Training mini-batch number 1370000\n",
            "Training mini-batch number 1371000\n",
            "Training mini-batch number 1372000\n",
            "Training mini-batch number 1373000\n",
            "Training mini-batch number 1374000\n",
            "Epoch 54: validation accuracy 99.38%\n",
            "Training mini-batch number 1375000\n",
            "Training mini-batch number 1376000\n",
            "Training mini-batch number 1377000\n",
            "Training mini-batch number 1378000\n",
            "Training mini-batch number 1379000\n",
            "Training mini-batch number 1380000\n",
            "Training mini-batch number 1381000\n",
            "Training mini-batch number 1382000\n",
            "Training mini-batch number 1383000\n",
            "Training mini-batch number 1384000\n",
            "Training mini-batch number 1385000\n",
            "Training mini-batch number 1386000\n",
            "Training mini-batch number 1387000\n",
            "Training mini-batch number 1388000\n",
            "Training mini-batch number 1389000\n",
            "Training mini-batch number 1390000\n",
            "Training mini-batch number 1391000\n",
            "Training mini-batch number 1392000\n",
            "Training mini-batch number 1393000\n",
            "Training mini-batch number 1394000\n",
            "Training mini-batch number 1395000\n",
            "Training mini-batch number 1396000\n",
            "Training mini-batch number 1397000\n",
            "Training mini-batch number 1398000\n",
            "Training mini-batch number 1399000\n",
            "Epoch 55: validation accuracy 99.38%\n",
            "Training mini-batch number 1400000\n",
            "Training mini-batch number 1401000\n",
            "Training mini-batch number 1402000\n",
            "Training mini-batch number 1403000\n",
            "Training mini-batch number 1404000\n",
            "Training mini-batch number 1405000\n",
            "Training mini-batch number 1406000\n",
            "Training mini-batch number 1407000\n",
            "Training mini-batch number 1408000\n",
            "Training mini-batch number 1409000\n",
            "Training mini-batch number 1410000\n",
            "Training mini-batch number 1411000\n",
            "Training mini-batch number 1412000\n",
            "Training mini-batch number 1413000\n",
            "Training mini-batch number 1414000\n",
            "Training mini-batch number 1415000\n",
            "Training mini-batch number 1416000\n",
            "Training mini-batch number 1417000\n",
            "Training mini-batch number 1418000\n",
            "Training mini-batch number 1419000\n",
            "Training mini-batch number 1420000\n",
            "Training mini-batch number 1421000\n",
            "Training mini-batch number 1422000\n",
            "Training mini-batch number 1423000\n",
            "Training mini-batch number 1424000\n",
            "Epoch 56: validation accuracy 99.38%\n",
            "Training mini-batch number 1425000\n",
            "Training mini-batch number 1426000\n",
            "Training mini-batch number 1427000\n",
            "Training mini-batch number 1428000\n",
            "Training mini-batch number 1429000\n",
            "Training mini-batch number 1430000\n",
            "Training mini-batch number 1431000\n",
            "Training mini-batch number 1432000\n",
            "Training mini-batch number 1433000\n",
            "Training mini-batch number 1434000\n",
            "Training mini-batch number 1435000\n",
            "Training mini-batch number 1436000\n",
            "Training mini-batch number 1437000\n",
            "Training mini-batch number 1438000\n",
            "Training mini-batch number 1439000\n",
            "Training mini-batch number 1440000\n",
            "Training mini-batch number 1441000\n",
            "Training mini-batch number 1442000\n",
            "Training mini-batch number 1443000\n",
            "Training mini-batch number 1444000\n",
            "Training mini-batch number 1445000\n",
            "Training mini-batch number 1446000\n",
            "Training mini-batch number 1447000\n",
            "Training mini-batch number 1448000\n",
            "Training mini-batch number 1449000\n",
            "Epoch 57: validation accuracy 99.38%\n",
            "Training mini-batch number 1450000\n",
            "Training mini-batch number 1451000\n",
            "Training mini-batch number 1452000\n",
            "Training mini-batch number 1453000\n",
            "Training mini-batch number 1454000\n",
            "Training mini-batch number 1455000\n",
            "Training mini-batch number 1456000\n",
            "Training mini-batch number 1457000\n",
            "Training mini-batch number 1458000\n",
            "Training mini-batch number 1459000\n",
            "Training mini-batch number 1460000\n",
            "Training mini-batch number 1461000\n",
            "Training mini-batch number 1462000\n",
            "Training mini-batch number 1463000\n",
            "Training mini-batch number 1464000\n",
            "Training mini-batch number 1465000\n",
            "Training mini-batch number 1466000\n",
            "Training mini-batch number 1467000\n",
            "Training mini-batch number 1468000\n",
            "Training mini-batch number 1469000\n",
            "Training mini-batch number 1470000\n",
            "Training mini-batch number 1471000\n",
            "Training mini-batch number 1472000\n",
            "Training mini-batch number 1473000\n",
            "Training mini-batch number 1474000\n",
            "Epoch 58: validation accuracy 99.38%\n",
            "Training mini-batch number 1475000\n",
            "Training mini-batch number 1476000\n",
            "Training mini-batch number 1477000\n",
            "Training mini-batch number 1478000\n",
            "Training mini-batch number 1479000\n",
            "Training mini-batch number 1480000\n",
            "Training mini-batch number 1481000\n",
            "Training mini-batch number 1482000\n",
            "Training mini-batch number 1483000\n",
            "Training mini-batch number 1484000\n",
            "Training mini-batch number 1485000\n",
            "Training mini-batch number 1486000\n",
            "Training mini-batch number 1487000\n",
            "Training mini-batch number 1488000\n",
            "Training mini-batch number 1489000\n",
            "Training mini-batch number 1490000\n",
            "Training mini-batch number 1491000\n",
            "Training mini-batch number 1492000\n",
            "Training mini-batch number 1493000\n",
            "Training mini-batch number 1494000\n",
            "Training mini-batch number 1495000\n",
            "Training mini-batch number 1496000\n",
            "Training mini-batch number 1497000\n",
            "Training mini-batch number 1498000\n",
            "Training mini-batch number 1499000\n",
            "Epoch 59: validation accuracy 99.38%\n",
            "Finished training network.\n",
            "Best validation accuracy of 99.40% obtained at iteration 499999\n",
            "Corresponding test accuracy of 99.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "_fDb0sPFnjWu"
      },
      "source": [
        "net = Network([\n",
        "        ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28),\n",
        "                     filter_shape=(20, 1, 5, 5),\n",
        "                     poolsize=(2, 2),\n",
        "                     activation_fn=ReLU),\n",
        "        ConvPoolLayer(image_shape=(mini_batch_size, 20, 12, 12),\n",
        "                     filter_shape=(40, 20, 5, 5),\n",
        "                     poolsize=(2, 2),\n",
        "                     activation_fn=ReLU),\n",
        "        FullyConnectedLayer(\n",
        "            n_in=40*4*4, n_out=1000, activation_fn=ReLU, p_dropout=0.5),\n",
        "        FullyConnectedLayer(\n",
        "            n_in=1000, n_out=1000, activation_fn=ReLU, p_dropout=0.5),\n",
        "        SoftmaxLayer(n_in=1000, n_out=10, p_dropout=0.5)],\n",
        "        mini_batch_size)"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PgDzneXnjWu",
        "outputId": "43aa7754-6e83-4388-e6f4-5459e86516e1"
      },
      "source": [
        "net.SGD(expanded_training_data, 40, mini_batch_size, 0.03, validation_data, test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
            "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(Rebroadcast{?,0}.0, Elemwise{second,no_inplace}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
            "    c = node.op(*inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Rebroadcast{?,0}.0) of Op Elemwise{add,no_inplace}(Rebroadcast{?,0}.0, TensorConstant{(1, 1, 1, 1) of 0.0}) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
            "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(Subtensor{::, ::, ::int64, ::int64}.0, Elemwise{second,no_inplace}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
            "    c = node.op(*inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Subtensor{::, ::, ::int64, ::int64}.0) of Op Elemwise{add,no_inplace}(Subtensor{::, ::, ::int64, ::int64}.0, TensorConstant{(1, 1, 1, 1) of 0.0}) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
            "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
            "    c = node.op(*inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{add,no_inplace}(dot.0, TensorConstant{(1, 1) of 0.0}) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
            "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
            "    c = node.op(*inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{add,no_inplace}(dot.0, TensorConstant{(1, 1) of 0.0}) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
            "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
            "    c = node.op(*inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{add,no_inplace}(dot.0, TensorConstant{(1, 1) of 0.0}) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_mul_canonizer\n",
            "ERROR (theano.gof.opt): node: Elemwise{mul,no_inplace}(Elemwise{add,no_inplace}.0, TensorConstant{10})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
            "    new = self.merge_num_denum(num, denum)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
            "    return self.main(*num)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 1 (Elemwise{add,no_inplace}.0) of Op Elemwise{mul,no_inplace}(TensorConstant{10}, Elemwise{add,no_inplace}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_mul_canonizer\n",
            "ERROR (theano.gof.opt): node: Elemwise{mul,no_inplace}(<TensorType(int64, scalar)>, TensorConstant{10})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
            "    new = self.merge_num_denum(num, denum)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
            "    return self.main(*num)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 1 (<TensorType(int64, scalar)>) of Op Elemwise{mul,no_inplace}(TensorConstant{10}, <TensorType(int64, scalar)>) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_add_canonizer\n",
            "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(<TensorType(int64, scalar)>, TensorConstant{1})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
            "    new = self.merge_num_denum(num, denum)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
            "    return self.main(*num)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 1 (<TensorType(int64, scalar)>) of Op Elemwise{add,no_inplace}(TensorConstant{1}, <TensorType(int64, scalar)>) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
            "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(Rebroadcast{?,0}.0, Elemwise{second,no_inplace}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
            "    c = node.op(*inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Rebroadcast{?,0}.0) of Op Elemwise{add,no_inplace}(Rebroadcast{?,0}.0, TensorConstant{(1, 1, 1, 1) of 0.0}) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
            "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(Subtensor{::, ::, ::int64, ::int64}.0, Elemwise{second,no_inplace}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
            "    c = node.op(*inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Subtensor{::, ::, ::int64, ::int64}.0) of Op Elemwise{add,no_inplace}(Subtensor{::, ::, ::int64, ::int64}.0, TensorConstant{(1, 1, 1, 1) of 0.0}) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
            "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
            "    c = node.op(*inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{add,no_inplace}(dot.0, TensorConstant{(1, 1) of 0.0}) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
            "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
            "    c = node.op(*inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{add,no_inplace}(dot.0, TensorConstant{(1, 1) of 0.0}) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
            "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
            "    c = node.op(*inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{add,no_inplace}(dot.0, TensorConstant{(1, 1) of 0.0}) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_mul_canonizer\n",
            "ERROR (theano.gof.opt): node: Elemwise{mul,no_inplace}(Elemwise{add,no_inplace}.0, TensorConstant{10})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
            "    new = self.merge_num_denum(num, denum)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
            "    return self.main(*num)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 1 (Elemwise{add,no_inplace}.0) of Op Elemwise{mul,no_inplace}(TensorConstant{10}, Elemwise{add,no_inplace}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_mul_canonizer\n",
            "ERROR (theano.gof.opt): node: Elemwise{mul,no_inplace}(<TensorType(int64, scalar)>, TensorConstant{10})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
            "    new = self.merge_num_denum(num, denum)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
            "    return self.main(*num)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 1 (<TensorType(int64, scalar)>) of Op Elemwise{mul,no_inplace}(TensorConstant{10}, <TensorType(int64, scalar)>) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_add_canonizer\n",
            "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(<TensorType(int64, scalar)>, TensorConstant{1})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
            "    new = self.merge_num_denum(num, denum)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
            "    return self.main(*num)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 1 (<TensorType(int64, scalar)>) of Op Elemwise{add,no_inplace}(TensorConstant{1}, <TensorType(int64, scalar)>) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
            "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(Rebroadcast{?,0}.0, Elemwise{second,no_inplace}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
            "    c = node.op(*inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Rebroadcast{?,0}.0) of Op Elemwise{add,no_inplace}(Rebroadcast{?,0}.0, TensorConstant{(1, 1, 1, 1) of 0.0}) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
            "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(Subtensor{::, ::, ::int64, ::int64}.0, Elemwise{second,no_inplace}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
            "    c = node.op(*inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Subtensor{::, ::, ::int64, ::int64}.0) of Op Elemwise{add,no_inplace}(Subtensor{::, ::, ::int64, ::int64}.0, TensorConstant{(1, 1, 1, 1) of 0.0}) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
            "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
            "    c = node.op(*inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{add,no_inplace}(dot.0, TensorConstant{(1, 1) of 0.0}) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
            "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
            "    c = node.op(*inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{add,no_inplace}(dot.0, TensorConstant{(1, 1) of 0.0}) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_sink\n",
            "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1621, in local_fill_sink\n",
            "    c = node.op(*inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{add,no_inplace}(dot.0, TensorConstant{(1, 1) of 0.0}) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_mul_canonizer\n",
            "ERROR (theano.gof.opt): node: Elemwise{mul,no_inplace}(Elemwise{add,no_inplace}.0, TensorConstant{10})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
            "    new = self.merge_num_denum(num, denum)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
            "    return self.main(*num)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 1 (Elemwise{add,no_inplace}.0) of Op Elemwise{mul,no_inplace}(TensorConstant{10}, Elemwise{add,no_inplace}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_mul_canonizer\n",
            "ERROR (theano.gof.opt): node: Elemwise{mul,no_inplace}(<TensorType(int64, scalar)>, TensorConstant{10})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
            "    new = self.merge_num_denum(num, denum)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
            "    return self.main(*num)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 1 (<TensorType(int64, scalar)>) of Op Elemwise{mul,no_inplace}(TensorConstant{10}, <TensorType(int64, scalar)>) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_add_canonizer\n",
            "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(<TensorType(int64, scalar)>, TensorConstant{1})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4505, in transform\n",
            "    new = self.merge_num_denum(num, denum)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 4326, in merge_num_denum\n",
            "    return self.main(*num)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 1 (<TensorType(int64, scalar)>) of Op Elemwise{add,no_inplace}(TensorConstant{1}, <TensorType(int64, scalar)>) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
            "ERROR (theano.gof.opt): node: Elemwise{second}(AdvancedSubtensor.0, Elemwise{true_div}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
            "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
            "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
            "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Elemwise{true_div}.0) of Op Alloc(Elemwise{true_div}.0, Elemwise{sub,no_inplace}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
            "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(Elemwise{log,no_inplace}.0, TensorConstant{(1, 1) of 0.0})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
            "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
            "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
            "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 2 (Shape_i{1}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, TensorConstant{10}, Shape_i{1}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
            "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(w, TensorConstant{(1, 1) of 0.0})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
            "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
            "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
            "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
            "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(w, TensorConstant{(1, 1) of 0.0})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
            "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
            "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
            "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
            "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(w, TensorConstant{(1, 1) of 0.0})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
            "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
            "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
            "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
            "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(<TensorType(float64, 4D)>, TensorConstant{(1, 1, 1, 1) of 0.0})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
            "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
            "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
            "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1, 1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0, Shape_i{2}.0, Shape_i{3}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
            "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(<TensorType(float64, 4D)>, TensorConstant{(1, 1, 1, 1) of 0.0})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
            "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
            "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
            "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1, 1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0, Shape_i{2}.0, Shape_i{3}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
            "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(<TensorType(float64, 4D)>, TensorConstant{(1, 1, 1, 1) of 0.0})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
            "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
            "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
            "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1, 1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0, Shape_i{2}.0, Shape_i{3}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
            "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(<TensorType(float64, 4D)>, TensorConstant{(1, 1, 1, 1) of 0.0})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
            "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
            "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
            "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1, 1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0, Shape_i{2}.0, Shape_i{3}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
            "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(w, TensorConstant{(1, 1) of 0.0})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
            "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
            "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
            "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
            "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(w, TensorConstant{(1, 1) of 0.0})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
            "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
            "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
            "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
            "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(w, TensorConstant{(1, 1) of 0.0})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
            "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
            "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
            "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
            "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(Elemwise{log,no_inplace}.0, TensorConstant{(1, 1) of 0.0})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
            "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
            "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
            "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 2 (Shape_i{1}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, TensorConstant{10}, Shape_i{1}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
            "ERROR (theano.gof.opt): node: Elemwise{second}(AdvancedSubtensor.0, Elemwise{true_div}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
            "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
            "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
            "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Elemwise{true_div}.0) of Op Alloc(Elemwise{true_div}.0, Elemwise{sub,no_inplace}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_dot_to_dot22\n",
            "ERROR (theano.gof.opt): node: dot(Elemwise{mul,no_inplace}.0, w)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/blas.py\", line 1633, in local_dot_to_dot22\n",
            "    return [_dot22(*node.inputs)]\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Elemwise{mul,no_inplace}.0) of Op Dot22(Elemwise{mul,no_inplace}.0, w) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_dot_to_dot22\n",
            "ERROR (theano.gof.opt): node: dot(Elemwise{mul,no_inplace}.0, w)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/blas.py\", line 1633, in local_dot_to_dot22\n",
            "    return [_dot22(*node.inputs)]\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Elemwise{mul,no_inplace}.0) of Op Dot22(Elemwise{mul,no_inplace}.0, w) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_dot_to_dot22\n",
            "ERROR (theano.gof.opt): node: dot(Elemwise{mul,no_inplace}.0, w)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/blas.py\", line 1633, in local_dot_to_dot22\n",
            "    return [_dot22(*node.inputs)]\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Elemwise{mul,no_inplace}.0) of Op Dot22(Elemwise{mul,no_inplace}.0, w) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_dot_to_dot22\n",
            "ERROR (theano.gof.opt): node: dot(DimShuffle{1,0}.0, SoftmaxGrad.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/blas.py\", line 1633, in local_dot_to_dot22\n",
            "    return [_dot22(*node.inputs)]\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (DimShuffle{1,0}.0) of Op Dot22(DimShuffle{1,0}.0, SoftmaxGrad.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_dot_to_dot22\n",
            "ERROR (theano.gof.opt): node: dot(SoftmaxGrad.0, w.T)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/blas.py\", line 1633, in local_dot_to_dot22\n",
            "    return [_dot22(*node.inputs)]\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (SoftmaxGrad.0) of Op Dot22(SoftmaxGrad.0, w.T) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_dot_to_dot22\n",
            "ERROR (theano.gof.opt): node: dot(DimShuffle{1,0}.0, Elemwise{mul}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/blas.py\", line 1633, in local_dot_to_dot22\n",
            "    return [_dot22(*node.inputs)]\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (DimShuffle{1,0}.0) of Op Dot22(DimShuffle{1,0}.0, Elemwise{mul}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_dot_to_dot22\n",
            "ERROR (theano.gof.opt): node: dot(Elemwise{mul}.0, w.T)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/blas.py\", line 1633, in local_dot_to_dot22\n",
            "    return [_dot22(*node.inputs)]\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Elemwise{mul}.0) of Op Dot22(Elemwise{mul}.0, w.T) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_dot_to_dot22\n",
            "ERROR (theano.gof.opt): node: dot(DimShuffle{1,0}.0, Elemwise{mul}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/blas.py\", line 1633, in local_dot_to_dot22\n",
            "    return [_dot22(*node.inputs)]\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (DimShuffle{1,0}.0) of Op Dot22(DimShuffle{1,0}.0, Elemwise{mul}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_dot_to_dot22\n",
            "ERROR (theano.gof.opt): node: dot(Elemwise{mul}.0, w.T)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/blas.py\", line 1633, in local_dot_to_dot22\n",
            "    return [_dot22(*node.inputs)]\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Elemwise{mul}.0) of Op Dot22(Elemwise{mul}.0, w.T) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_add_specialize\n",
            "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(Rebroadcast{?,0}.0, Elemwise{second,no_inplace}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5485, in local_add_specialize\n",
            "    ret = fill_chain(new_inputs[0])\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5458, in fill_chain\n",
            "    out = _fill_chain(v, node.inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 142, in _fill_chain\n",
            "    new_out = T.fill(i, new_out)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Rebroadcast{?,0}.0) of Op Elemwise{second,no_inplace}(Rebroadcast{?,0}.0, Rebroadcast{?,0}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_add_specialize\n",
            "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(Subtensor{::, ::, ::int64, ::int64}.0, Elemwise{second,no_inplace}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5485, in local_add_specialize\n",
            "    ret = fill_chain(new_inputs[0])\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5458, in fill_chain\n",
            "    out = _fill_chain(v, node.inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 142, in _fill_chain\n",
            "    new_out = T.fill(i, new_out)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Subtensor{::, ::, ::int64, ::int64}.0) of Op Elemwise{second,no_inplace}(Subtensor{::, ::, ::int64, ::int64}.0, Subtensor{::, ::, ::int64, ::int64}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_add_specialize\n",
            "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5485, in local_add_specialize\n",
            "    ret = fill_chain(new_inputs[0])\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5458, in fill_chain\n",
            "    out = _fill_chain(v, node.inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 142, in _fill_chain\n",
            "    new_out = T.fill(i, new_out)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{second,no_inplace}(dot.0, dot.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_add_specialize\n",
            "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5485, in local_add_specialize\n",
            "    ret = fill_chain(new_inputs[0])\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5458, in fill_chain\n",
            "    out = _fill_chain(v, node.inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 142, in _fill_chain\n",
            "    new_out = T.fill(i, new_out)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{second,no_inplace}(dot.0, dot.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_add_specialize\n",
            "ERROR (theano.gof.opt): node: Elemwise{add,no_inplace}(dot.0, Elemwise{second,no_inplace}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5485, in local_add_specialize\n",
            "    ret = fill_chain(new_inputs[0])\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5458, in fill_chain\n",
            "    out = _fill_chain(v, node.inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 142, in _fill_chain\n",
            "    new_out = T.fill(i, new_out)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (dot.0) of Op Elemwise{second,no_inplace}(dot.0, dot.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_advanced_indexing_crossentropy_onehot_grad\n",
            "ERROR (theano.gof.opt): node: SoftmaxGrad(Elemwise{true_div}.0, Softmax.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/nnet/nnet.py\", line 1907, in local_advanced_indexing_crossentropy_onehot_grad\n",
            "    out_grad = -incr\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/var.py\", line 39, in __neg__\n",
            "    return theano.tensor.basic.neg(self)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Elemwise{second}.0) of Op Elemwise{neg,no_inplace}(Elemwise{second}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_mul_specialize\n",
            "ERROR (theano.gof.opt): node: Elemwise{mul,no_inplace}(TensorConstant{-1.0}, mean)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 5427, in local_mul_specialize\n",
            "    rval = -new_inputs[0]\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/var.py\", line 39, in __neg__\n",
            "    return theano.tensor.basic.neg(self)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (mean) of Op Elemwise{neg,no_inplace}(mean) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
            "ERROR (theano.gof.opt): node: Elemwise{second}(AdvancedSubtensor.0, Elemwise{true_div}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
            "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
            "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
            "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Elemwise{true_div}.0) of Op Alloc(Elemwise{true_div}.0, Elemwise{sub,no_inplace}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
            "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(Elemwise{log,no_inplace}.0, TensorConstant{(1, 1) of 0.0})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
            "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
            "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
            "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 2 (Shape_i{1}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, TensorConstant{10}, Shape_i{1}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_advanced_indexing_crossentropy_onehot\n",
            "ERROR (theano.gof.opt): node: AdvancedSubtensor(Elemwise{log,no_inplace}.0, ARange{dtype='int64'}.0, Elemwise{Cast{int32}}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/nnet/nnet.py\", line 1692, in local_advanced_indexing_crossentropy_onehot\n",
            "    b_var = tensor.zeros_like(x_var[0])\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/var.py\", line 532, in __getitem__\n",
            "    lambda entry: isinstance(entry, Variable)))\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Elemwise{add,no_inplace}.0) of Op Subtensor{int64}(Elemwise{add,no_inplace}.0, Constant{0}) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_logsoftmax\n",
            "ERROR (theano.gof.opt): node: Elemwise{log,no_inplace}(Softmax.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/nnet/nnet.py\", line 756, in local_logsoftmax\n",
            "    ret = new_op(inVars)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Elemwise{add,no_inplace}.0) of Op LogSoftmax(Elemwise{add,no_inplace}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
            "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(w, TensorConstant{(1, 1) of 0.0})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
            "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
            "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
            "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
            "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(w, TensorConstant{(1, 1) of 0.0})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
            "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
            "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
            "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
            "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(w, TensorConstant{(1, 1) of 0.0})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
            "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
            "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
            "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
            "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(<TensorType(float64, 4D)>, TensorConstant{(1, 1, 1, 1) of 0.0})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
            "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
            "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
            "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1, 1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0, Shape_i{2}.0, Shape_i{3}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: local_fill_to_alloc\n",
            "ERROR (theano.gof.opt): node: Elemwise{second,no_inplace}(<TensorType(float64, 4D)>, TensorConstant{(1, 1, 1, 1) of 0.0})\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 1669, in local_fill_to_alloc\n",
            "    o = broadcast_like(v, r, node.fgraph, dtype=v.dtype)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 208, in broadcast_like\n",
            "    rval = T.alloc(T.cast(value, dtype), *new_shape)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/basic.py\", line 2810, in __call__\n",
            "    ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 1 (Shape_i{0}.0) of Op Alloc(TensorConstant{(1, 1, 1, 1) of 0.0}, Shape_i{0}.0, Shape_i{1}.0, Shape_i{2}.0, Shape_i{3}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): SeqOptimizer apply <theano.tensor.opt.FusionOptimizer object at 0x7f862ca00ad0>\n",
            "ERROR (theano.gof.opt): Traceback:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 230, in apply\n",
            "    sub_prof = optimizer.optimize(fgraph)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 89, in optimize\n",
            "    ret = self.apply(fgraph, *args, **kwargs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 6658, in apply\n",
            "    new_outputs = self.optimizer(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 6502, in local_fuse\n",
            "    return_list=True)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 1 (<float64>) of Op mul(<float64>, <float64>) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): SeqOptimizer apply <theano.compile.mode.AddDestroyHandler object at 0x7f8649eada50>\n",
            "ERROR (theano.gof.opt): Traceback:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 230, in apply\n",
            "    sub_prof = optimizer.optimize(fgraph)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 89, in optimize\n",
            "    ret = self.apply(fgraph, *args, **kwargs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/compile/mode.py\", line 134, in apply\n",
            "    fgraph.replace_validate(o, _output_guard(o),\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Elemwise{mul,no_inplace}.0) of Op OutputGuard(Elemwise{mul,no_inplace}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: dimshuffle_as_view\n",
            "ERROR (theano.gof.opt): node: DimShuffle{1,0,2,3}(Reshape{4}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 644, in dimshuffle_as_view\n",
            "    v = new_op(*node.inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Reshape{4}.0) of Op InplaceDimShuffle{1,0,2,3}(Reshape{4}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: dimshuffle_as_view\n",
            "ERROR (theano.gof.opt): node: DimShuffle{1,0,2,3}(Reshape{4}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 644, in dimshuffle_as_view\n",
            "    v = new_op(*node.inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Reshape{4}.0) of Op InplaceDimShuffle{1,0,2,3}(Reshape{4}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: dimshuffle_as_view\n",
            "ERROR (theano.gof.opt): node: DimShuffle{x}(Elemwise{Cast{float64}}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 644, in dimshuffle_as_view\n",
            "    v = new_op(*node.inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Elemwise{Cast{float64}}.0) of Op InplaceDimShuffle{x}(Elemwise{Cast{float64}}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: dimshuffle_as_view\n",
            "ERROR (theano.gof.opt): node: DimShuffle{1,0}(Elemwise{mul,no_inplace}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 644, in dimshuffle_as_view\n",
            "    v = new_op(*node.inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Elemwise{mul,no_inplace}.0) of Op InplaceDimShuffle{1,0}(Elemwise{mul,no_inplace}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: dimshuffle_as_view\n",
            "ERROR (theano.gof.opt): node: DimShuffle{1,0}(Elemwise{mul,no_inplace}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 644, in dimshuffle_as_view\n",
            "    v = new_op(*node.inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Elemwise{mul,no_inplace}.0) of Op InplaceDimShuffle{1,0}(Elemwise{mul,no_inplace}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: dimshuffle_as_view\n",
            "ERROR (theano.gof.opt): node: DimShuffle{1,0}(Elemwise{mul,no_inplace}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 644, in dimshuffle_as_view\n",
            "    v = new_op(*node.inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (Elemwise{mul,no_inplace}.0) of Op InplaceDimShuffle{1,0}(Elemwise{mul,no_inplace}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: dimshuffle_as_view\n",
            "ERROR (theano.gof.opt): node: DimShuffle{1,0,2,3}(MaxPoolGrad{ds=(2, 2), ignore_border=True, st=(2, 2), padding=(0, 0), mode='max'}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 644, in dimshuffle_as_view\n",
            "    v = new_op(*node.inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (MaxPoolGrad{ds=(2, 2), ignore_border=True, st=(2, 2), padding=(0, 0), mode='max'}.0) of Op InplaceDimShuffle{1,0,2,3}(MaxPoolGrad{ds=(2, 2), ignore_border=True, st=(2, 2), padding=(0, 0), mode='max'}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: dimshuffle_as_view\n",
            "ERROR (theano.gof.opt): node: DimShuffle{1,0,2,3}(ConvOp{('imshp', (10, 12, 12)),('kshp', (8, 8)),('nkern', 40),('bsize', 20),('dx', 1),('dy', 1),('out_mode', 'valid'),('unroll_batch', 5),('unroll_kern', 2),('unroll_patch', False),('imshp_logical', (10, 12, 12)),('kshp_logical', (8, 8)),('kshp_logical_top_aligned', False)}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 644, in dimshuffle_as_view\n",
            "    v = new_op(*node.inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (ConvOp{('imshp', (10, 12, 12)),('kshp', (8, 8)),('nkern', 40),('bsize', 20),('dx', 1),('dy', 1),('out_mode', 'valid'),('unroll_batch', 5),('unroll_kern', 2),('unroll_patch', False),('imshp_logical', (10, 12, 12)),('kshp_logical', (8, 8)),('kshp_logical_top_aligned', False)}.0) of Op InplaceDimShuffle{1,0,2,3}(ConvOp{('imshp', (10, 12, 12)),('kshp', (8, 8)),('nkern', 40),('bsize', 20),('dx', 1),('dy', 1),('out_mode', 'valid'),('unroll_batch', 5),('unroll_kern', 2),('unroll_patch', False),('imshp_logical', (10, 12, 12)),('kshp_logical', (8, 8)),('kshp_logical_top_aligned', False)}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: dimshuffle_as_view\n",
            "ERROR (theano.gof.opt): node: DimShuffle{1,0,2,3}(MaxPoolGrad{ds=(2, 2), ignore_border=True, st=(2, 2), padding=(0, 0), mode='max'}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 644, in dimshuffle_as_view\n",
            "    v = new_op(*node.inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (MaxPoolGrad{ds=(2, 2), ignore_border=True, st=(2, 2), padding=(0, 0), mode='max'}.0) of Op InplaceDimShuffle{1,0,2,3}(MaxPoolGrad{ds=(2, 2), ignore_border=True, st=(2, 2), padding=(0, 0), mode='max'}.0) missing default value\n",
            "\n",
            "ERROR (theano.gof.opt): Optimization failure due to: dimshuffle_as_view\n",
            "ERROR (theano.gof.opt): node: DimShuffle{1,0,2,3}(ConvOp{('imshp', (10, 28, 28)),('kshp', (24, 24)),('nkern', 20),('bsize', 1),('dx', 1),('dy', 1),('out_mode', 'valid'),('unroll_batch', 1),('unroll_kern', 10),('unroll_patch', False),('imshp_logical', (10, 28, 28)),('kshp_logical', (24, 24)),('kshp_logical_top_aligned', False)}.0)\n",
            "ERROR (theano.gof.opt): TRACEBACK:\n",
            "ERROR (theano.gof.opt): Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/opt.py\", line 1772, in process_node\n",
            "    replacements = lopt.transform(node)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/tensor/opt.py\", line 644, in dimshuffle_as_view\n",
            "    v = new_op(*node.inputs)\n",
            "  File \"/home/ubuntu/miniconda2/lib/python2.7/site-packages/theano/gof/op.py\", line 633, in __call__\n",
            "    (i, ins, node))\n",
            "ValueError: Cannot compute test value: input 0 (ConvOp{('imshp', (10, 28, 28)),('kshp', (24, 24)),('nkern', 20),('bsize', 1),('dx', 1),('dy', 1),('out_mode', 'valid'),('unroll_batch', 1),('unroll_kern', 10),('unroll_patch', False),('imshp_logical', (10, 28, 28)),('kshp_logical', (24, 24)),('kshp_logical_top_aligned', False)}.0) of Op InplaceDimShuffle{1,0,2,3}(ConvOp{('imshp', (10, 28, 28)),('kshp', (24, 24)),('nkern', 20),('bsize', 1),('dx', 1),('dy', 1),('out_mode', 'valid'),('unroll_batch', 1),('unroll_kern', 10),('unroll_patch', False),('imshp_logical', (10, 28, 28)),('kshp_logical', (24, 24)),('kshp_logical_top_aligned', False)}.0) missing default value\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training mini-batch number 0\n",
            "Training mini-batch number 1000\n",
            "Training mini-batch number 2000\n",
            "Training mini-batch number 3000\n",
            "Training mini-batch number 4000\n",
            "Training mini-batch number 5000\n",
            "Training mini-batch number 6000\n",
            "Training mini-batch number 7000\n",
            "Training mini-batch number 8000\n",
            "Training mini-batch number 9000\n",
            "Training mini-batch number 10000\n",
            "Training mini-batch number 11000\n",
            "Training mini-batch number 12000\n",
            "Training mini-batch number 13000\n",
            "Training mini-batch number 14000\n",
            "Training mini-batch number 15000\n",
            "Training mini-batch number 16000\n",
            "Training mini-batch number 17000\n",
            "Training mini-batch number 18000\n",
            "Training mini-batch number 19000\n",
            "Training mini-batch number 20000\n",
            "Training mini-batch number 21000\n",
            "Training mini-batch number 22000\n",
            "Training mini-batch number 23000\n",
            "Training mini-batch number 24000\n",
            "Epoch 0: validation accuracy 98.63%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 98.81%\n",
            "Training mini-batch number 25000\n",
            "Training mini-batch number 26000\n",
            "Training mini-batch number 27000\n",
            "Training mini-batch number 28000\n",
            "Training mini-batch number 29000\n",
            "Training mini-batch number 30000\n",
            "Training mini-batch number 31000\n",
            "Training mini-batch number 32000\n",
            "Training mini-batch number 33000\n",
            "Training mini-batch number 34000\n",
            "Training mini-batch number 35000\n",
            "Training mini-batch number 36000\n",
            "Training mini-batch number 37000\n",
            "Training mini-batch number 38000\n",
            "Training mini-batch number 39000\n",
            "Training mini-batch number 40000\n",
            "Training mini-batch number 41000\n",
            "Training mini-batch number 42000\n",
            "Training mini-batch number 43000\n",
            "Training mini-batch number 44000\n",
            "Training mini-batch number 45000\n",
            "Training mini-batch number 46000\n",
            "Training mini-batch number 47000\n",
            "Training mini-batch number 48000\n",
            "Training mini-batch number 49000\n",
            "Epoch 1: validation accuracy 99.01%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 99.07%\n",
            "Training mini-batch number 50000\n",
            "Training mini-batch number 51000\n",
            "Training mini-batch number 52000\n",
            "Training mini-batch number 53000\n",
            "Training mini-batch number 54000\n",
            "Training mini-batch number 55000\n",
            "Training mini-batch number 56000\n",
            "Training mini-batch number 57000\n",
            "Training mini-batch number 58000\n",
            "Training mini-batch number 59000\n",
            "Training mini-batch number 60000\n",
            "Training mini-batch number 61000\n",
            "Training mini-batch number 62000\n",
            "Training mini-batch number 63000\n",
            "Training mini-batch number 64000\n",
            "Training mini-batch number 65000\n",
            "Training mini-batch number 66000\n",
            "Training mini-batch number 67000\n",
            "Training mini-batch number 68000\n",
            "Training mini-batch number 69000\n",
            "Training mini-batch number 70000\n",
            "Training mini-batch number 71000\n",
            "Training mini-batch number 72000\n",
            "Training mini-batch number 73000\n",
            "Training mini-batch number 74000\n",
            "Epoch 2: validation accuracy 99.17%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 99.22%\n",
            "Training mini-batch number 75000\n",
            "Training mini-batch number 76000\n",
            "Training mini-batch number 77000\n",
            "Training mini-batch number 78000\n",
            "Training mini-batch number 79000\n",
            "Training mini-batch number 80000\n",
            "Training mini-batch number 81000\n",
            "Training mini-batch number 82000\n",
            "Training mini-batch number 83000\n",
            "Training mini-batch number 84000\n",
            "Training mini-batch number 85000\n",
            "Training mini-batch number 86000\n",
            "Training mini-batch number 87000\n",
            "Training mini-batch number 88000\n",
            "Training mini-batch number 89000\n",
            "Training mini-batch number 90000\n",
            "Training mini-batch number 91000\n",
            "Training mini-batch number 92000\n",
            "Training mini-batch number 93000\n",
            "Training mini-batch number 94000\n",
            "Training mini-batch number 95000\n",
            "Training mini-batch number 96000\n",
            "Training mini-batch number 97000\n",
            "Training mini-batch number 98000\n",
            "Training mini-batch number 99000\n",
            "Epoch 3: validation accuracy 99.25%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 99.33%\n",
            "Training mini-batch number 100000\n",
            "Training mini-batch number 101000\n",
            "Training mini-batch number 102000\n",
            "Training mini-batch number 103000\n",
            "Training mini-batch number 104000\n",
            "Training mini-batch number 105000\n",
            "Training mini-batch number 106000\n",
            "Training mini-batch number 107000\n",
            "Training mini-batch number 108000\n",
            "Training mini-batch number 109000\n",
            "Training mini-batch number 110000\n",
            "Training mini-batch number 111000\n",
            "Training mini-batch number 112000\n",
            "Training mini-batch number 113000\n",
            "Training mini-batch number 114000\n",
            "Training mini-batch number 115000\n",
            "Training mini-batch number 116000\n",
            "Training mini-batch number 117000\n",
            "Training mini-batch number 118000\n",
            "Training mini-batch number 119000\n",
            "Training mini-batch number 120000\n",
            "Training mini-batch number 121000\n",
            "Training mini-batch number 122000\n",
            "Training mini-batch number 123000\n",
            "Training mini-batch number 124000\n",
            "Epoch 4: validation accuracy 99.29%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 99.44%\n",
            "Training mini-batch number 125000\n",
            "Training mini-batch number 126000\n",
            "Training mini-batch number 127000\n",
            "Training mini-batch number 128000\n",
            "Training mini-batch number 129000\n",
            "Training mini-batch number 130000\n",
            "Training mini-batch number 131000\n",
            "Training mini-batch number 132000\n",
            "Training mini-batch number 133000\n",
            "Training mini-batch number 134000\n",
            "Training mini-batch number 135000\n",
            "Training mini-batch number 136000\n",
            "Training mini-batch number 137000\n",
            "Training mini-batch number 138000\n",
            "Training mini-batch number 139000\n",
            "Training mini-batch number 140000\n",
            "Training mini-batch number 141000\n",
            "Training mini-batch number 142000\n",
            "Training mini-batch number 143000\n",
            "Training mini-batch number 144000\n",
            "Training mini-batch number 145000\n",
            "Training mini-batch number 146000\n",
            "Training mini-batch number 147000\n",
            "Training mini-batch number 148000\n",
            "Training mini-batch number 149000\n",
            "Epoch 5: validation accuracy 99.34%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 99.47%\n",
            "Training mini-batch number 150000\n",
            "Training mini-batch number 151000\n",
            "Training mini-batch number 152000\n",
            "Training mini-batch number 153000\n",
            "Training mini-batch number 154000\n",
            "Training mini-batch number 155000\n",
            "Training mini-batch number 156000\n",
            "Training mini-batch number 157000\n",
            "Training mini-batch number 158000\n",
            "Training mini-batch number 159000\n",
            "Training mini-batch number 160000\n",
            "Training mini-batch number 161000\n",
            "Training mini-batch number 162000\n",
            "Training mini-batch number 163000\n",
            "Training mini-batch number 164000\n",
            "Training mini-batch number 165000\n",
            "Training mini-batch number 166000\n",
            "Training mini-batch number 167000\n",
            "Training mini-batch number 168000\n",
            "Training mini-batch number 169000\n",
            "Training mini-batch number 170000\n",
            "Training mini-batch number 171000\n",
            "Training mini-batch number 172000\n",
            "Training mini-batch number 173000\n",
            "Training mini-batch number 174000\n",
            "Epoch 6: validation accuracy 99.39%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 99.54%\n",
            "Training mini-batch number 175000\n",
            "Training mini-batch number 176000\n",
            "Training mini-batch number 177000\n",
            "Training mini-batch number 178000\n",
            "Training mini-batch number 179000\n",
            "Training mini-batch number 180000\n",
            "Training mini-batch number 181000\n",
            "Training mini-batch number 182000\n",
            "Training mini-batch number 183000\n",
            "Training mini-batch number 184000\n",
            "Training mini-batch number 185000\n",
            "Training mini-batch number 186000\n",
            "Training mini-batch number 187000\n",
            "Training mini-batch number 188000\n",
            "Training mini-batch number 189000\n",
            "Training mini-batch number 190000\n",
            "Training mini-batch number 191000\n",
            "Training mini-batch number 192000\n",
            "Training mini-batch number 193000\n",
            "Training mini-batch number 194000\n",
            "Training mini-batch number 195000\n",
            "Training mini-batch number 196000\n",
            "Training mini-batch number 197000\n",
            "Training mini-batch number 198000\n",
            "Training mini-batch number 199000\n",
            "Epoch 7: validation accuracy 99.37%\n",
            "Training mini-batch number 200000\n",
            "Training mini-batch number 201000\n",
            "Training mini-batch number 202000\n",
            "Training mini-batch number 203000\n",
            "Training mini-batch number 204000\n",
            "Training mini-batch number 205000\n",
            "Training mini-batch number 206000\n",
            "Training mini-batch number 207000\n",
            "Training mini-batch number 208000\n",
            "Training mini-batch number 209000\n",
            "Training mini-batch number 210000\n",
            "Training mini-batch number 211000\n",
            "Training mini-batch number 212000\n",
            "Training mini-batch number 213000\n",
            "Training mini-batch number 214000\n",
            "Training mini-batch number 215000\n",
            "Training mini-batch number 216000\n",
            "Training mini-batch number 217000\n",
            "Training mini-batch number 218000\n",
            "Training mini-batch number 219000\n",
            "Training mini-batch number 220000\n",
            "Training mini-batch number 221000\n",
            "Training mini-batch number 222000\n",
            "Training mini-batch number 223000\n",
            "Training mini-batch number 224000\n",
            "Epoch 8: validation accuracy 99.40%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 99.47%\n",
            "Training mini-batch number 225000\n",
            "Training mini-batch number 226000\n",
            "Training mini-batch number 227000\n",
            "Training mini-batch number 228000\n",
            "Training mini-batch number 229000\n",
            "Training mini-batch number 230000\n",
            "Training mini-batch number 231000\n",
            "Training mini-batch number 232000\n",
            "Training mini-batch number 233000\n",
            "Training mini-batch number 234000\n",
            "Training mini-batch number 235000\n",
            "Training mini-batch number 236000\n",
            "Training mini-batch number 237000\n",
            "Training mini-batch number 238000\n",
            "Training mini-batch number 239000\n",
            "Training mini-batch number 240000\n",
            "Training mini-batch number 241000\n",
            "Training mini-batch number 242000\n",
            "Training mini-batch number 243000\n",
            "Training mini-batch number 244000\n",
            "Training mini-batch number 245000\n",
            "Training mini-batch number 246000\n",
            "Training mini-batch number 247000\n",
            "Training mini-batch number 248000\n",
            "Training mini-batch number 249000\n",
            "Epoch 9: validation accuracy 99.46%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 99.51%\n",
            "Training mini-batch number 250000\n",
            "Training mini-batch number 251000\n",
            "Training mini-batch number 252000\n",
            "Training mini-batch number 253000\n",
            "Training mini-batch number 254000\n",
            "Training mini-batch number 255000\n",
            "Training mini-batch number 256000\n",
            "Training mini-batch number 257000\n",
            "Training mini-batch number 258000\n",
            "Training mini-batch number 259000\n",
            "Training mini-batch number 260000\n",
            "Training mini-batch number 261000\n",
            "Training mini-batch number 262000\n",
            "Training mini-batch number 263000\n",
            "Training mini-batch number 264000\n",
            "Training mini-batch number 265000\n",
            "Training mini-batch number 266000\n",
            "Training mini-batch number 267000\n",
            "Training mini-batch number 268000\n",
            "Training mini-batch number 269000\n",
            "Training mini-batch number 270000\n",
            "Training mini-batch number 271000\n",
            "Training mini-batch number 272000\n",
            "Training mini-batch number 273000\n",
            "Training mini-batch number 274000\n",
            "Epoch 10: validation accuracy 99.49%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 99.57%\n",
            "Training mini-batch number 275000\n",
            "Training mini-batch number 276000\n",
            "Training mini-batch number 277000\n",
            "Training mini-batch number 278000\n",
            "Training mini-batch number 279000\n",
            "Training mini-batch number 280000\n",
            "Training mini-batch number 281000\n",
            "Training mini-batch number 282000\n",
            "Training mini-batch number 283000\n",
            "Training mini-batch number 284000\n",
            "Training mini-batch number 285000\n",
            "Training mini-batch number 286000\n",
            "Training mini-batch number 287000\n",
            "Training mini-batch number 288000\n",
            "Training mini-batch number 289000\n",
            "Training mini-batch number 290000\n",
            "Training mini-batch number 291000\n",
            "Training mini-batch number 292000\n",
            "Training mini-batch number 293000\n",
            "Training mini-batch number 294000\n",
            "Training mini-batch number 295000\n",
            "Training mini-batch number 296000\n",
            "Training mini-batch number 297000\n",
            "Training mini-batch number 298000\n",
            "Training mini-batch number 299000\n",
            "Epoch 11: validation accuracy 99.40%\n",
            "Training mini-batch number 300000\n",
            "Training mini-batch number 301000\n",
            "Training mini-batch number 302000\n",
            "Training mini-batch number 303000\n",
            "Training mini-batch number 304000\n",
            "Training mini-batch number 305000\n",
            "Training mini-batch number 306000\n",
            "Training mini-batch number 307000\n",
            "Training mini-batch number 308000\n",
            "Training mini-batch number 309000\n",
            "Training mini-batch number 310000\n",
            "Training mini-batch number 311000\n",
            "Training mini-batch number 312000\n",
            "Training mini-batch number 313000\n",
            "Training mini-batch number 314000\n",
            "Training mini-batch number 315000\n",
            "Training mini-batch number 316000\n",
            "Training mini-batch number 317000\n",
            "Training mini-batch number 318000\n",
            "Training mini-batch number 319000\n",
            "Training mini-batch number 320000\n",
            "Training mini-batch number 321000\n",
            "Training mini-batch number 322000\n",
            "Training mini-batch number 323000\n",
            "Training mini-batch number 324000\n",
            "Epoch 12: validation accuracy 99.41%\n",
            "Training mini-batch number 325000\n",
            "Training mini-batch number 326000\n",
            "Training mini-batch number 327000\n",
            "Training mini-batch number 328000\n",
            "Training mini-batch number 329000\n",
            "Training mini-batch number 330000\n",
            "Training mini-batch number 331000\n",
            "Training mini-batch number 332000\n",
            "Training mini-batch number 333000\n",
            "Training mini-batch number 334000\n",
            "Training mini-batch number 335000\n",
            "Training mini-batch number 336000\n",
            "Training mini-batch number 337000\n",
            "Training mini-batch number 338000\n",
            "Training mini-batch number 339000\n",
            "Training mini-batch number 340000\n",
            "Training mini-batch number 341000\n",
            "Training mini-batch number 342000\n",
            "Training mini-batch number 343000\n",
            "Training mini-batch number 344000\n",
            "Training mini-batch number 345000\n",
            "Training mini-batch number 346000\n",
            "Training mini-batch number 347000\n",
            "Training mini-batch number 348000\n",
            "Training mini-batch number 349000\n",
            "Epoch 13: validation accuracy 99.35%\n",
            "Training mini-batch number 350000\n",
            "Training mini-batch number 351000\n",
            "Training mini-batch number 352000\n",
            "Training mini-batch number 353000\n",
            "Training mini-batch number 354000\n",
            "Training mini-batch number 355000\n",
            "Training mini-batch number 356000\n",
            "Training mini-batch number 357000\n",
            "Training mini-batch number 358000\n",
            "Training mini-batch number 359000\n",
            "Training mini-batch number 360000\n",
            "Training mini-batch number 361000\n",
            "Training mini-batch number 362000\n",
            "Training mini-batch number 363000\n",
            "Training mini-batch number 364000\n",
            "Training mini-batch number 365000\n",
            "Training mini-batch number 366000\n",
            "Training mini-batch number 367000\n",
            "Training mini-batch number 368000\n",
            "Training mini-batch number 369000\n",
            "Training mini-batch number 370000\n",
            "Training mini-batch number 371000\n",
            "Training mini-batch number 372000\n",
            "Training mini-batch number 373000\n",
            "Training mini-batch number 374000\n",
            "Epoch 14: validation accuracy 99.47%\n",
            "Training mini-batch number 375000\n",
            "Training mini-batch number 376000\n",
            "Training mini-batch number 377000\n",
            "Training mini-batch number 378000\n",
            "Training mini-batch number 379000\n",
            "Training mini-batch number 380000\n",
            "Training mini-batch number 381000\n",
            "Training mini-batch number 382000\n",
            "Training mini-batch number 383000\n",
            "Training mini-batch number 384000\n",
            "Training mini-batch number 385000\n",
            "Training mini-batch number 386000\n",
            "Training mini-batch number 387000\n",
            "Training mini-batch number 388000\n",
            "Training mini-batch number 389000\n",
            "Training mini-batch number 390000\n",
            "Training mini-batch number 391000\n",
            "Training mini-batch number 392000\n",
            "Training mini-batch number 393000\n",
            "Training mini-batch number 394000\n",
            "Training mini-batch number 395000\n",
            "Training mini-batch number 396000\n",
            "Training mini-batch number 397000\n",
            "Training mini-batch number 398000\n",
            "Training mini-batch number 399000\n",
            "Epoch 15: validation accuracy 99.45%\n",
            "Training mini-batch number 400000\n",
            "Training mini-batch number 401000\n",
            "Training mini-batch number 402000\n",
            "Training mini-batch number 403000\n",
            "Training mini-batch number 404000\n",
            "Training mini-batch number 405000\n",
            "Training mini-batch number 406000\n",
            "Training mini-batch number 407000\n",
            "Training mini-batch number 408000\n",
            "Training mini-batch number 409000\n",
            "Training mini-batch number 410000\n",
            "Training mini-batch number 411000\n",
            "Training mini-batch number 412000\n",
            "Training mini-batch number 413000\n",
            "Training mini-batch number 414000\n",
            "Training mini-batch number 415000\n",
            "Training mini-batch number 416000\n",
            "Training mini-batch number 417000\n",
            "Training mini-batch number 418000\n",
            "Training mini-batch number 419000\n",
            "Training mini-batch number 420000\n",
            "Training mini-batch number 421000\n",
            "Training mini-batch number 422000\n",
            "Training mini-batch number 423000\n",
            "Training mini-batch number 424000\n",
            "Epoch 16: validation accuracy 99.54%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 99.61%\n",
            "Training mini-batch number 425000\n",
            "Training mini-batch number 426000\n",
            "Training mini-batch number 427000\n",
            "Training mini-batch number 428000\n",
            "Training mini-batch number 429000\n",
            "Training mini-batch number 430000\n",
            "Training mini-batch number 431000\n",
            "Training mini-batch number 432000\n",
            "Training mini-batch number 433000\n",
            "Training mini-batch number 434000\n",
            "Training mini-batch number 435000\n",
            "Training mini-batch number 436000\n",
            "Training mini-batch number 437000\n",
            "Training mini-batch number 438000\n",
            "Training mini-batch number 439000\n",
            "Training mini-batch number 440000\n",
            "Training mini-batch number 441000\n",
            "Training mini-batch number 442000\n",
            "Training mini-batch number 443000\n",
            "Training mini-batch number 444000\n",
            "Training mini-batch number 445000\n",
            "Training mini-batch number 446000\n",
            "Training mini-batch number 447000\n",
            "Training mini-batch number 448000\n",
            "Training mini-batch number 449000\n",
            "Epoch 17: validation accuracy 99.47%\n",
            "Training mini-batch number 450000\n",
            "Training mini-batch number 451000\n",
            "Training mini-batch number 452000\n",
            "Training mini-batch number 453000\n",
            "Training mini-batch number 454000\n",
            "Training mini-batch number 455000\n",
            "Training mini-batch number 456000\n",
            "Training mini-batch number 457000\n",
            "Training mini-batch number 458000\n",
            "Training mini-batch number 459000\n",
            "Training mini-batch number 460000\n",
            "Training mini-batch number 461000\n",
            "Training mini-batch number 462000\n",
            "Training mini-batch number 463000\n",
            "Training mini-batch number 464000\n",
            "Training mini-batch number 465000\n",
            "Training mini-batch number 466000\n",
            "Training mini-batch number 467000\n",
            "Training mini-batch number 468000\n",
            "Training mini-batch number 469000\n",
            "Training mini-batch number 470000\n",
            "Training mini-batch number 471000\n",
            "Training mini-batch number 472000\n",
            "Training mini-batch number 473000\n",
            "Training mini-batch number 474000\n",
            "Epoch 18: validation accuracy 99.45%\n",
            "Training mini-batch number 475000\n",
            "Training mini-batch number 476000\n",
            "Training mini-batch number 477000\n",
            "Training mini-batch number 478000\n",
            "Training mini-batch number 479000\n",
            "Training mini-batch number 480000\n",
            "Training mini-batch number 481000\n",
            "Training mini-batch number 482000\n",
            "Training mini-batch number 483000\n",
            "Training mini-batch number 484000\n",
            "Training mini-batch number 485000\n",
            "Training mini-batch number 486000\n",
            "Training mini-batch number 487000\n",
            "Training mini-batch number 488000\n",
            "Training mini-batch number 489000\n",
            "Training mini-batch number 490000\n",
            "Training mini-batch number 491000\n",
            "Training mini-batch number 492000\n",
            "Training mini-batch number 493000\n",
            "Training mini-batch number 494000\n",
            "Training mini-batch number 495000\n",
            "Training mini-batch number 496000\n",
            "Training mini-batch number 497000\n",
            "Training mini-batch number 498000\n",
            "Training mini-batch number 499000\n",
            "Epoch 19: validation accuracy 99.47%\n",
            "Training mini-batch number 500000\n",
            "Training mini-batch number 501000\n",
            "Training mini-batch number 502000\n",
            "Training mini-batch number 503000\n",
            "Training mini-batch number 504000\n",
            "Training mini-batch number 505000\n",
            "Training mini-batch number 506000\n",
            "Training mini-batch number 507000\n",
            "Training mini-batch number 508000\n",
            "Training mini-batch number 509000\n",
            "Training mini-batch number 510000\n",
            "Training mini-batch number 511000\n",
            "Training mini-batch number 512000\n",
            "Training mini-batch number 513000\n",
            "Training mini-batch number 514000\n",
            "Training mini-batch number 515000\n",
            "Training mini-batch number 516000\n",
            "Training mini-batch number 517000\n",
            "Training mini-batch number 518000\n",
            "Training mini-batch number 519000\n",
            "Training mini-batch number 520000\n",
            "Training mini-batch number 521000\n",
            "Training mini-batch number 522000\n",
            "Training mini-batch number 523000\n",
            "Training mini-batch number 524000\n",
            "Epoch 20: validation accuracy 99.54%\n",
            "Training mini-batch number 525000\n",
            "Training mini-batch number 526000\n",
            "Training mini-batch number 527000\n",
            "Training mini-batch number 528000\n",
            "Training mini-batch number 529000\n",
            "Training mini-batch number 530000\n",
            "Training mini-batch number 531000\n",
            "Training mini-batch number 532000\n",
            "Training mini-batch number 533000\n",
            "Training mini-batch number 534000\n",
            "Training mini-batch number 535000\n",
            "Training mini-batch number 536000\n",
            "Training mini-batch number 537000\n",
            "Training mini-batch number 538000\n",
            "Training mini-batch number 539000\n",
            "Training mini-batch number 540000\n",
            "Training mini-batch number 541000\n",
            "Training mini-batch number 542000\n",
            "Training mini-batch number 543000\n",
            "Training mini-batch number 544000\n",
            "Training mini-batch number 545000\n",
            "Training mini-batch number 546000\n",
            "Training mini-batch number 547000\n",
            "Training mini-batch number 548000\n",
            "Training mini-batch number 549000\n",
            "Epoch 21: validation accuracy 99.50%\n",
            "Training mini-batch number 550000\n",
            "Training mini-batch number 551000\n",
            "Training mini-batch number 552000\n",
            "Training mini-batch number 553000\n",
            "Training mini-batch number 554000\n",
            "Training mini-batch number 555000\n",
            "Training mini-batch number 556000\n",
            "Training mini-batch number 557000\n",
            "Training mini-batch number 558000\n",
            "Training mini-batch number 559000\n",
            "Training mini-batch number 560000\n",
            "Training mini-batch number 561000\n",
            "Training mini-batch number 562000\n",
            "Training mini-batch number 563000\n",
            "Training mini-batch number 564000\n",
            "Training mini-batch number 565000\n",
            "Training mini-batch number 566000\n",
            "Training mini-batch number 567000\n",
            "Training mini-batch number 568000\n",
            "Training mini-batch number 569000\n",
            "Training mini-batch number 570000\n",
            "Training mini-batch number 571000\n",
            "Training mini-batch number 572000\n",
            "Training mini-batch number 573000\n",
            "Training mini-batch number 574000\n",
            "Epoch 22: validation accuracy 99.47%\n",
            "Training mini-batch number 575000\n",
            "Training mini-batch number 576000\n",
            "Training mini-batch number 577000\n",
            "Training mini-batch number 578000\n",
            "Training mini-batch number 579000\n",
            "Training mini-batch number 580000\n",
            "Training mini-batch number 581000\n",
            "Training mini-batch number 582000\n",
            "Training mini-batch number 583000\n",
            "Training mini-batch number 584000\n",
            "Training mini-batch number 585000\n",
            "Training mini-batch number 586000\n",
            "Training mini-batch number 587000\n",
            "Training mini-batch number 588000\n",
            "Training mini-batch number 589000\n",
            "Training mini-batch number 590000\n",
            "Training mini-batch number 591000\n",
            "Training mini-batch number 592000\n",
            "Training mini-batch number 593000\n",
            "Training mini-batch number 594000\n",
            "Training mini-batch number 595000\n",
            "Training mini-batch number 596000\n",
            "Training mini-batch number 597000\n",
            "Training mini-batch number 598000\n",
            "Training mini-batch number 599000\n",
            "Epoch 23: validation accuracy 99.52%\n",
            "Training mini-batch number 600000\n",
            "Training mini-batch number 601000\n",
            "Training mini-batch number 602000\n",
            "Training mini-batch number 603000\n",
            "Training mini-batch number 604000\n",
            "Training mini-batch number 605000\n",
            "Training mini-batch number 606000\n",
            "Training mini-batch number 607000\n",
            "Training mini-batch number 608000\n",
            "Training mini-batch number 609000\n",
            "Training mini-batch number 610000\n",
            "Training mini-batch number 611000\n",
            "Training mini-batch number 612000\n",
            "Training mini-batch number 613000\n",
            "Training mini-batch number 614000\n",
            "Training mini-batch number 615000\n",
            "Training mini-batch number 616000\n",
            "Training mini-batch number 617000\n",
            "Training mini-batch number 618000\n",
            "Training mini-batch number 619000\n",
            "Training mini-batch number 620000\n",
            "Training mini-batch number 621000\n",
            "Training mini-batch number 622000\n",
            "Training mini-batch number 623000\n",
            "Training mini-batch number 624000\n",
            "Epoch 24: validation accuracy 99.46%\n",
            "Training mini-batch number 625000\n",
            "Training mini-batch number 626000\n",
            "Training mini-batch number 627000\n",
            "Training mini-batch number 628000\n",
            "Training mini-batch number 629000\n",
            "Training mini-batch number 630000\n",
            "Training mini-batch number 631000\n",
            "Training mini-batch number 632000\n",
            "Training mini-batch number 633000\n",
            "Training mini-batch number 634000\n",
            "Training mini-batch number 635000\n",
            "Training mini-batch number 636000\n",
            "Training mini-batch number 637000\n",
            "Training mini-batch number 638000\n",
            "Training mini-batch number 639000\n",
            "Training mini-batch number 640000\n",
            "Training mini-batch number 641000\n",
            "Training mini-batch number 642000\n",
            "Training mini-batch number 643000\n",
            "Training mini-batch number 644000\n",
            "Training mini-batch number 645000\n",
            "Training mini-batch number 646000\n",
            "Training mini-batch number 647000\n",
            "Training mini-batch number 648000\n",
            "Training mini-batch number 649000\n",
            "Epoch 25: validation accuracy 99.57%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 99.56%\n",
            "Training mini-batch number 650000\n",
            "Training mini-batch number 651000\n",
            "Training mini-batch number 652000\n",
            "Training mini-batch number 653000\n",
            "Training mini-batch number 654000\n",
            "Training mini-batch number 655000\n",
            "Training mini-batch number 656000\n",
            "Training mini-batch number 657000\n",
            "Training mini-batch number 658000\n",
            "Training mini-batch number 659000\n",
            "Training mini-batch number 660000\n",
            "Training mini-batch number 661000\n",
            "Training mini-batch number 662000\n",
            "Training mini-batch number 663000\n",
            "Training mini-batch number 664000\n",
            "Training mini-batch number 665000\n",
            "Training mini-batch number 666000\n",
            "Training mini-batch number 667000\n",
            "Training mini-batch number 668000\n",
            "Training mini-batch number 669000\n",
            "Training mini-batch number 670000\n",
            "Training mini-batch number 671000\n",
            "Training mini-batch number 672000\n",
            "Training mini-batch number 673000\n",
            "Training mini-batch number 674000\n",
            "Epoch 26: validation accuracy 99.56%\n",
            "Training mini-batch number 675000\n",
            "Training mini-batch number 676000\n",
            "Training mini-batch number 677000\n",
            "Training mini-batch number 678000\n",
            "Training mini-batch number 679000\n",
            "Training mini-batch number 680000\n",
            "Training mini-batch number 681000\n",
            "Training mini-batch number 682000\n",
            "Training mini-batch number 683000\n",
            "Training mini-batch number 684000\n",
            "Training mini-batch number 685000\n",
            "Training mini-batch number 686000\n",
            "Training mini-batch number 687000\n",
            "Training mini-batch number 688000\n",
            "Training mini-batch number 689000\n",
            "Training mini-batch number 690000\n",
            "Training mini-batch number 691000\n",
            "Training mini-batch number 692000\n",
            "Training mini-batch number 693000\n",
            "Training mini-batch number 694000\n",
            "Training mini-batch number 695000\n",
            "Training mini-batch number 696000\n",
            "Training mini-batch number 697000\n",
            "Training mini-batch number 698000\n",
            "Training mini-batch number 699000\n",
            "Epoch 27: validation accuracy 99.53%\n",
            "Training mini-batch number 700000\n",
            "Training mini-batch number 701000\n",
            "Training mini-batch number 702000\n",
            "Training mini-batch number 703000\n",
            "Training mini-batch number 704000\n",
            "Training mini-batch number 705000\n",
            "Training mini-batch number 706000\n",
            "Training mini-batch number 707000\n",
            "Training mini-batch number 708000\n",
            "Training mini-batch number 709000\n",
            "Training mini-batch number 710000\n",
            "Training mini-batch number 711000\n",
            "Training mini-batch number 712000\n",
            "Training mini-batch number 713000\n",
            "Training mini-batch number 714000\n",
            "Training mini-batch number 715000\n",
            "Training mini-batch number 716000\n",
            "Training mini-batch number 717000\n",
            "Training mini-batch number 718000\n",
            "Training mini-batch number 719000\n",
            "Training mini-batch number 720000\n",
            "Training mini-batch number 721000\n",
            "Training mini-batch number 722000\n",
            "Training mini-batch number 723000\n",
            "Training mini-batch number 724000\n",
            "Epoch 28: validation accuracy 99.56%\n",
            "Training mini-batch number 725000\n",
            "Training mini-batch number 726000\n",
            "Training mini-batch number 727000\n",
            "Training mini-batch number 728000\n",
            "Training mini-batch number 729000\n",
            "Training mini-batch number 730000\n",
            "Training mini-batch number 731000\n",
            "Training mini-batch number 732000\n",
            "Training mini-batch number 733000\n",
            "Training mini-batch number 734000\n",
            "Training mini-batch number 735000\n",
            "Training mini-batch number 736000\n",
            "Training mini-batch number 737000\n",
            "Training mini-batch number 738000\n",
            "Training mini-batch number 739000\n",
            "Training mini-batch number 740000\n",
            "Training mini-batch number 741000\n",
            "Training mini-batch number 742000\n",
            "Training mini-batch number 743000\n",
            "Training mini-batch number 744000\n",
            "Training mini-batch number 745000\n",
            "Training mini-batch number 746000\n",
            "Training mini-batch number 747000\n",
            "Training mini-batch number 748000\n",
            "Training mini-batch number 749000\n",
            "Epoch 29: validation accuracy 99.60%\n",
            "This is the best validation accuracy to date.\n",
            "The corresponding test accuracy is 99.61%\n",
            "Training mini-batch number 750000\n",
            "Training mini-batch number 751000\n",
            "Training mini-batch number 752000\n",
            "Training mini-batch number 753000\n",
            "Training mini-batch number 754000\n",
            "Training mini-batch number 755000\n",
            "Training mini-batch number 756000\n",
            "Training mini-batch number 757000\n",
            "Training mini-batch number 758000\n",
            "Training mini-batch number 759000\n",
            "Training mini-batch number 760000\n",
            "Training mini-batch number 761000\n",
            "Training mini-batch number 762000\n",
            "Training mini-batch number 763000\n",
            "Training mini-batch number 764000\n",
            "Training mini-batch number 765000\n",
            "Training mini-batch number 766000\n",
            "Training mini-batch number 767000\n",
            "Training mini-batch number 768000\n",
            "Training mini-batch number 769000\n",
            "Training mini-batch number 770000\n",
            "Training mini-batch number 771000\n",
            "Training mini-batch number 772000\n",
            "Training mini-batch number 773000\n",
            "Training mini-batch number 774000\n",
            "Epoch 30: validation accuracy 99.48%\n",
            "Training mini-batch number 775000\n",
            "Training mini-batch number 776000\n",
            "Training mini-batch number 777000\n",
            "Training mini-batch number 778000\n",
            "Training mini-batch number 779000\n",
            "Training mini-batch number 780000\n",
            "Training mini-batch number 781000\n",
            "Training mini-batch number 782000\n",
            "Training mini-batch number 783000\n",
            "Training mini-batch number 784000\n",
            "Training mini-batch number 785000\n",
            "Training mini-batch number 786000\n",
            "Training mini-batch number 787000\n",
            "Training mini-batch number 788000\n",
            "Training mini-batch number 789000\n",
            "Training mini-batch number 790000\n",
            "Training mini-batch number 791000\n",
            "Training mini-batch number 792000\n",
            "Training mini-batch number 793000\n",
            "Training mini-batch number 794000\n",
            "Training mini-batch number 795000\n",
            "Training mini-batch number 796000\n",
            "Training mini-batch number 797000\n",
            "Training mini-batch number 798000\n",
            "Training mini-batch number 799000\n",
            "Epoch 31: validation accuracy 99.55%\n",
            "Training mini-batch number 800000\n",
            "Training mini-batch number 801000\n",
            "Training mini-batch number 802000\n",
            "Training mini-batch number 803000\n",
            "Training mini-batch number 804000\n",
            "Training mini-batch number 805000\n",
            "Training mini-batch number 806000\n",
            "Training mini-batch number 807000\n",
            "Training mini-batch number 808000\n",
            "Training mini-batch number 809000\n",
            "Training mini-batch number 810000\n",
            "Training mini-batch number 811000\n",
            "Training mini-batch number 812000\n",
            "Training mini-batch number 813000\n",
            "Training mini-batch number 814000\n",
            "Training mini-batch number 815000\n",
            "Training mini-batch number 816000\n",
            "Training mini-batch number 817000\n",
            "Training mini-batch number 818000\n",
            "Training mini-batch number 819000\n",
            "Training mini-batch number 820000\n",
            "Training mini-batch number 821000\n",
            "Training mini-batch number 822000\n",
            "Training mini-batch number 823000\n",
            "Training mini-batch number 824000\n",
            "Epoch 32: validation accuracy 99.53%\n",
            "Training mini-batch number 825000\n",
            "Training mini-batch number 826000\n",
            "Training mini-batch number 827000\n",
            "Training mini-batch number 828000\n",
            "Training mini-batch number 829000\n",
            "Training mini-batch number 830000\n",
            "Training mini-batch number 831000\n",
            "Training mini-batch number 832000\n",
            "Training mini-batch number 833000\n",
            "Training mini-batch number 834000\n",
            "Training mini-batch number 835000\n",
            "Training mini-batch number 836000\n",
            "Training mini-batch number 837000\n",
            "Training mini-batch number 838000\n",
            "Training mini-batch number 839000\n",
            "Training mini-batch number 840000\n",
            "Training mini-batch number 841000\n",
            "Training mini-batch number 842000\n",
            "Training mini-batch number 843000\n",
            "Training mini-batch number 844000\n",
            "Training mini-batch number 845000\n",
            "Training mini-batch number 846000\n",
            "Training mini-batch number 847000\n",
            "Training mini-batch number 848000\n",
            "Training mini-batch number 849000\n",
            "Epoch 33: validation accuracy 99.50%\n",
            "Training mini-batch number 850000\n",
            "Training mini-batch number 851000\n",
            "Training mini-batch number 852000\n",
            "Training mini-batch number 853000\n",
            "Training mini-batch number 854000\n",
            "Training mini-batch number 855000\n",
            "Training mini-batch number 856000\n",
            "Training mini-batch number 857000\n",
            "Training mini-batch number 858000\n",
            "Training mini-batch number 859000\n",
            "Training mini-batch number 860000\n",
            "Training mini-batch number 861000\n",
            "Training mini-batch number 862000\n",
            "Training mini-batch number 863000\n",
            "Training mini-batch number 864000\n",
            "Training mini-batch number 865000\n",
            "Training mini-batch number 866000\n",
            "Training mini-batch number 867000\n",
            "Training mini-batch number 868000\n",
            "Training mini-batch number 869000\n",
            "Training mini-batch number 870000\n",
            "Training mini-batch number 871000\n",
            "Training mini-batch number 872000\n",
            "Training mini-batch number 873000\n",
            "Training mini-batch number 874000\n",
            "Epoch 34: validation accuracy 99.51%\n",
            "Training mini-batch number 875000\n",
            "Training mini-batch number 876000\n",
            "Training mini-batch number 877000\n",
            "Training mini-batch number 878000\n",
            "Training mini-batch number 879000\n",
            "Training mini-batch number 880000\n",
            "Training mini-batch number 881000\n",
            "Training mini-batch number 882000\n",
            "Training mini-batch number 883000\n",
            "Training mini-batch number 884000\n",
            "Training mini-batch number 885000\n",
            "Training mini-batch number 886000\n",
            "Training mini-batch number 887000\n",
            "Training mini-batch number 888000\n",
            "Training mini-batch number 889000\n",
            "Training mini-batch number 890000\n",
            "Training mini-batch number 891000\n",
            "Training mini-batch number 892000\n",
            "Training mini-batch number 893000\n",
            "Training mini-batch number 894000\n",
            "Training mini-batch number 895000\n",
            "Training mini-batch number 896000\n",
            "Training mini-batch number 897000\n",
            "Training mini-batch number 898000\n",
            "Training mini-batch number 899000\n",
            "Epoch 35: validation accuracy 99.49%\n",
            "Training mini-batch number 900000\n",
            "Training mini-batch number 901000\n",
            "Training mini-batch number 902000\n",
            "Training mini-batch number 903000\n",
            "Training mini-batch number 904000\n",
            "Training mini-batch number 905000\n",
            "Training mini-batch number 906000\n",
            "Training mini-batch number 907000\n",
            "Training mini-batch number 908000\n",
            "Training mini-batch number 909000\n",
            "Training mini-batch number 910000\n",
            "Training mini-batch number 911000\n",
            "Training mini-batch number 912000\n",
            "Training mini-batch number 913000\n",
            "Training mini-batch number 914000\n",
            "Training mini-batch number 915000\n",
            "Training mini-batch number 916000\n",
            "Training mini-batch number 917000\n",
            "Training mini-batch number 918000\n",
            "Training mini-batch number 919000\n",
            "Training mini-batch number 920000\n",
            "Training mini-batch number 921000\n",
            "Training mini-batch number 922000\n",
            "Training mini-batch number 923000\n",
            "Training mini-batch number 924000\n",
            "Epoch 36: validation accuracy 99.57%\n",
            "Training mini-batch number 925000\n",
            "Training mini-batch number 926000\n",
            "Training mini-batch number 927000\n",
            "Training mini-batch number 928000\n",
            "Training mini-batch number 929000\n",
            "Training mini-batch number 930000\n",
            "Training mini-batch number 931000\n",
            "Training mini-batch number 932000\n",
            "Training mini-batch number 933000\n",
            "Training mini-batch number 934000\n",
            "Training mini-batch number 935000\n",
            "Training mini-batch number 936000\n",
            "Training mini-batch number 937000\n",
            "Training mini-batch number 938000\n",
            "Training mini-batch number 939000\n",
            "Training mini-batch number 940000\n",
            "Training mini-batch number 941000\n",
            "Training mini-batch number 942000\n",
            "Training mini-batch number 943000\n",
            "Training mini-batch number 944000\n",
            "Training mini-batch number 945000\n",
            "Training mini-batch number 946000\n",
            "Training mini-batch number 947000\n",
            "Training mini-batch number 948000\n",
            "Training mini-batch number 949000\n",
            "Epoch 37: validation accuracy 99.59%\n",
            "Training mini-batch number 950000\n",
            "Training mini-batch number 951000\n",
            "Training mini-batch number 952000\n",
            "Training mini-batch number 953000\n",
            "Training mini-batch number 954000\n",
            "Training mini-batch number 955000\n",
            "Training mini-batch number 956000\n",
            "Training mini-batch number 957000\n",
            "Training mini-batch number 958000\n",
            "Training mini-batch number 959000\n",
            "Training mini-batch number 960000\n",
            "Training mini-batch number 961000\n",
            "Training mini-batch number 962000\n",
            "Training mini-batch number 963000\n",
            "Training mini-batch number 964000\n",
            "Training mini-batch number 965000\n",
            "Training mini-batch number 966000\n",
            "Training mini-batch number 967000\n",
            "Training mini-batch number 968000\n",
            "Training mini-batch number 969000\n",
            "Training mini-batch number 970000\n",
            "Training mini-batch number 971000\n",
            "Training mini-batch number 972000\n",
            "Training mini-batch number 973000\n",
            "Training mini-batch number 974000\n",
            "Epoch 38: validation accuracy 99.55%\n",
            "Training mini-batch number 975000\n",
            "Training mini-batch number 976000\n",
            "Training mini-batch number 977000\n",
            "Training mini-batch number 978000\n",
            "Training mini-batch number 979000\n",
            "Training mini-batch number 980000\n",
            "Training mini-batch number 981000\n",
            "Training mini-batch number 982000\n",
            "Training mini-batch number 983000\n",
            "Training mini-batch number 984000\n",
            "Training mini-batch number 985000\n",
            "Training mini-batch number 986000\n",
            "Training mini-batch number 987000\n",
            "Training mini-batch number 988000\n",
            "Training mini-batch number 989000\n",
            "Training mini-batch number 990000\n",
            "Training mini-batch number 991000\n",
            "Training mini-batch number 992000\n",
            "Training mini-batch number 993000\n",
            "Training mini-batch number 994000\n",
            "Training mini-batch number 995000\n",
            "Training mini-batch number 996000\n",
            "Training mini-batch number 997000\n",
            "Training mini-batch number 998000\n",
            "Training mini-batch number 999000\n",
            "Epoch 39: validation accuracy 99.59%\n",
            "Finished training network.\n",
            "Best validation accuracy of 99.60% obtained at iteration 749999\n",
            "Corresponding test accuracy of 99.61%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "x-aJSTaunjWv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}